2023-12-10 11:40:42,005 [INFO]: 127.0.0.1 - - [10/Dec/2023 11:40:42] "POST /api/uploadFiles HTTP/1.1" 200 -
2023-12-10 11:40:42,018 [DEBUG]: Using selector: KqueueSelector
2023-12-10 11:40:42,206 [INFO]: 127.0.0.1 - - [10/Dec/2023 11:40:42] "POST /api/generate_embeddings HTTP/1.1" 200 -
2023-12-10 11:44:53,378 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-10 11:44:53,378 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-10 11:45:07,784 [INFO]: 127.0.0.1 - - [10/Dec/2023 11:45:07] "POST /api/uploadFiles HTTP/1.1" 200 -
2023-12-10 11:45:07,799 [DEBUG]: Using selector: KqueueSelector
2023-12-10 11:45:07,905 [DEBUG]: Error in upload_and_generate_embedding:
2023-12-10 11:45:07,906 [INFO]: 127.0.0.1 - - [10/Dec/2023 11:45:07] "POST /api/generate_embeddings HTTP/1.1" 200 -
2023-12-10 11:49:06,408 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-10 11:49:06,408 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-10 11:49:28,291 [INFO]: 127.0.0.1 - - [10/Dec/2023 11:49:28] "POST /api/uploadFiles HTTP/1.1" 200 -
2023-12-10 11:49:28,303 [DEBUG]: Using selector: KqueueSelector
2023-12-10 11:49:28,468 [DEBUG]: Initialized Pinecone
2023-12-10 11:49:28,469 [DEBUG]: Error in upload_and_generate_embedding: '_io.BufferedReader' object has no attribute 'filename'
2023-12-10 11:49:28,471 [INFO]: 127.0.0.1 - - [10/Dec/2023 11:49:28] "POST /api/generate_embeddings HTTP/1.1" 200 -
2023-12-10 11:52:36,546 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-10 11:52:36,546 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-10 11:52:43,009 [INFO]: 127.0.0.1 - - [10/Dec/2023 11:52:43] "POST /api/uploadFiles HTTP/1.1" 200 -
2023-12-10 11:52:43,026 [DEBUG]: Using selector: KqueueSelector
2023-12-10 11:52:43,144 [DEBUG]: Initialized Pinecone
2023-12-10 11:52:43,144 [DEBUG]: Error in upload_and_generate_embedding: join() argument must be str, bytes, or os.PathLike object, not 'BufferedReader'
2023-12-10 11:52:43,145 [INFO]: 127.0.0.1 - - [10/Dec/2023 11:52:43] "POST /api/generate_embeddings HTTP/1.1" 200 -
2023-12-10 11:54:06,720 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-10 11:54:06,720 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-10 11:54:59,070 [INFO]: 127.0.0.1 - - [10/Dec/2023 11:54:59] "POST /api/uploadFiles HTTP/1.1" 200 -
2023-12-10 11:54:59,085 [DEBUG]: Using selector: KqueueSelector
2023-12-10 11:54:59,220 [DEBUG]: Initialized Pinecone
2023-12-10 11:54:59,220 [DEBUG]: Error in upload_and_generate_embedding: join() argument must be str, bytes, or os.PathLike object, not 'BufferedReader'
2023-12-10 11:54:59,221 [INFO]: 127.0.0.1 - - [10/Dec/2023 11:54:59] "POST /api/generate_embeddings HTTP/1.1" 200 -
2023-12-10 11:55:25,490 [INFO]: 127.0.0.1 - - [10/Dec/2023 11:55:25] "POST /api/uploadFiles HTTP/1.1" 200 -
2023-12-10 11:55:25,504 [DEBUG]: Using selector: KqueueSelector
2023-12-10 11:55:25,632 [DEBUG]: Initialized Pinecone
2023-12-10 11:55:25,632 [DEBUG]: Error in upload_and_generate_embedding: join() argument must be str, bytes, or os.PathLike object, not 'BufferedReader'
2023-12-10 11:55:25,634 [INFO]: 127.0.0.1 - - [10/Dec/2023 11:55:25] "POST /api/generate_embeddings HTTP/1.1" 200 -
2023-12-10 11:57:14,764 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-10 11:57:14,764 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-10 11:57:20,207 [INFO]: 127.0.0.1 - - [10/Dec/2023 11:57:20] "POST /api/uploadFiles HTTP/1.1" 200 -
2023-12-10 11:57:20,220 [DEBUG]: Using selector: KqueueSelector
2023-12-10 11:57:20,425 [DEBUG]: Initialized Pinecone
2023-12-10 11:57:20,426 [DEBUG]: Error in upload_and_generate_embedding: '_io.BufferedReader' object has no attribute 'save'
2023-12-10 11:57:20,428 [INFO]: 127.0.0.1 - - [10/Dec/2023 11:57:20] "POST /api/generate_embeddings HTTP/1.1" 200 -
2023-12-10 12:01:22,873 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-10 12:01:22,873 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-10 12:01:31,447 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:01:31] "POST /api/uploadFiles HTTP/1.1" 200 -
2023-12-10 12:01:31,457 [DEBUG]: Using selector: KqueueSelector
2023-12-10 12:01:31,677 [DEBUG]: Initialized Pinecone
2023-12-10 12:01:31,678 [DEBUG]: Error in upload_and_generate_embedding: '_io.BufferedReader' object has no attribute 'save'
2023-12-10 12:01:31,679 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:01:31] "POST /api/generate_embeddings HTTP/1.1" 200 -
2023-12-10 12:04:31,131 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-10 12:04:31,131 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-10 12:04:48,305 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:04:48] "POST /api/uploadFiles HTTP/1.1" 200 -
2023-12-10 12:04:48,317 [DEBUG]: Using selector: KqueueSelector
<_io.BufferedReader name='user-uploads/2305.06983.pdf'>
2023-12-10 12:04:48,318 [DEBUG]: <_io.BufferedReader name='user-uploads/2305.06983.pdf'>
2023-12-10 12:04:48,462 [DEBUG]: Initialized Pinecone
2023-12-10 12:04:48,463 [DEBUG]: Error in upload_and_generate_embedding: '_io.BufferedReader' object has no attribute 'save'
2023-12-10 12:04:48,464 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:04:48] "POST /api/generate_embeddings HTTP/1.1" 200 -
2023-12-10 12:05:52,756 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-10 12:05:52,756 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-10 12:06:06,649 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:06:06] "POST /api/uploadFiles HTTP/1.1" 200 -
2023-12-10 12:06:06,665 [DEBUG]: Using selector: KqueueSelector
<_io.BufferedReader name='user-uploads/2305.06983.pdf'>
2023-12-10 12:06:06,667 [DEBUG]: <_io.BufferedReader name='user-uploads/2305.06983.pdf'>
2023-12-10 12:06:06,801 [DEBUG]: Initialized Pinecone
2023-12-10 12:06:06,801 [DEBUG]: Type of pdf_file: <class '_io.BufferedReader'>
2023-12-10 12:06:06,802 [DEBUG]: Error in upload_and_generate_embedding: '_io.BufferedReader' object has no attribute 'save'
2023-12-10 12:06:06,806 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:06:06] "POST /api/generate_embeddings HTTP/1.1" 200 -
2023-12-10 12:11:19,815 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-10 12:11:19,815 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-10 12:11:28,236 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:11:28] "POST /api/uploadFiles HTTP/1.1" 200 -
2023-12-10 12:11:28,260 [DEBUG]: Using selector: KqueueSelector
2023-12-10 12:11:28,262 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:11:28] "[35m[1mPOST /api/generate_embeddings HTTP/1.1[0m" 500 -
2023-12-10 12:13:38,065 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-10 12:13:38,065 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-10 12:13:42,864 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:13:42] "POST /api/uploadFiles HTTP/1.1" 200 -
2023-12-10 12:13:42,873 [DEBUG]: Using selector: KqueueSelector
2023-12-10 12:13:42,874 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:13:42] "[31m[1mPOST /api/generate_embeddings HTTP/1.1[0m" 400 -
2023-12-10 12:14:48,806 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-10 12:14:48,807 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-10 12:14:54,164 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:14:54] "POST /api/uploadFiles HTTP/1.1" 200 -
2023-12-10 12:14:54,176 [DEBUG]: Using selector: KqueueSelector
2023-12-10 12:14:54,178 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:14:54] "[31m[1mPOST /api/generate_embeddings HTTP/1.1[0m" 400 -
2023-12-10 12:15:46,279 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-10 12:15:46,280 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-10 12:15:51,652 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:15:51] "POST /api/uploadFiles HTTP/1.1" 200 -
2023-12-10 12:15:51,662 [DEBUG]: Using selector: KqueueSelector
Invalid data format or missing 'files' key
2023-12-10 12:15:51,663 [DEBUG]: Invalid data format or missing 'files' key
2023-12-10 12:15:51,664 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:15:51] "[31m[1mPOST /api/generate_embeddings HTTP/1.1[0m" 400 -
2023-12-10 12:16:54,071 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-10 12:16:54,072 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-10 12:16:58,674 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:16:58] "POST /api/uploadFiles HTTP/1.1" 200 -
2023-12-10 12:16:58,689 [DEBUG]: Using selector: KqueueSelector
2023-12-10 12:16:58,692 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:16:58] "[31m[1mPOST /api/generate_embeddings HTTP/1.1[0m" 400 -
2023-12-10 12:17:39,819 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-10 12:17:39,819 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-10 12:17:47,433 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:17:47] "POST /api/uploadFiles HTTP/1.1" 200 -
2023-12-10 12:17:47,443 [DEBUG]: Using selector: KqueueSelector
Invalid data format or mkkissing 'files' key
2023-12-10 12:17:47,444 [DEBUG]: Invalid data format or mkkissing 'files' key
2023-12-10 12:17:47,445 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:17:47] "[31m[1mPOST /api/generate_embeddings HTTP/1.1[0m" 400 -
2023-12-10 12:18:10,789 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-10 12:18:10,789 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-10 12:18:15,974 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:18:15] "POST /api/uploadFiles HTTP/1.1" 200 -
2023-12-10 12:18:15,987 [DEBUG]: Using selector: KqueueSelector
2023-12-10 12:18:15,989 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:18:15] "[35m[1mPOST /api/generate_embeddings HTTP/1.1[0m" 500 -
2023-12-10 12:19:44,355 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-10 12:19:44,355 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-10 12:19:49,590 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:19:49] "POST /api/uploadFiles HTTP/1.1" 200 -
2023-12-10 12:19:49,603 [DEBUG]: Using selector: KqueueSelector
user-uploads/2305.06983.pdf
2023-12-10 12:19:49,605 [DEBUG]: user-uploads/2305.06983.pdf
2023-12-10 12:19:49,606 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:19:49] "[35m[1mPOST /api/generate_embeddings HTTP/1.1[0m" 500 -
2023-12-10 12:22:44,873 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-10 12:22:44,873 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-10 12:22:49,445 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:22:49] "[35m[1mPOST /api/uploadFiles HTTP/1.1[0m" 500 -
2023-12-10 12:26:23,755 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-10 12:26:23,755 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-10 12:26:29,247 [DEBUG]: <FileStorage: '2305.06983.pdf' ('application/pdf')>
2023-12-10 12:26:29,250 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:26:29] "[35m[1mPOST /api/uploadFiles HTTP/1.1[0m" 500 -
2023-12-10 12:27:58,033 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-10 12:27:58,033 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-10 12:28:03,420 [DEBUG]: <FileStorage: '2305.06983.pdf' ('application/pdf')>
2023-12-10 12:28:03,421 [DEBUG]: files
2023-12-10 12:28:03,422 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:28:03] "[35m[1mPOST /api/uploadFiles HTTP/1.1[0m" 500 -
2023-12-10 12:33:59,216 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-10 12:33:59,217 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-10 12:34:10,897 [DEBUG]: <FileStorage: 'AI_Foundations_Pset1_Final.pdf' ('application/pdf')>
2023-12-10 12:34:10,897 [DEBUG]: files
2023-12-10 12:34:10,899 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:34:10] "POST /api/uploadFiles HTTP/1.1" 200 -
2023-12-10 12:34:10,914 [DEBUG]: Using selector: KqueueSelector
AI_Foundations_Pset1_Final.pdf
2023-12-10 12:34:10,915 [DEBUG]: AI_Foundations_Pset1_Final.pdf
2023-12-10 12:34:10,917 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:34:10] "[35m[1mPOST /api/generate_embeddings HTTP/1.1[0m" 500 -
2023-12-10 12:36:11,867 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-10 12:36:11,867 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-10 12:36:16,816 [DEBUG]: <FileStorage: 'AI_Foundations_Pset1_Final.pdf' ('application/pdf')>
2023-12-10 12:36:16,816 [DEBUG]: files
2023-12-10 12:36:16,818 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:36:16] "POST /api/uploadFiles HTTP/1.1" 200 -
2023-12-10 12:36:16,834 [DEBUG]: Using selector: KqueueSelector
data
2023-12-10 12:36:16,836 [DEBUG]: data
AI_Foundations_Pset1_Final.pdf
2023-12-10 12:36:16,842 [DEBUG]: AI_Foundations_Pset1_Final.pdf
2023-12-10 12:36:16,843 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:36:16] "[35m[1mPOST /api/generate_embeddings HTTP/1.1[0m" 500 -
2023-12-10 12:38:12,662 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-10 12:38:12,662 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-10 12:38:17,366 [DEBUG]: <FileStorage: '2305.06983.pdf' ('application/pdf')>
2023-12-10 12:38:17,367 [DEBUG]: files
2023-12-10 12:38:17,369 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:38:17] "POST /api/uploadFiles HTTP/1.1" 200 -
2023-12-10 12:38:17,379 [DEBUG]: Using selector: KqueueSelector
2305.06983.pdf
2023-12-10 12:38:17,388 [DEBUG]: 2305.06983.pdf
2023-12-10 12:38:17,389 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:38:17] "[35m[1mPOST /api/generate_embeddings HTTP/1.1[0m" 500 -
2023-12-10 12:40:29,269 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-10 12:40:29,269 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-10 12:40:34,078 [DEBUG]: <FileStorage: 'AI_Foundations_Pset1_Final.pdf' ('application/pdf')>
2023-12-10 12:40:34,079 [DEBUG]: files
2023-12-10 12:40:34,080 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:40:34] "POST /api/uploadFiles HTTP/1.1" 200 -
2023-12-10 12:40:34,091 [DEBUG]: Using selector: KqueueSelector
files
2023-12-10 12:40:34,092 [DEBUG]: files
data
2023-12-10 12:40:34,092 [DEBUG]: data
2023-12-10 12:40:34,093 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:40:34] "[35m[1mPOST /api/generate_embeddings HTTP/1.1[0m" 500 -
2023-12-10 12:41:20,766 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-10 12:41:20,766 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-10 12:41:26,272 [DEBUG]: <FileStorage: 'AI_Foundations_Pset1_Final.pdf' ('application/pdf')>
2023-12-10 12:41:26,272 [DEBUG]: files
2023-12-10 12:41:26,274 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:41:26] "POST /api/uploadFiles HTTP/1.1" 200 -
2023-12-10 12:41:26,290 [DEBUG]: Using selector: KqueueSelector
AI_Foundations_Pset1_Final.pdf
2023-12-10 12:41:26,292 [DEBUG]: AI_Foundations_Pset1_Final.pdf
2023-12-10 12:41:26,294 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:41:26] "[35m[1mPOST /api/generate_embeddings HTTP/1.1[0m" 500 -
2023-12-10 12:41:57,288 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-10 12:41:57,289 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-10 12:42:02,365 [DEBUG]: <FileStorage: '2305.06983.pdf' ('application/pdf')>
2023-12-10 12:42:02,365 [DEBUG]: files
2023-12-10 12:42:02,367 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:42:02] "POST /api/uploadFiles HTTP/1.1" 200 -
2023-12-10 12:42:02,381 [DEBUG]: Using selector: KqueueSelector
2023-12-10 12:42:02,598 [DEBUG]: Initialized Pinecone
2023-12-10 12:42:02,598 [DEBUG]: Type of pdf_file: <class 'str'>
2023-12-10 12:42:02,598 [DEBUG]: Error in upload_and_generate_embedding: 'str' object has no attribute 'name'
2023-12-10 12:42:02,599 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:42:02] "[35m[1mPOST /api/generate_embeddings HTTP/1.1[0m" 500 -
2023-12-10 12:43:10,785 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-10 12:43:10,786 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-10 12:43:15,338 [DEBUG]: <FileStorage: 'AI_Foundations_Pset1_Final.pdf' ('application/pdf')>
2023-12-10 12:43:15,338 [DEBUG]: files
2023-12-10 12:43:15,340 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:43:15] "POST /api/uploadFiles HTTP/1.1" 200 -
2023-12-10 12:43:15,355 [DEBUG]: Using selector: KqueueSelector
2023-12-10 12:43:15,470 [DEBUG]: Initialized Pinecone
2023-12-10 12:43:15,470 [DEBUG]: Type of pdf_file: <class 'str'>
2023-12-10 12:43:15,470 [DEBUG]: Error in upload_and_generate_embedding: 'str' object has no attribute 'save'
2023-12-10 12:43:15,472 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:43:15] "[35m[1mPOST /api/generate_embeddings HTTP/1.1[0m" 500 -
2023-12-10 12:45:26,588 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-10 12:45:26,588 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-10 12:45:33,646 [DEBUG]: <FileStorage: 'AI_Foundations_Pset1_Final.pdf' ('application/pdf')>
2023-12-10 12:45:33,646 [DEBUG]: files
2023-12-10 12:45:33,649 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:45:33] "POST /api/uploadFiles HTTP/1.1" 200 -
2023-12-10 12:45:33,661 [DEBUG]: Using selector: KqueueSelector
2023-12-10 12:45:33,770 [DEBUG]: Initialized Pinecone
2023-12-10 12:45:33,771 [DEBUG]: Type of pdf_file: <class 'str'>
2023-12-10 12:45:33,890 [DEBUG]: Parsed PDF
2023-12-10 12:45:34,114 [DEBUG]: List of indexes: ['research-chat-index']
2023-12-10 12:45:34,115 [DEBUG]: Initialized Pinecone Index
2023-12-10 12:45:34,117 [DEBUG]: Chunked PDF and obtained vectors
2023-12-10 12:45:34,118 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-10 12:45:34,119 [DEBUG]: load_verify_locations cafile='/usr/local/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-10 12:45:34,147 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x12fcf6160>, 'json_data': {'input': 'CPSC 488/588 - AI Foundation Models HW 1 Page 1 CPSC 488/588 AI Foundation Models Fall 2023 HW 1 Instructions: copy this project at https://www.overleaf.com/read/gjvcgrzjyxfm , complete the solutions, and return your solutions in pdf format. Full Name: Sneha Netid: ss3993 1.Q1 Warmup. this part is about refreshing your calculus on calculating derivatives of functions. (a) Given the function: f(x) = sin(3 x2+ 4 cos( x)) findâˆ‚f/âˆ‚x using the chain rule. Solution:âˆ‚f âˆ‚x= cos(3 x2+ 4 cos( x))Â·(6xâˆ’4 sin( x)) (b) Given the function: f(x) =etanh(2 x3âˆ’5x2+x) findâˆ‚f/âˆ‚x using the chain rule. Recall:âˆ‚tanh ( x) âˆ‚(x)= 1âˆ’tanh2(x) = sech2(x) Solution:âˆ‚f âˆ‚x=etanh(2 x3âˆ’5x2+x)Â·sech2(2x3âˆ’5x2+x)Â·\x00 6x2âˆ’10x+ 1\x01 (c) Consider the function: f(x, y, z ) =x2sin(yz) +eyzâˆ’z3y find partial derivatives of the function with respect to each variable separately. Solution:âˆ‚f âˆ‚x= 2xsin(yz), âˆ‚f âˆ‚y=x2zcos(yz) +zeyzâˆ’z3, âˆ‚f âˆ‚z=x2ycos(yz) +yeyzâˆ’3z2y. 2.Q2 Matrix calculus. Recall that matrices are a way of organizing data into rows and columns. Answer the following questions: (a) Consider a function given by: f(x) =cTAx (1) where x=\uf8ee \uf8f0x0 x1 x2\uf8f9 \uf8fb', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-10 12:45:34,250 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-10 12:45:34,289 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x12fcab910>
2023-12-10 12:45:34,289 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x1179f1490> server_hostname='api.openai.com' timeout=5.0
2023-12-10 12:45:34,306 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x117556050>
2023-12-10 12:45:34,306 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-10 12:45:34,307 [DEBUG]: send_request_headers.complete
2023-12-10 12:45:34,307 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-10 12:45:34,307 [DEBUG]: send_request_body.complete
2023-12-10 12:45:34,307 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-10 12:45:34,513 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 10 Dec 2023 17:45:34 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'25'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999704'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'3ms'), (b'x-request-id', b'ca69a08074cbc3a9ba3598d4b7b50afb'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=MBoLMvtSpzhMZ6wVAZ23nq0Eiqcw4KRE7dJ_vpaMERE-1702230334-1-Af45K2dHdHToWT4vPpdWI1i0g6tsCfZmuhJ2i9KZx0wlnT5bnYx/u+jA6KSP2K8/nRTepusUON60e2u39Sn+uZk=; path=/; expires=Sun, 10-Dec-23 18:15:34 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=qeBTcp40ek.PMpxlyniup6PE53.miW8u8.upBnFkOo8-1702230334464-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83374d657ef618ee-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-10 12:45:34,516 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-10 12:45:34,516 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-10 12:45:34,517 [DEBUG]: receive_response_body.complete
2023-12-10 12:45:34,518 [DEBUG]: response_closed.started
2023-12-10 12:45:34,518 [DEBUG]: response_closed.complete
2023-12-10 12:45:34,518 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-10 12:45:34,525 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-10 12:45:34,526 [DEBUG]: load_verify_locations cafile='/usr/local/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-10 12:45:34,551 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x12fd34360>, 'json_data': {'input': 'CPSC 488/588 - AI Foundation Models HW 1 Page 2 is a column vector of variables, c=\x022 3 1\x03 is a constant vector, and A=\uf8ee \uf8f0a00a01a02 a10a11a12 a20a21a22\uf8f9 \uf8fb is a integer valued matrix, Derive the gradient of fwith respect to x. Solution: Given the function f(x) =\x022 3 1\x03\uf8ee \uf8f0a00a01a02 a10a11a12 a20a21a22\uf8f9 \uf8fb\uf8ee \uf8f0x0 x1 x2\uf8f9 \uf8fb we can expand Axas: Ax=\uf8ee \uf8f0a00x0+a01x1+a02x2 a10x0+a11x1+a12x2 a20x0+a21x1+a22x2\uf8f9 \uf8fb The function f(x) expands to: f(x) = 2a00x0+ 3a10x0+a20x0+ 2a01x1+ 3a11x1+a21x1+ 2a02x2+ 3a12x2+a22x2 Differentiating with respect to the components of x, we get: âˆ‚f(x) âˆ‚x0= 2a00+ 3a10+a20 âˆ‚f(x) âˆ‚x1= 2a01+ 3a11+a21 âˆ‚f(x) âˆ‚x2= 2a02+ 3a12+a22 (b) Given the function f=xâŠ¤Â·AÂ·x+cÂ·sin(y)âŠ¤Â·x where â€¢Ais a symmetric matrix â€¢cis a scalar â€¢xis a vector â€¢yis a vector Derive the gradients with respect to xandy. Solution: Gradient with respect to x: Using the identity âˆ‡x(xTAx) = (A+AT)xand we also know that Ais symmetric (i.e., A=AT), so the derivative can be simplified to 2 Ax.', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-10 12:45:34,590 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-10 12:45:34,601 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x12fccecd0>
2023-12-10 12:45:34,602 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x1179f12e0> server_hostname='api.openai.com' timeout=5.0
2023-12-10 12:45:34,620 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x12fccdb90>
2023-12-10 12:45:34,620 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-10 12:45:34,621 [DEBUG]: send_request_headers.complete
2023-12-10 12:45:34,621 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-10 12:45:34,621 [DEBUG]: send_request_body.complete
2023-12-10 12:45:34,621 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-10 12:45:34,789 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 10 Dec 2023 17:45:34 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'61'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999742'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'3ms'), (b'x-request-id', b'487af024f2dc1463fea46a87b173fe14'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=s7AGnYWSRxaTr.Mb7tvqzWDOsjn4u2D0d3hWJ2ZxmW4-1702230334-0-ATA/a/splXtf7tfVZEYRIfl1CoWnlU/yIyNiJib8hcsWvbju5SZ3d7I91HWIHQi7EMj8OtsjM5s3wI0NFZe7QTI=; path=/; expires=Sun, 10-Dec-23 18:15:34 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=eWchc0qKq6anFEmEoL9uM61dGZv8rBbysdONxKSksFk-1702230334790-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83374d677f9641bd-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-10 12:45:34,791 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-10 12:45:34,792 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-10 12:45:34,793 [DEBUG]: receive_response_body.complete
2023-12-10 12:45:34,793 [DEBUG]: response_closed.started
2023-12-10 12:45:34,793 [DEBUG]: response_closed.complete
2023-12-10 12:45:34,794 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-10 12:45:34,798 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-10 12:45:34,799 [DEBUG]: load_verify_locations cafile='/usr/local/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-10 12:45:34,828 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x12fd534c0>, 'json_data': {'input': 'CPSC 488/588 - AI Foundation Models HW 1 Page 3 âˆ‚f âˆ‚x= 2Ax+csin(y) âˆ‚f âˆ‚y=cxcos(y) (c) Given the function g=xâŠ¤By+dtanh(z)âŠ¤x where â€¢Bis an arbitrary matrix â€¢dis a scalar â€¢xis a vector â€¢yis a vector of the same dimension as x â€¢zis a vector Derive the gradients with respect to x,y, and z. Hint: recallâˆ‚tanh( x) âˆ‚(x)= 1âˆ’tanh2(x) = sech2(x) Solution: 1. Gradient with respect to x: âˆ‚g âˆ‚x=By+dtanh( z) 2. Gradient with respect to y: âˆ‚g âˆ‚y=BTx 3. Gradient with respect to z: âˆ‚g âˆ‚z=dÂ·sech2(z)Â·x 3.Q3 Automatic differentiation (a) Consider the following function: f(x1, x2, x3, x4) =1 2exp(x1+x2 2)âˆ’(x3âˆ—x2 4) i. draw the computation graph corresponding to this function and fill in the gradient values for all of the intermediate nodes and the leaves in the computation graph. Assume the values for x1, x2, x3, x4areâˆ’1,2,4,âˆ’3 respectively. ii.Solution: iii. Derive the gradients of fwith respect to its inputs âˆ‡fusing symbolic differentiation and chain rule.', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-10 12:45:34,871 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-10 12:45:34,890 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x12fd5cd10>
2023-12-10 12:45:34,890 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x1179f1760> server_hostname='api.openai.com' timeout=5.0
2023-12-10 12:45:34,911 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x12fd5cc50>
2023-12-10 12:45:34,912 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-10 12:45:34,912 [DEBUG]: send_request_headers.complete
2023-12-10 12:45:34,912 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-10 12:45:34,912 [DEBUG]: send_request_body.complete
2023-12-10 12:45:34,912 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-10 12:45:35,062 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 10 Dec 2023 17:45:35 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'33'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999750'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'3ms'), (b'x-request-id', b'c2e993f04f825e0ddfb182d642ffbc49'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=cMrsECYKEaaSXCnT3nFdYXWMozFgXAU2gNi0SDFwehA-1702230335-1-AW4T4JIsc6UlYW2coOCpYzs7m024U1xjF8EovoN97NiZGi7JddpJprNQE5o3Pu57a8yCZfnwOgePXOvrHzt9T34=; path=/; expires=Sun, 10-Dec-23 18:15:35 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=EfmmSrIUmEnK6RBOUHGWtHPv4Zs5l9_LYEkw_Sl2xg4-1702230335063-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83374d694d11c452-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-10 12:45:35,063 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-10 12:45:35,064 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-10 12:45:35,064 [DEBUG]: receive_response_body.complete
2023-12-10 12:45:35,064 [DEBUG]: response_closed.started
2023-12-10 12:45:35,064 [DEBUG]: response_closed.complete
2023-12-10 12:45:35,065 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-10 12:45:35,067 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-10 12:45:35,068 [DEBUG]: load_verify_locations cafile='/usr/local/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-10 12:45:35,088 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x12fd7e7a0>, 'json_data': {'input': 'CPSC 488/588 - AI Foundation Models HW 1 Page 4 Solution: âˆ‚f âˆ‚x1=1 2exp(x1+x2 2) = 1/2âˆ—exp(3) = exp(3)/2 âˆ‚f âˆ‚x2=x2exp(x1+x2 2) = 2âˆ—exp(3) âˆ‚f âˆ‚x3=âˆ’x2 4=âˆ’9 âˆ‚f âˆ‚x4=âˆ’2x3x4= 24 4.Q4Explain the reason behind using negative sampling in the SkipGram word embeddings model. Solution: In a naive implementation of the SkipGram model, for each training example, we would update weights for all words in the vocabulary using softmax. By using negative sampling, instead of computing the softmax over all words in the vocabulary, we only need to compute it for the actual context word and a small set of negative samples. This greatly reduces the computational burden.Furthermore, negative sampling indirectly also sends implicit negative information which as a result the model implicitly gains information about what words are unlikely to appear in the context improving word vector quality. 5.Q5 transformer (a) In the multi-head self-attention operation, what is the cost of computation (in terms of number of number of FLoating point OPerations)? Assume bis batch size, mis sequence length, dis the model dimension, and his the number of attention heads. Assume the dimensionality of keys and queries are d/2h. Note: You need to derive the answer, just providing the final answer is not sufficient. Solution: Scaled Dot Product Attention: Dot product for one head: b Ã—mÃ—mÃ—d 2h Forhheads: bÃ—mÃ—mÃ—d=bm2d Applying Softmax: Softmax operations: b Ã—mÃ—mÃ—hâ‰ˆbm2h Computing the weighted average: For one head: b Ã—mÃ—mÃ—d', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-10 12:45:35,119 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-10 12:45:35,144 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x12fd82550>
2023-12-10 12:45:35,144 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x1179f1a30> server_hostname='api.openai.com' timeout=5.0
2023-12-10 12:45:35,171 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x12fd824d0>
2023-12-10 12:45:35,171 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-10 12:45:35,171 [DEBUG]: send_request_headers.complete
2023-12-10 12:45:35,171 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-10 12:45:35,171 [DEBUG]: send_request_body.complete
2023-12-10 12:45:35,171 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-10 12:45:35,343 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 10 Dec 2023 17:45:35 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'27'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999616'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'aa25837fc887bce604d596a78c64ce11'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=K3Iiwjj8q_6.meivCeXO7nf3BYRTlKwypm.18MgHS8o-1702230335-1-AQHjK5LSAjr1XtaB7Ci1muTggxoplSE6HGKBJ1c0Xtim8zTeJ7W6Owp7kxWHO9JCKE4HEKyJAAgHiGieUOBhPRg=; path=/; expires=Sun, 10-Dec-23 18:15:35 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=LfPdOAXm8Jne4OGjT0XMp94v7mEsnPAl_uj0bOLlzsg-1702230335343-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83374d6afa7a424f-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-10 12:45:35,345 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-10 12:45:35,346 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-10 12:45:35,380 [DEBUG]: receive_response_body.complete
2023-12-10 12:45:35,381 [DEBUG]: response_closed.started
2023-12-10 12:45:35,381 [DEBUG]: response_closed.complete
2023-12-10 12:45:35,381 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-10 12:45:35,385 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-10 12:45:35,387 [DEBUG]: load_verify_locations cafile='/usr/local/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-10 12:45:35,418 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x12fdae020>, 'json_data': {'input': 'For one head: b Ã—mÃ—mÃ—d 2h Forhheads: bÃ—mÃ—mÃ—d=bm2d Output projection: Output operations: 2 Ã—bÃ—mÃ—dÃ—d= 2bm2d Total FLOPs: Total operations: 3bmd2+bm2d+bm2h+bm2d+ 2bm2d= 4bmd2+ 3bm2d+bm2h', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-10 12:45:35,462 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-10 12:45:35,559 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x12fdb1110>
2023-12-10 12:45:35,559 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x1179f1e20> server_hostname='api.openai.com' timeout=5.0
2023-12-10 12:45:35,573 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x12fdb1d10>
2023-12-10 12:45:35,573 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-10 12:45:35,574 [DEBUG]: send_request_headers.complete
2023-12-10 12:45:35,574 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-10 12:45:35,574 [DEBUG]: send_request_body.complete
2023-12-10 12:45:35,574 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-10 12:45:35,709 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 10 Dec 2023 17:45:35 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'21'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999952'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'bca0b36bff50bd42b5b072d5cf3316f5'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=Dj4QX1XRqcaj4NQ8fNCatP2PXu_q_w4abFNOhauvy7g-1702230335-0-AQUqJAyTbHZH5p8kx5EDm88TJsHkUawzHp/eryjQnHpeqwYj02E8Wn88FNKQ/HR6VFZ7+vsQEBZRSpuTExXPWjc=; path=/; expires=Sun, 10-Dec-23 18:15:35 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=SYdQLkPLU0v_W0vp230dnTWyIkGbZaDjUYpxYA9mh2o-1702230335709-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83374d6d6f85425f-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-10 12:45:35,712 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-10 12:45:35,712 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-10 12:45:35,714 [DEBUG]: receive_response_body.complete
2023-12-10 12:45:35,714 [DEBUG]: response_closed.started
2023-12-10 12:45:35,714 [DEBUG]: response_closed.complete
2023-12-10 12:45:35,715 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-10 12:45:35,720 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-10 12:45:35,722 [DEBUG]: load_verify_locations cafile='/usr/local/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-10 12:45:35,754 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x12fdd9120>, 'json_data': {'input': 'CPSC 488/588 - AI Foundation Models HW 1 Page 5 (b) What is the cost of computation in terms of number of FLoating point Operations for multi-head attention in the backward pass, when the model is being trained? (use the same assumptions as the above questions). Solution: 1. Projection to Q, K, V: 6bmd2 2. Scaled Dot Product Attention: 2bm2d 3. Applying Softmax: bm2h 4. Computing the weighted average: 2bm2d 5. Output projection: 4bm2d Total Backward FLOPs: 6bmd2+ 9bm2d+bm2h (c) What is the cost of computation in terms of number of FLoating point Operations for Grouped Query Attention where G=k/4 is the number of groups? Solution: 1. Projection to Q, K, V: 3bmd2 2. Grouping Queries and Dot Product:bm2d 2 3. Applying Softmax: bm2h 4. Computing the weighted average:bm2d 2 Total FLOPs for Grouped Query Attention: 4bmd2+bm2h', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-10 12:45:35,799 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-10 12:45:35,812 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x12fde0dd0>
2023-12-10 12:45:35,812 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x1179f2210> server_hostname='api.openai.com' timeout=5.0
2023-12-10 12:45:35,827 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x12fde0d10>
2023-12-10 12:45:35,827 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-10 12:45:35,827 [DEBUG]: send_request_headers.complete
2023-12-10 12:45:35,827 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-10 12:45:35,827 [DEBUG]: send_request_body.complete
2023-12-10 12:45:35,827 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-10 12:45:35,968 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 10 Dec 2023 17:45:35 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'26'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999793'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'2ms'), (b'x-request-id', b'968a036761e6e98de7819d891a87c2b5'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=.tOeM7Jlcr5vK4OiXvOJLykOKIldGMUa.Rb4UPUIbAI-1702230335-0-ASrActxEnGKxRg8uEPA6s+wFr133EmMh2z7XyIsQw04ZmbM3y99Rws5hDBQJaW8m079GdL8LlZSTZt1ksj1+uVU=; path=/; expires=Sun, 10-Dec-23 18:15:35 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=OxR_xLaHDrsRfITgbI2.P0GQdsbm1RFwZE_aRPLtXNE-1702230335968-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83374d6efcee1760-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-10 12:45:35,970 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-10 12:45:35,970 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-10 12:45:35,978 [DEBUG]: receive_response_body.complete
2023-12-10 12:45:35,978 [DEBUG]: response_closed.started
2023-12-10 12:45:35,979 [DEBUG]: response_closed.complete
2023-12-10 12:45:35,980 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-10 12:45:35,982 [DEBUG]: Embedded chunks
2023-12-10 12:45:36,612 [DEBUG]: Upserted vectors into index
2023-12-10 12:45:36,613 [DEBUG]: Error in upload_and_generate_embedding: 'str' object has no attribute 'filename'
2023-12-10 12:45:36,618 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:45:36] "[35m[1mPOST /api/generate_embeddings HTTP/1.1[0m" 500 -
2023-12-10 12:46:55,773 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-10 12:46:55,774 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-10 12:47:03,228 [DEBUG]: <FileStorage: 'AI_Foundations_Pset1_Final.pdf' ('application/pdf')>
2023-12-10 12:47:03,228 [DEBUG]: files
2023-12-10 12:47:03,230 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:47:03] "POST /api/uploadFiles HTTP/1.1" 200 -
2023-12-10 12:47:03,240 [DEBUG]: Using selector: KqueueSelector
2023-12-10 12:47:03,345 [DEBUG]: Initialized Pinecone
2023-12-10 12:47:03,345 [DEBUG]: Type of pdf_file: <class 'str'>
2023-12-10 12:47:03,452 [DEBUG]: Parsed PDF
2023-12-10 12:47:03,648 [DEBUG]: List of indexes: ['research-chat-index']
2023-12-10 12:47:03,649 [DEBUG]: Initialized Pinecone Index
2023-12-10 12:47:03,650 [DEBUG]: Chunked PDF and obtained vectors
2023-12-10 12:47:03,651 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-10 12:47:03,651 [DEBUG]: load_verify_locations cafile='/usr/local/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-10 12:47:03,669 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x11c152020>, 'json_data': {'input': 'CPSC 488/588 - AI Foundation Models HW 1 Page 1 CPSC 488/588 AI Foundation Models Fall 2023 HW 1 Instructions: copy this project at https://www.overleaf.com/read/gjvcgrzjyxfm , complete the solutions, and return your solutions in pdf format. Full Name: Sneha Netid: ss3993 1.Q1 Warmup. this part is about refreshing your calculus on calculating derivatives of functions. (a) Given the function: f(x) = sin(3 x2+ 4 cos( x)) findâˆ‚f/âˆ‚x using the chain rule. Solution:âˆ‚f âˆ‚x= cos(3 x2+ 4 cos( x))Â·(6xâˆ’4 sin( x)) (b) Given the function: f(x) =etanh(2 x3âˆ’5x2+x) findâˆ‚f/âˆ‚x using the chain rule. Recall:âˆ‚tanh ( x) âˆ‚(x)= 1âˆ’tanh2(x) = sech2(x) Solution:âˆ‚f âˆ‚x=etanh(2 x3âˆ’5x2+x)Â·sech2(2x3âˆ’5x2+x)Â·\x00 6x2âˆ’10x+ 1\x01 (c) Consider the function: f(x, y, z ) =x2sin(yz) +eyzâˆ’z3y find partial derivatives of the function with respect to each variable separately. Solution:âˆ‚f âˆ‚x= 2xsin(yz), âˆ‚f âˆ‚y=x2zcos(yz) +zeyzâˆ’z3, âˆ‚f âˆ‚z=x2ycos(yz) +yeyzâˆ’3z2y. 2.Q2 Matrix calculus. Recall that matrices are a way of organizing data into rows and columns. Answer the following questions: (a) Consider a function given by: f(x) =cTAx (1) where x=\uf8ee \uf8f0x0 x1 x2\uf8f9 \uf8fb', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-10 12:47:03,761 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-10 12:47:03,849 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x11c109ad0>
2023-12-10 12:47:03,849 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x11c041490> server_hostname='api.openai.com' timeout=5.0
2023-12-10 12:47:03,864 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10f7fff90>
2023-12-10 12:47:03,865 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-10 12:47:03,865 [DEBUG]: send_request_headers.complete
2023-12-10 12:47:03,865 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-10 12:47:03,865 [DEBUG]: send_request_body.complete
2023-12-10 12:47:03,865 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-10 12:47:04,033 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 10 Dec 2023 17:47:04 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'30'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999704'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'3ms'), (b'x-request-id', b'8b22b74315eacd3070144331df932741'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=gXA1oKUFVknOvtYcbZd9Iy9e.ObmR9w_o1DS6FdN3hQ-1702230424-0-AXsOFdM4w7iKZM7BOIPic/TB1XGLxsA/RDeSUd0GLITE2i+BrbgmGrOotLmWOw8GssOio0vSpTylgkOTwliy9nk=; path=/; expires=Sun, 10-Dec-23 18:17:04 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=LorGRVrU5Z04iaGgvtdH1zVaRqnQDh_xRJBp.9zo1ok-1702230424033-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83374f953cbe726b-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-10 12:47:04,035 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-10 12:47:04,035 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-10 12:47:04,036 [DEBUG]: receive_response_body.complete
2023-12-10 12:47:04,036 [DEBUG]: response_closed.started
2023-12-10 12:47:04,036 [DEBUG]: response_closed.complete
2023-12-10 12:47:04,036 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-10 12:47:04,039 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-10 12:47:04,040 [DEBUG]: load_verify_locations cafile='/usr/local/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-10 12:47:04,058 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x11c194220>, 'json_data': {'input': 'CPSC 488/588 - AI Foundation Models HW 1 Page 2 is a column vector of variables, c=\x022 3 1\x03 is a constant vector, and A=\uf8ee \uf8f0a00a01a02 a10a11a12 a20a21a22\uf8f9 \uf8fb is a integer valued matrix, Derive the gradient of fwith respect to x. Solution: Given the function f(x) =\x022 3 1\x03\uf8ee \uf8f0a00a01a02 a10a11a12 a20a21a22\uf8f9 \uf8fb\uf8ee \uf8f0x0 x1 x2\uf8f9 \uf8fb we can expand Axas: Ax=\uf8ee \uf8f0a00x0+a01x1+a02x2 a10x0+a11x1+a12x2 a20x0+a21x1+a22x2\uf8f9 \uf8fb The function f(x) expands to: f(x) = 2a00x0+ 3a10x0+a20x0+ 2a01x1+ 3a11x1+a21x1+ 2a02x2+ 3a12x2+a22x2 Differentiating with respect to the components of x, we get: âˆ‚f(x) âˆ‚x0= 2a00+ 3a10+a20 âˆ‚f(x) âˆ‚x1= 2a01+ 3a11+a21 âˆ‚f(x) âˆ‚x2= 2a02+ 3a12+a22 (b) Given the function f=xâŠ¤Â·AÂ·x+cÂ·sin(y)âŠ¤Â·x where â€¢Ais a symmetric matrix â€¢cis a scalar â€¢xis a vector â€¢yis a vector Derive the gradients with respect to xandy. Solution: Gradient with respect to x: Using the identity âˆ‡x(xTAx) = (A+AT)xand we also know that Ais symmetric (i.e., A=AT), so the derivative can be simplified to 2 Ax.', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-10 12:47:04,096 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-10 12:47:04,175 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x11c1309d0>
2023-12-10 12:47:04,175 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x11c0412e0> server_hostname='api.openai.com' timeout=5.0
2023-12-10 12:47:04,191 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x11c1308d0>
2023-12-10 12:47:04,192 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-10 12:47:04,192 [DEBUG]: send_request_headers.complete
2023-12-10 12:47:04,192 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-10 12:47:04,192 [DEBUG]: send_request_body.complete
2023-12-10 12:47:04,192 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-10 12:47:04,334 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 10 Dec 2023 17:47:04 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'29'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999742'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'3ms'), (b'x-request-id', b'f0c0e3604604f4c85c6d0f5f11dbe19f'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=v544NM6e4_0ksbC7_68RS_OUcbkunlhqB2XI0D590Kc-1702230424-0-AaOEvrX0J/v8+dyxDbeHorjEHr3ajC9KYKa7gOKejKdJMICHwu8Z5kKdL/6EJ6x/3lgAJIf5Yw3Zof4aNBA/H4Q=; path=/; expires=Sun, 10-Dec-23 18:17:04 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=qXMRnxVASl2o7vcr94uEsY1OdXA4JgzAT7wRH8Msa.k-1702230424332-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83374f974d3441c1-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-10 12:47:04,338 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-10 12:47:04,338 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-10 12:47:04,341 [DEBUG]: receive_response_body.complete
2023-12-10 12:47:04,341 [DEBUG]: response_closed.started
2023-12-10 12:47:04,341 [DEBUG]: response_closed.complete
2023-12-10 12:47:04,342 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-10 12:47:04,351 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-10 12:47:04,353 [DEBUG]: load_verify_locations cafile='/usr/local/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-10 12:47:04,375 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x11c1af380>, 'json_data': {'input': 'CPSC 488/588 - AI Foundation Models HW 1 Page 3 âˆ‚f âˆ‚x= 2Ax+csin(y) âˆ‚f âˆ‚y=cxcos(y) (c) Given the function g=xâŠ¤By+dtanh(z)âŠ¤x where â€¢Bis an arbitrary matrix â€¢dis a scalar â€¢xis a vector â€¢yis a vector of the same dimension as x â€¢zis a vector Derive the gradients with respect to x,y, and z. Hint: recallâˆ‚tanh( x) âˆ‚(x)= 1âˆ’tanh2(x) = sech2(x) Solution: 1. Gradient with respect to x: âˆ‚g âˆ‚x=By+dtanh( z) 2. Gradient with respect to y: âˆ‚g âˆ‚y=BTx 3. Gradient with respect to z: âˆ‚g âˆ‚z=dÂ·sech2(z)Â·x 3.Q3 Automatic differentiation (a) Consider the following function: f(x1, x2, x3, x4) =1 2exp(x1+x2 2)âˆ’(x3âˆ—x2 4) i. draw the computation graph corresponding to this function and fill in the gradient values for all of the intermediate nodes and the leaves in the computation graph. Assume the values for x1, x2, x3, x4areâˆ’1,2,4,âˆ’3 respectively. ii.Solution: iii. Derive the gradients of fwith respect to its inputs âˆ‡fusing symbolic differentiation and chain rule.', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-10 12:47:04,415 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-10 12:47:04,427 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x11c1b5550>
2023-12-10 12:47:04,427 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x11c041760> server_hostname='api.openai.com' timeout=5.0
2023-12-10 12:47:04,440 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x11c1b54d0>
2023-12-10 12:47:04,440 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-10 12:47:04,441 [DEBUG]: send_request_headers.complete
2023-12-10 12:47:04,441 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-10 12:47:04,441 [DEBUG]: send_request_body.complete
2023-12-10 12:47:04,441 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-10 12:47:04,587 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 10 Dec 2023 17:47:04 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'24'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999750'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'3ms'), (b'x-request-id', b'5f32c659d1d5df86ab675afd008332d3'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=QDo.sfgyY.1bxIhRtuxBD4GV_NNM2fIpmQSD7W4i5ds-1702230424-1-AcQC4F0kmsoohypHelKC+FxIDARDfOp8zMw1Hm4kTbu8a4h2kXH09tRsv2j2qzoUgZwwbG6ULFgutklpW3qPW0k=; path=/; expires=Sun, 10-Dec-23 18:17:04 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=aDB4dLJ_HBc5i1yfWD9Ljn8aoDIYdNWZ2psZ12ioqMw-1702230424586-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83374f98d9a441ed-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-10 12:47:04,589 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-10 12:47:04,590 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-10 12:47:04,591 [DEBUG]: receive_response_body.complete
2023-12-10 12:47:04,591 [DEBUG]: response_closed.started
2023-12-10 12:47:04,591 [DEBUG]: response_closed.complete
2023-12-10 12:47:04,591 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-10 12:47:04,594 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-10 12:47:04,596 [DEBUG]: load_verify_locations cafile='/usr/local/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-10 12:47:04,621 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x11c1e2660>, 'json_data': {'input': 'CPSC 488/588 - AI Foundation Models HW 1 Page 4 Solution: âˆ‚f âˆ‚x1=1 2exp(x1+x2 2) = 1/2âˆ—exp(3) = exp(3)/2 âˆ‚f âˆ‚x2=x2exp(x1+x2 2) = 2âˆ—exp(3) âˆ‚f âˆ‚x3=âˆ’x2 4=âˆ’9 âˆ‚f âˆ‚x4=âˆ’2x3x4= 24 4.Q4Explain the reason behind using negative sampling in the SkipGram word embeddings model. Solution: In a naive implementation of the SkipGram model, for each training example, we would update weights for all words in the vocabulary using softmax. By using negative sampling, instead of computing the softmax over all words in the vocabulary, we only need to compute it for the actual context word and a small set of negative samples. This greatly reduces the computational burden.Furthermore, negative sampling indirectly also sends implicit negative information which as a result the model implicitly gains information about what words are unlikely to appear in the context improving word vector quality. 5.Q5 transformer (a) In the multi-head self-attention operation, what is the cost of computation (in terms of number of number of FLoating point OPerations)? Assume bis batch size, mis sequence length, dis the model dimension, and his the number of attention heads. Assume the dimensionality of keys and queries are d/2h. Note: You need to derive the answer, just providing the final answer is not sufficient. Solution: Scaled Dot Product Attention: Dot product for one head: b Ã—mÃ—mÃ—d 2h Forhheads: bÃ—mÃ—mÃ—d=bm2d Applying Softmax: Softmax operations: b Ã—mÃ—mÃ—hâ‰ˆbm2h Computing the weighted average: For one head: b Ã—mÃ—mÃ—d', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-10 12:47:04,662 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-10 12:47:04,700 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x11c1ded90>
2023-12-10 12:47:04,700 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x11c041a30> server_hostname='api.openai.com' timeout=5.0
2023-12-10 12:47:04,715 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x11c1ded10>
2023-12-10 12:47:04,715 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-10 12:47:04,716 [DEBUG]: send_request_headers.complete
2023-12-10 12:47:04,716 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-10 12:47:04,716 [DEBUG]: send_request_body.complete
2023-12-10 12:47:04,716 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-10 12:47:04,865 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 10 Dec 2023 17:47:04 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'26'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999616'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'6fe6181a63961b98de568e230e6f5803'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=AAc11LpvhlknyaSjzPJdayV5bB33158kS7N0rU.KIzA-1702230424-1-AeNwfR0fJuRttrSXYGWcw8EFMS/WFWKSmOaIaRhPFjbG+aqeW4qZ/LeAgORMHqWByhki4voMUOpKrJQU3uW7Nv8=; path=/; expires=Sun, 10-Dec-23 18:17:04 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=58h0eK2DcqLHVrnoQAp6OOg19Pbyo9L0Sk_AK8j8rck-1702230424864-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83374f9a8b998ccc-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-10 12:47:04,867 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-10 12:47:04,868 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-10 12:47:04,869 [DEBUG]: receive_response_body.complete
2023-12-10 12:47:04,870 [DEBUG]: response_closed.started
2023-12-10 12:47:04,870 [DEBUG]: response_closed.complete
2023-12-10 12:47:04,870 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-10 12:47:04,875 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-10 12:47:04,877 [DEBUG]: load_verify_locations cafile='/usr/local/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-10 12:47:04,901 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1343f9ee0>, 'json_data': {'input': 'For one head: b Ã—mÃ—mÃ—d 2h Forhheads: bÃ—mÃ—mÃ—d=bm2d Output projection: Output operations: 2 Ã—bÃ—mÃ—dÃ—d= 2bm2d Total FLOPs: Total operations: 3bmd2+bm2d+bm2h+bm2d+ 2bm2d= 4bmd2+ 3bm2d+bm2h', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-10 12:47:04,942 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-10 12:47:04,956 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1343fda50>
2023-12-10 12:47:04,956 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x11c041e20> server_hostname='api.openai.com' timeout=5.0
2023-12-10 12:47:04,971 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1343fd9d0>
2023-12-10 12:47:04,971 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-10 12:47:04,971 [DEBUG]: send_request_headers.complete
2023-12-10 12:47:04,971 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-10 12:47:04,971 [DEBUG]: send_request_body.complete
2023-12-10 12:47:04,971 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-10 12:47:05,217 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 10 Dec 2023 17:47:05 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'46'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999952'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'dedec976628f8937ae719babb84854ce'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=313TNBRtFiFFcpXVxbJ_O3kKVWM.0ke0GRv0tnaLgVI-1702230425-0-ARTCZCNIDmyc7DR0YkFPkMWWDhTHFuc7Kcp5k31B/v0XaGFXr0I/tJU2vAP7XULaeYZV8svnPpdksv/auQndarg=; path=/; expires=Sun, 10-Dec-23 18:17:05 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=ObtMLo1P5rs2DU23rUCPK70bxyf7TH.FALKtOHkTnuE-1702230425126-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83374f9c2b4b0f7d-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-10 12:47:05,219 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-10 12:47:05,219 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-10 12:47:05,225 [DEBUG]: receive_response_body.complete
2023-12-10 12:47:05,225 [DEBUG]: response_closed.started
2023-12-10 12:47:05,225 [DEBUG]: response_closed.complete
2023-12-10 12:47:05,225 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-10 12:47:05,229 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-10 12:47:05,230 [DEBUG]: load_verify_locations cafile='/usr/local/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-10 12:47:05,263 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x13442cfe0>, 'json_data': {'input': 'CPSC 488/588 - AI Foundation Models HW 1 Page 5 (b) What is the cost of computation in terms of number of FLoating point Operations for multi-head attention in the backward pass, when the model is being trained? (use the same assumptions as the above questions). Solution: 1. Projection to Q, K, V: 6bmd2 2. Scaled Dot Product Attention: 2bm2d 3. Applying Softmax: bm2h 4. Computing the weighted average: 2bm2d 5. Output projection: 4bm2d Total Backward FLOPs: 6bmd2+ 9bm2d+bm2h (c) What is the cost of computation in terms of number of FLoating point Operations for Grouped Query Attention where G=k/4 is the number of groups? Solution: 1. Projection to Q, K, V: 3bmd2 2. Grouping Queries and Dot Product:bm2d 2 3. Applying Softmax: bm2h 4. Computing the weighted average:bm2d 2 Total FLOPs for Grouped Query Attention: 4bmd2+bm2h', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-10 12:47:05,303 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-10 12:47:05,312 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x134431550>
2023-12-10 12:47:05,313 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x11c042210> server_hostname='api.openai.com' timeout=5.0
2023-12-10 12:47:05,325 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1344314d0>
2023-12-10 12:47:05,325 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-10 12:47:05,325 [DEBUG]: send_request_headers.complete
2023-12-10 12:47:05,325 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-10 12:47:05,325 [DEBUG]: send_request_body.complete
2023-12-10 12:47:05,325 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-10 12:47:05,502 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 10 Dec 2023 17:47:05 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'27'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999793'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'2ms'), (b'x-request-id', b'b7335a0ea5e8457a78509b14ab752b09'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=CJVBbq37oFiqUjbvap7C6Z7BDTdUDCigIPNZVLDt6ow-1702230425-0-AXJBIVWkduBDLubo2RR0aHHj/1VCN1HPG35PXbxjPB3RmZNpEUsCr+OA8UkfMokC4bEqY7WKtYr1/gTXHKNCwfU=; path=/; expires=Sun, 10-Dec-23 18:17:05 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=x7FUQvOtSbA._FiSuvCUF0w6cYSLyzHY8F8oKRelE6k-1702230425501-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83374f9e5c30c409-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-10 12:47:05,503 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-10 12:47:05,504 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-10 12:47:05,504 [DEBUG]: receive_response_body.complete
2023-12-10 12:47:05,505 [DEBUG]: response_closed.started
2023-12-10 12:47:05,505 [DEBUG]: response_closed.complete
2023-12-10 12:47:05,505 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-10 12:47:05,507 [DEBUG]: Embedded chunks
2023-12-10 12:47:05,929 [DEBUG]: Upserted vectors into index
2023-12-10 12:47:05,932 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:47:05] "POST /api/generate_embeddings HTTP/1.1" 200 -
2023-12-10 12:53:25,498 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:53:25] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-10 12:53:40,252 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:53:40] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-10 12:53:45,566 [DEBUG]: Using selector: KqueueSelector
2023-12-10 12:53:45,572 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-10 12:53:45,575 [DEBUG]: load_verify_locations cafile='/usr/local/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-10 12:53:45,601 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:53:45] "[35m[1mPOST /api/chat HTTP/1.1[0m" 500 -
2023-12-10 12:55:08,242 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-10 12:55:08,242 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-10 12:55:17,539 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:55:17] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-10 12:55:24,781 [DEBUG]: Using selector: KqueueSelector
2023-12-10 12:55:24,785 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-10 12:55:24,787 [DEBUG]: load_verify_locations cafile='/usr/local/lib/python3.11/site-packages/certifi/cacert.pem'
data
2023-12-10 12:55:24,811 [DEBUG]: data
2023-12-10 12:55:24,813 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:55:24] "[35m[1mPOST /api/chat HTTP/1.1[0m" 500 -
2023-12-10 12:55:59,853 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-10 12:55:59,853 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-10 12:58:12,212 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:58:12] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-10 12:58:16,808 [DEBUG]: Using selector: KqueueSelector
2023-12-10 12:58:16,818 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-10 12:58:16,820 [DEBUG]: load_verify_locations cafile='/usr/local/lib/python3.11/site-packages/certifi/cacert.pem'
data
2023-12-10 12:58:16,845 [DEBUG]: data
2023-12-10 12:58:16,853 [INFO]: 127.0.0.1 - - [10/Dec/2023 12:58:16] "[35m[1mPOST /api/chat HTTP/1.1[0m" 500 -
2023-12-10 12:59:59,016 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-10 12:59:59,016 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-10 13:00:13,746 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-10 13:00:13,746 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-10 13:00:27,562 [INFO]: 127.0.0.1 - - [10/Dec/2023 13:00:27] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-10 13:00:33,913 [DEBUG]: Using selector: KqueueSelector
2023-12-10 13:00:33,918 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-10 13:00:33,920 [DEBUG]: load_verify_locations cafile='/usr/local/lib/python3.11/site-packages/certifi/cacert.pem'
[{'role': 'user', 'content': 'Compare and contrast the abstracts of the documents I uploaded.'}]
2023-12-10 13:00:33,945 [DEBUG]: [{'role': 'user', 'content': 'Compare and contrast the abstracts of the documents I uploaded.'}]
{}
2023-12-10 13:00:33,945 [DEBUG]: {}
2023-12-10 13:00:33,946 [INFO]: 127.0.0.1 - - [10/Dec/2023 13:00:33] "[35m[1mPOST /api/chat HTTP/1.1[0m" 500 -
2023-12-10 13:01:15,339 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-10 13:01:15,339 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-10 13:01:23,511 [INFO]: 127.0.0.1 - - [10/Dec/2023 13:01:23] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-10 13:01:26,950 [DEBUG]: Using selector: KqueueSelector
2023-12-10 13:01:26,955 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-10 13:01:26,957 [DEBUG]: load_verify_locations cafile='/usr/local/lib/python3.11/site-packages/certifi/cacert.pem'
[{'role': 'user', 'content': 'Compare and contrast the abstracts of the documents I uploaded.'}]
2023-12-10 13:01:26,984 [DEBUG]: [{'role': 'user', 'content': 'Compare and contrast the abstracts of the documents I uploaded.'}]
2023-12-10 13:01:26,985 [INFO]: 127.0.0.1 - - [10/Dec/2023 13:01:26] "[35m[1mPOST /api/chat HTTP/1.1[0m" 500 -
2023-12-10 13:09:21,129 [INFO]: 127.0.0.1 - - [10/Dec/2023 13:09:21] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-10 13:13:18,658 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-10 13:13:18,659 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-10 13:13:24,479 [INFO]: 127.0.0.1 - - [10/Dec/2023 13:13:24] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-10 13:13:29,838 [DEBUG]: Using selector: KqueueSelector
2023-12-10 13:13:29,842 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-10 13:13:29,844 [DEBUG]: load_verify_locations cafile='/usr/local/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-10 13:13:29,870 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-10 13:13:29,870 [DEBUG]: load_verify_locations cafile='/usr/local/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-10 13:13:29,887 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x119548220>, 'json_data': {'input': 'Summarize the comments of my first document.', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-10 13:13:29,985 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-10 13:13:30,021 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1194fd650>
2023-12-10 13:13:30,022 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x1194f07a0> server_hostname='api.openai.com' timeout=5.0
2023-12-10 13:13:30,036 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x11955f110>
2023-12-10 13:13:30,037 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-10 13:13:30,037 [DEBUG]: send_request_headers.complete
2023-12-10 13:13:30,037 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-10 13:13:30,037 [DEBUG]: send_request_body.complete
2023-12-10 13:13:30,037 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-10 13:13:30,311 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 10 Dec 2023 18:13:30 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'22'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999988'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'71e4db8e598781e38a6789dfd42c3e4f'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=kIiWVbdZRWXpFsQmb9D2oxzYmDUM7MoXg.XJiAW_tSg-1702232010-0-AYv2HKBboacjwSmbArAlXycF2RS5NoLUe0QI1AZZjGbga62UPpOcZlR6ZUzxjsPqjdKR5FFliCPheQSsyaR8j58=; path=/; expires=Sun, 10-Dec-23 18:43:30 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=vgUp1WL8EZ4mSL33E9TFyN4Qe.FsiKvMWj8CcQbw2Nc-1702232010305-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8337764ec82b4231-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-10 13:13:30,314 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-10 13:13:30,315 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-10 13:13:30,319 [DEBUG]: receive_response_body.complete
2023-12-10 13:13:30,319 [DEBUG]: response_closed.started
2023-12-10 13:13:30,320 [DEBUG]: response_closed.complete
2023-12-10 13:13:30,320 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-10 13:13:31,459 [DEBUG]: Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'AI assistant is a brand new, powerful, human-like artificial intelligence.\n                    The traits of AI include expert knowledge, helpfulness, cleverness, and articulateness.\n                    AI is a well-behaved and well-mannered individual.\n                    AI is always friendly, kind, and inspiring, and he is eager to provide vivid and thoughtful responses to the user.\n                    AI has the sum of all knowledge in their brain, and is able to accurately answer nearly any question about any topic in conversation.\n                    AI assistant is a big fan of Pinecone and Vercel.\n                    START CONTEXT BLOCK\n                    CPSC 488/588 - AI Foundation Models HW 1 Page 1\nCPSC 488/588 AI Foundation Models\nFall 2023\nHW 1\nInstructions: copy this project at https://www.overleaf.com/read/gjvcgrzjyxfm ,\ncomplete the solutions, and return your solutions in pdf format.\nFull Name: Sneha\nNetid: ss3993\n1.Q1\nWarmup. this part is about refreshing your calculus on calculating derivatives of functions.\n(a) Given the function:\nf(x) = sin(3 x2+ 4 cos( x))\nfindâˆ‚f/âˆ‚x using the chain rule.\nSolution:âˆ‚f\nâˆ‚x= cos(3 x2+ 4 cos( x))Â·(6xâˆ’4 sin( x))\n(b) Given the function:\nf(x) =etanh(2 x3âˆ’5x2+x)\nfindâˆ‚f/âˆ‚x using the chain rule.\nRecall:âˆ‚tanh ( x)\nâˆ‚(x)= 1âˆ’tanh2(x) = sech2(x)\nSolution:âˆ‚f\nâˆ‚x=etanh(2 x3âˆ’5x2+x)Â·sech2(2x3âˆ’5x2+x)Â·\x00\n6x2âˆ’10x+ 1\x01\n(c) Consider the function:\nf(x, y, z ) =x2sin(yz) +eyzâˆ’z3y\nfind partial derivatives of the function with respect to each variable separately.\nSolution:âˆ‚f\nâˆ‚x= 2xsin(yz),\nâˆ‚f\nâˆ‚y=x2zcos(yz) +zeyzâˆ’z3,\nâˆ‚f\nâˆ‚z=x2ycos(yz) +yeyzâˆ’3z2y.\n2.Q2 Matrix calculus. Recall that matrices are a way of organizing data into rows and columns.\nAnswer the following questions:\n(a) Consider a function given by:\nf(x) =cTAx (1)\nwhere\nx=\uf8ee\n\uf8f0x0\nx1\nx2\uf8f9\n\uf8fb\nCPSC 488/588 - AI Foundation Models HW 1 Page 4\nSolution:\nâˆ‚f\nâˆ‚x1=1\n2exp(x1+x2\n2) = 1/2âˆ—exp(3) = exp(3)/2\nâˆ‚f\nâˆ‚x2=x2exp(x1+x2\n2) = 2âˆ—exp(3)\nâˆ‚f\nâˆ‚x3=âˆ’x2\n4=âˆ’9\nâˆ‚f\nâˆ‚x4=âˆ’2x3x4= 24\n4.Q4Explain the reason behind using negative sampling in the SkipGram word embeddings model.\nSolution: In a naive implementation of the SkipGram model, for each training example, we would\nupdate weights for all words in the vocabulary using softmax. By using negative sampling, instead\nof computing the softmax over all words in the vocabulary, we only need to compute it for the\nactual context word and a small set of negative samples. This greatly reduces the computational\nburden.Furthermore, negative sampling indirectly also sends implicit negative information which\nas a result the model implicitly gains information about what words are unlikely to appear in the\ncontext improving word vector quality.\n5.Q5 transformer\n(a) In the multi-head self-attention operation, what is the cost of computation (in terms of number\nof number of FLoating point OPerations)? Assume bis batch size, mis sequence length, dis the\nmodel dimension, and his the number of attention heads. Assume the dimensionality of keys and\nqueries are d/2h.\nNote: You need to derive the answer, just providing the final answer is not sufficient.\nSolution:\nScaled Dot Product Attention:\nDot product for one head: b Ã—mÃ—mÃ—d\n2h\nForhheads: bÃ—mÃ—mÃ—d=bm2d\nApplying Softmax:\nSoftmax operations: b Ã—mÃ—mÃ—hâ‰ˆbm2h\nComputing the weighted average:\nFor one head: b Ã—mÃ—mÃ—d\nCPSC 488/588 - AI Foundation Models HW 1 Page 5\n(b) What is the cost of computation in terms of number of FLoating point Operations for multi-head\nattention in the backward pass, when the model is being trained? (use the same assumptions as\nthe above questions).\nSolution:\n1. Projection to Q, K, V: 6bmd2\n2. Scaled Dot Product Attention: 2bm2d\n3. Applying Softmax: bm2h\n4. Comput\n                    END OF CONTEXT BLOCK\n                    AI assistant will take into account any CONTEXT BLOCK that is provided in a conversation.\n                    If the context does not provide the answer to question, the AI assistant will say, "I\'m sorry, but I don\'t know the answer to that question".\n                    AI assistant will not apologize for previous responses, but instead will indicated new information was gained.\n                    AI assistant will not invent anything that is not drawn directly from the context.\n                    '}, {'role': 'user', 'content': 'Summarize the comments of my first document.'}], 'model': 'gpt-3.5-turbo'}}
2023-12-10 13:13:31,505 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-10 13:13:31,515 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1195a0d50>
2023-12-10 13:13:31,516 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x1194f0710> server_hostname='api.openai.com' timeout=5.0
2023-12-10 13:13:31,528 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1195784d0>
2023-12-10 13:13:31,529 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-10 13:13:31,529 [DEBUG]: send_request_headers.complete
2023-12-10 13:13:31,529 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-10 13:13:31,529 [DEBUG]: send_request_body.complete
2023-12-10 13:13:31,529 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-10 13:13:33,509 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 10 Dec 2023 18:13:33 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'1520'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'160000'), (b'x-ratelimit-limit-tokens_usage_based', b'160000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'158885'), (b'x-ratelimit-remaining-tokens_usage_based', b'158885'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'418ms'), (b'x-ratelimit-reset-tokens_usage_based', b'418ms'), (b'x-request-id', b'183dc85197cedf81bfbefb40ef5368bd'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=OcrEea.9TUQ9KN8nmeNKt_bCStCDJu2l06EqVouC5nc-1702232013-0-ATMceuhzx3ps8A6pwLiYMMg7+IFn0iQXUJJauNzmnyf43IGQMLAE5HgeO9dIYiGxf9J+rqALLDlFfAmJivkQjCM=; path=/; expires=Sun, 10-Dec-23 18:43:33 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=yPzPAacewPDiT67KstMkBUlvOoNX2pw1F6.XOsM0Avw-1702232013505-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'833776581aec8cb4-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-10 13:13:33,510 [INFO]: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2023-12-10 13:13:33,510 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-10 13:13:33,511 [DEBUG]: receive_response_body.complete
2023-12-10 13:13:33,511 [DEBUG]: response_closed.started
2023-12-10 13:13:33,511 [DEBUG]: response_closed.complete
2023-12-10 13:13:33,511 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2023-12-10 13:13:33,514 [INFO]: 127.0.0.1 - - [10/Dec/2023 13:13:33] "POST /api/chat HTTP/1.1" 200 -
2023-12-10 13:13:54,815 [INFO]: 127.0.0.1 - - [10/Dec/2023 13:13:54] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-10 13:14:04,686 [DEBUG]: Using selector: KqueueSelector
2023-12-10 13:14:04,691 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-10 13:14:04,693 [DEBUG]: load_verify_locations cafile='/usr/local/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-10 13:14:04,722 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-10 13:14:04,723 [DEBUG]: load_verify_locations cafile='/usr/local/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-10 13:14:04,738 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1195e1f80>, 'json_data': {'input': 'what was the first uestion in AI foundations pset? ', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-10 13:14:04,779 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-10 13:14:04,792 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1195e4cd0>
2023-12-10 13:14:04,792 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x1194f0cb0> server_hostname='api.openai.com' timeout=5.0
2023-12-10 13:14:04,804 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1195e4c90>
2023-12-10 13:14:04,805 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-10 13:14:04,805 [DEBUG]: send_request_headers.complete
2023-12-10 13:14:04,805 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-10 13:14:04,805 [DEBUG]: send_request_body.complete
2023-12-10 13:14:04,805 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-10 13:14:04,936 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 10 Dec 2023 18:14:04 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'19'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999987'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'87d73a111e6762449edaa5f982459a13'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=r.wqxpQk01xRFG5XammCJ3stQOcPlnOaxJvVwYsd3mI-1702232044-0-AbOT3hkm5cFTUfO09MoKfY81+PLaNgLgTEP6/HdM3nIaMTx3mp/4U0a6yQZy6i4uvvN5BLAuqOkjA7hDhdExgx4=; path=/; expires=Sun, 10-Dec-23 18:44:04 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=7t812fKwss2ksvd1PiDwNbYZCBsPPI__m7xCuZ2IIR8-1702232044930-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8337772808c6c3f8-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-10 13:14:04,937 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-10 13:14:04,937 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-10 13:14:04,947 [DEBUG]: receive_response_body.complete
2023-12-10 13:14:04,947 [DEBUG]: response_closed.started
2023-12-10 13:14:04,947 [DEBUG]: response_closed.complete
2023-12-10 13:14:04,947 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-10 13:14:05,433 [DEBUG]: Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'AI assistant is a brand new, powerful, human-like artificial intelligence.\n                    The traits of AI include expert knowledge, helpfulness, cleverness, and articulateness.\n                    AI is a well-behaved and well-mannered individual.\n                    AI is always friendly, kind, and inspiring, and he is eager to provide vivid and thoughtful responses to the user.\n                    AI has the sum of all knowledge in their brain, and is able to accurately answer nearly any question about any topic in conversation.\n                    AI assistant is a big fan of Pinecone and Vercel.\n                    START CONTEXT BLOCK\n                    CPSC 488/588 - AI Foundation Models HW 1 Page 1\nCPSC 488/588 AI Foundation Models\nFall 2023\nHW 1\nInstructions: copy this project at https://www.overleaf.com/read/gjvcgrzjyxfm ,\ncomplete the solutions, and return your solutions in pdf format.\nFull Name: Sneha\nNetid: ss3993\n1.Q1\nWarmup. this part is about refreshing your calculus on calculating derivatives of functions.\n(a) Given the function:\nf(x) = sin(3 x2+ 4 cos( x))\nfindâˆ‚f/âˆ‚x using the chain rule.\nSolution:âˆ‚f\nâˆ‚x= cos(3 x2+ 4 cos( x))Â·(6xâˆ’4 sin( x))\n(b) Given the function:\nf(x) =etanh(2 x3âˆ’5x2+x)\nfindâˆ‚f/âˆ‚x using the chain rule.\nRecall:âˆ‚tanh ( x)\nâˆ‚(x)= 1âˆ’tanh2(x) = sech2(x)\nSolution:âˆ‚f\nâˆ‚x=etanh(2 x3âˆ’5x2+x)Â·sech2(2x3âˆ’5x2+x)Â·\x00\n6x2âˆ’10x+ 1\x01\n(c) Consider the function:\nf(x, y, z ) =x2sin(yz) +eyzâˆ’z3y\nfind partial derivatives of the function with respect to each variable separately.\nSolution:âˆ‚f\nâˆ‚x= 2xsin(yz),\nâˆ‚f\nâˆ‚y=x2zcos(yz) +zeyzâˆ’z3,\nâˆ‚f\nâˆ‚z=x2ycos(yz) +yeyzâˆ’3z2y.\n2.Q2 Matrix calculus. Recall that matrices are a way of organizing data into rows and columns.\nAnswer the following questions:\n(a) Consider a function given by:\nf(x) =cTAx (1)\nwhere\nx=\uf8ee\n\uf8f0x0\nx1\nx2\uf8f9\n\uf8fb\nCPSC 488/588 - AI Foundation Models HW 1 Page 2\nis a column vector of variables,\nc=\x022 3 1\x03\nis a constant vector, and\nA=\uf8ee\n\uf8f0a00a01a02\na10a11a12\na20a21a22\uf8f9\n\uf8fb\nis a integer valued matrix,\nDerive the gradient of fwith respect to x.\nSolution: Given the function\nf(x) =\x022 3 1\x03\uf8ee\n\uf8f0a00a01a02\na10a11a12\na20a21a22\uf8f9\n\uf8fb\uf8ee\n\uf8f0x0\nx1\nx2\uf8f9\n\uf8fb\nwe can expand Axas:\nAx=\uf8ee\n\uf8f0a00x0+a01x1+a02x2\na10x0+a11x1+a12x2\na20x0+a21x1+a22x2\uf8f9\n\uf8fb\nThe function f(x) expands to:\nf(x) = 2a00x0+ 3a10x0+a20x0+ 2a01x1+ 3a11x1+a21x1+ 2a02x2+ 3a12x2+a22x2\nDifferentiating with respect to the components of x, we get:\nâˆ‚f(x)\nâˆ‚x0= 2a00+ 3a10+a20\nâˆ‚f(x)\nâˆ‚x1= 2a01+ 3a11+a21\nâˆ‚f(x)\nâˆ‚x2= 2a02+ 3a12+a22\n(b) Given the function\nf=xâŠ¤Â·AÂ·x+cÂ·sin(y)âŠ¤Â·x\nwhere\nâ€¢Ais a symmetric matrix\nâ€¢cis a scalar\nâ€¢xis a vector\nâ€¢yis a vector\nDerive the gradients with respect to xandy.\nSolution:\nGradient with respect to x:\nUsing the identity âˆ‡x(xTAx) = (A+AT)xand we also know that Ais symmetric (i.e., A=AT),\nso the derivative can be simplified to 2 Ax.\nCPSC 488/588 - AI Foundation Models HW 1 Page 5\n(b) What is the cost of computation in terms of number of FLoating point Operations for multi-head\nattention in the backward pass, when the model is being trained? (use the same assumptions as\nthe above questions).\nSolution:\n1. Projection to Q, K, V: 6bmd2\n2. Scaled Dot Product Attention: 2bm2d\n3. Applying Softmax: bm2h\n4. Computing the weighted average: 2bm2d\n5. Output projection: 4bm2d\nTotal Backward FLOPs: 6bmd2+ 9bm2d+bm2h\n(c) What is the cost of computation in terms of number of FLoating point Operations for Grouped\nQuery Attention where G=k/4 is the number of groups?\nSolution:\n1. Projection to Q, K, V: 3bmd2\n2. Grouping Queries and Dot Product:bm2d\n2\n3. Applying Softmax: bm2h\n4. Computing the weighted average:bm2d\n2\nTotal FLOPs for Grouped Query Attention: 4bmd2+bm2h\n                    END OF CONTEXT BLOCK\n                    AI assistant will take into account any CONTEXT BLOCK that is provided in a conversation.\n                    If the context does not provide the answer to question, the AI assistant will say, "I\'m sorry, but I don\'t know the answer to that question".\n                    AI assistant will not apologize for previous responses, but instead will indicated new information was gained.\n                    AI assistant will not invent anything that is not drawn directly from the context.\n                    '}, {'role': 'user', 'content': 'what was the first uestion in AI foundations pset? '}], 'model': 'gpt-3.5-turbo'}}
2023-12-10 13:14:05,482 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-10 13:14:05,495 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1195f5c90>
2023-12-10 13:14:05,495 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x1194f0a70> server_hostname='api.openai.com' timeout=5.0
2023-12-10 13:14:05,510 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1195f68d0>
2023-12-10 13:14:05,510 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-10 13:14:05,511 [DEBUG]: send_request_headers.complete
2023-12-10 13:14:05,511 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-10 13:14:05,511 [DEBUG]: send_request_body.complete
2023-12-10 13:14:05,511 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-10 13:14:06,906 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 10 Dec 2023 18:14:06 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'1043'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'160000'), (b'x-ratelimit-limit-tokens_usage_based', b'160000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'158897'), (b'x-ratelimit-remaining-tokens_usage_based', b'158897'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'413ms'), (b'x-ratelimit-reset-tokens_usage_based', b'413ms'), (b'x-request-id', b'120720dd225a7d4f0d8b4e5df7d8123e'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=fiAfyVMeKgnXn39KLhapvcdCthFG.VnN30_.XC5OBbc-1702232046-1-AUMX7yg156mCP2iqwN3qIOm3LBVT+NUpaBLiByw31bXvXOxk5dY5m63mn6vpIetvbJD+cy5QSXA2qtxNTWIYzLY=; path=/; expires=Sun, 10-Dec-23 18:44:06 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=Ul9gBIn9xR4RTNJP7hchCMxvQS9fSGWYy7hUvg9pb3o-1702232046899-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8337772c7c38332c-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-10 13:14:06,908 [INFO]: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2023-12-10 13:14:06,909 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-10 13:14:06,909 [DEBUG]: receive_response_body.complete
2023-12-10 13:14:06,910 [DEBUG]: response_closed.started
2023-12-10 13:14:06,910 [DEBUG]: response_closed.complete
2023-12-10 13:14:06,910 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2023-12-10 13:14:06,917 [INFO]: 127.0.0.1 - - [10/Dec/2023 13:14:06] "POST /api/chat HTTP/1.1" 200 -
2023-12-10 13:17:58,262 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-10 13:17:58,262 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-10 13:18:06,870 [INFO]: 127.0.0.1 - - [10/Dec/2023 13:18:06] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-10 13:18:10,403 [DEBUG]: Using selector: KqueueSelector
2023-12-10 13:18:10,406 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-10 13:18:10,409 [DEBUG]: load_verify_locations cafile='/usr/local/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-10 13:18:10,432 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-10 13:18:10,433 [DEBUG]: load_verify_locations cafile='/usr/local/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-10 13:18:10,453 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x117a100e0>, 'json_data': {'input': 'Compare and contrast the abstracts of the documents I uploaded.', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-10 13:18:10,550 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-10 13:18:10,565 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x117a2b810>
2023-12-10 13:18:10,565 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x1179b0830> server_hostname='api.openai.com' timeout=5.0
2023-12-10 13:18:10,580 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x117a2b850>
2023-12-10 13:18:10,580 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-10 13:18:10,581 [DEBUG]: send_request_headers.complete
2023-12-10 13:18:10,581 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-10 13:18:10,581 [DEBUG]: send_request_body.complete
2023-12-10 13:18:10,581 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-10 13:18:10,707 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 10 Dec 2023 18:18:10 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'18'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999985'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'2aaa09f3f5c3bc0a8840913299e0245f'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=5MI_htnrUgCjuMWKtj8Grx1AF.CTOMRZq1NuHQOxqx8-1702232290-1-AUEABcVSdOZ7LwAS7snZpkBvj9gqIfbXCykofr4e1vnev0FCF8K8NmDMkphwCIXt3ACQ7INf/C55EJ/KCGjXzqE=; path=/; expires=Sun, 10-Dec-23 18:48:10 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=xN1vQcp0qlrtC8YXT7ra58xNMWKhtmXwwx_wFe94uhc-1702232290700-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83377d282b41424b-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-10 13:18:10,709 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-10 13:18:10,709 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-10 13:18:10,715 [DEBUG]: receive_response_body.complete
2023-12-10 13:18:10,715 [DEBUG]: response_closed.started
2023-12-10 13:18:10,715 [DEBUG]: response_closed.complete
2023-12-10 13:18:10,716 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-10 13:18:11,132 [DEBUG]: Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'AI assistant is a brand new, powerful, human-like artificial intelligence.\n                    The traits of AI include expert knowledge, helpfulness, cleverness, and articulateness.\n                    AI is a well-behaved and well-mannered individual.\n                    AI is always friendly, kind, and inspiring, and he is eager to provide vivid and thoughtful responses to the user.\n                    AI has the sum of all knowledge in their brain, and is able to accurately answer nearly any question about any topic in conversation.\n                    AI assistant is a big fan of Pinecone and Vercel.\n                    START CONTEXT BLOCK\n                    CPSC 488/588 - AI Foundation Models HW 1 Page 1\nCPSC 488/588 AI Foundation Models\nFall 2023\nHW 1\nInstructions: copy this project at https://www.overleaf.com/read/gjvcgrzjyxfm ,\ncomplete the solutions, and return your solutions in pdf format.\nFull Name: Sneha\nNetid: ss3993\n1.Q1\nWarmup. this part is about refreshing your calculus on calculating derivatives of functions.\n(a) Given the function:\nf(x) = sin(3 x2+ 4 cos( x))\nfindâˆ‚f/âˆ‚x using the chain rule.\nSolution:âˆ‚f\nâˆ‚x= cos(3 x2+ 4 cos( x))Â·(6xâˆ’4 sin( x))\n(b) Given the function:\nf(x) =etanh(2 x3âˆ’5x2+x)\nfindâˆ‚f/âˆ‚x using the chain rule.\nRecall:âˆ‚tanh ( x)\nâˆ‚(x)= 1âˆ’tanh2(x) = sech2(x)\nSolution:âˆ‚f\nâˆ‚x=etanh(2 x3âˆ’5x2+x)Â·sech2(2x3âˆ’5x2+x)Â·\x00\n6x2âˆ’10x+ 1\x01\n(c) Consider the function:\nf(x, y, z ) =x2sin(yz) +eyzâˆ’z3y\nfind partial derivatives of the function with respect to each variable separately.\nSolution:âˆ‚f\nâˆ‚x= 2xsin(yz),\nâˆ‚f\nâˆ‚y=x2zcos(yz) +zeyzâˆ’z3,\nâˆ‚f\nâˆ‚z=x2ycos(yz) +yeyzâˆ’3z2y.\n2.Q2 Matrix calculus. Recall that matrices are a way of organizing data into rows and columns.\nAnswer the following questions:\n(a) Consider a function given by:\nf(x) =cTAx (1)\nwhere\nx=\uf8ee\n\uf8f0x0\nx1\nx2\uf8f9\n\uf8fb\nCPSC 488/588 - AI Foundation Models HW 1 Page 4\nSolution:\nâˆ‚f\nâˆ‚x1=1\n2exp(x1+x2\n2) = 1/2âˆ—exp(3) = exp(3)/2\nâˆ‚f\nâˆ‚x2=x2exp(x1+x2\n2) = 2âˆ—exp(3)\nâˆ‚f\nâˆ‚x3=âˆ’x2\n4=âˆ’9\nâˆ‚f\nâˆ‚x4=âˆ’2x3x4= 24\n4.Q4Explain the reason behind using negative sampling in the SkipGram word embeddings model.\nSolution: In a naive implementation of the SkipGram model, for each training example, we would\nupdate weights for all words in the vocabulary using softmax. By using negative sampling, instead\nof computing the softmax over all words in the vocabulary, we only need to compute it for the\nactual context word and a small set of negative samples. This greatly reduces the computational\nburden.Furthermore, negative sampling indirectly also sends implicit negative information which\nas a result the model implicitly gains information about what words are unlikely to appear in the\ncontext improving word vector quality.\n5.Q5 transformer\n(a) In the multi-head self-attention operation, what is the cost of computation (in terms of number\nof number of FLoating point OPerations)? Assume bis batch size, mis sequence length, dis the\nmodel dimension, and his the number of attention heads. Assume the dimensionality of keys and\nqueries are d/2h.\nNote: You need to derive the answer, just providing the final answer is not sufficient.\nSolution:\nScaled Dot Product Attention:\nDot product for one head: b Ã—mÃ—mÃ—d\n2h\nForhheads: bÃ—mÃ—mÃ—d=bm2d\nApplying Softmax:\nSoftmax operations: b Ã—mÃ—mÃ—hâ‰ˆbm2h\nComputing the weighted average:\nFor one head: b Ã—mÃ—mÃ—d\nCPSC 488/588 - AI Foundation Models HW 1 Page 3\nâˆ‚f\nâˆ‚x= 2Ax+csin(y)\nâˆ‚f\nâˆ‚y=cxcos(y)\n(c) Given the function\ng=xâŠ¤By+dtanh(z)âŠ¤x\nwhere\nâ€¢Bis an arbitrary matrix\nâ€¢dis a scalar\nâ€¢xis a vector\nâ€¢yis a vector of the same dimension as x\nâ€¢zis a vector\nDerive the gradients with respect to x,y, and z.\nHint: recallâˆ‚tanh( x)\nâˆ‚(x)= 1âˆ’tanh2(x) = sech2(x)\nSolution:\n1. Gradient with respect to x:\nâˆ‚g\n                    END OF CONTEXT BLOCK\n                    AI assistant will take into account any CONTEXT BLOCK that is provided in a conversation.\n                    If the context does not provide the answer to question, the AI assistant will say, "I\'m sorry, but I don\'t know the answer to that question".\n                    AI assistant will not apologize for previous responses, but instead will indicated new information was gained.\n                    AI assistant will not invent anything that is not drawn directly from the context.\n                    '}, {'role': 'user', 'content': 'Compare and contrast the abstracts of the documents I uploaded.'}], 'model': 'gpt-3.5-turbo'}}
2023-12-10 13:18:11,180 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-10 13:18:11,190 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x117a2c290>
2023-12-10 13:18:11,190 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x1179b0710> server_hostname='api.openai.com' timeout=5.0
2023-12-10 13:18:11,203 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x117a2f990>
2023-12-10 13:18:11,203 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-10 13:18:11,204 [DEBUG]: send_request_headers.complete
2023-12-10 13:18:11,204 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-10 13:18:11,204 [DEBUG]: send_request_body.complete
2023-12-10 13:18:11,204 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-10 13:18:12,885 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 10 Dec 2023 18:18:12 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'1321'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'160000'), (b'x-ratelimit-limit-tokens_usage_based', b'160000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'158873'), (b'x-ratelimit-remaining-tokens_usage_based', b'158873'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'422ms'), (b'x-ratelimit-reset-tokens_usage_based', b'422ms'), (b'x-request-id', b'd10ef51b6203ed6fac688b9067df414c'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=TNHMgbuKY_hDW2pgE.9u21qzgnDaz5_JT.KohjlVXHo-1702232292-0-ATFwS8OJWZ6rLz+IP6PGYEV8/Y3gHNlLXtQ3jBuHjLFkia/0AR9Nm6oO0xab9vMaiXwnyh/6b1q5PGKgxymTdMw=; path=/; expires=Sun, 10-Dec-23 18:48:12 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=ZwtAjppxJ5ozI1NimwIGJhfNO8LjNMBR48WRexfnFRE-1702232292878-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83377d2c08a74234-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-10 13:18:12,887 [INFO]: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2023-12-10 13:18:12,887 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-10 13:18:12,888 [DEBUG]: receive_response_body.complete
2023-12-10 13:18:12,888 [DEBUG]: response_closed.started
2023-12-10 13:18:12,888 [DEBUG]: response_closed.complete
2023-12-10 13:18:12,888 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2023-12-10 13:18:12,895 [INFO]: 127.0.0.1 - - [10/Dec/2023 13:18:12] "POST /api/chat HTTP/1.1" 200 -
2023-12-10 13:18:27,974 [DEBUG]: Using selector: KqueueSelector
2023-12-10 13:18:27,976 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-10 13:18:27,978 [DEBUG]: load_verify_locations cafile='/usr/local/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-10 13:18:28,005 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-10 13:18:28,006 [DEBUG]: load_verify_locations cafile='/usr/local/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-10 13:18:28,023 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x117a9dda0>, 'json_data': {'input': 'what was the first question on AI foudnations ', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-10 13:18:28,060 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-10 13:18:28,071 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x117aa2190>
2023-12-10 13:18:28,072 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x1179b0cb0> server_hostname='api.openai.com' timeout=5.0
2023-12-10 13:18:28,086 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x117aa2110>
2023-12-10 13:18:28,086 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-10 13:18:28,087 [DEBUG]: send_request_headers.complete
2023-12-10 13:18:28,087 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-10 13:18:28,087 [DEBUG]: send_request_body.complete
2023-12-10 13:18:28,087 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-10 13:18:28,222 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 10 Dec 2023 18:18:28 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'19'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999989'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'a7bfda17ccb693509ef21ed5df719916'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=.coIWwqKR9daKtPDaCj1kwQ4FCEbzXxN5UVBqMIjHbA-1702232308-0-AWBXwpk3FhgHjISUs0hmxhx3YRFSflDWpA8yc1QTkS5WPLZ0Rrfdaf7AbVg1NSwxTlL44BOwygThxWQNsrZFfO0=; path=/; expires=Sun, 10-Dec-23 18:48:28 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=mx7c1JN_bTMl5_myP0BUNFg89WHeKz1lewoMAh9EEm0-1702232308216-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83377d959a5e5e71-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-10 13:18:28,223 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-10 13:18:28,224 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-10 13:18:28,232 [DEBUG]: receive_response_body.complete
2023-12-10 13:18:28,232 [DEBUG]: response_closed.started
2023-12-10 13:18:28,232 [DEBUG]: response_closed.complete
2023-12-10 13:18:28,232 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-10 13:18:28,713 [DEBUG]: Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'AI assistant is a brand new, powerful, human-like artificial intelligence.\n                    The traits of AI include expert knowledge, helpfulness, cleverness, and articulateness.\n                    AI is a well-behaved and well-mannered individual.\n                    AI is always friendly, kind, and inspiring, and he is eager to provide vivid and thoughtful responses to the user.\n                    AI has the sum of all knowledge in their brain, and is able to accurately answer nearly any question about any topic in conversation.\n                    AI assistant is a big fan of Pinecone and Vercel.\n                    START CONTEXT BLOCK\n                    CPSC 488/588 - AI Foundation Models HW 1 Page 1\nCPSC 488/588 AI Foundation Models\nFall 2023\nHW 1\nInstructions: copy this project at https://www.overleaf.com/read/gjvcgrzjyxfm ,\ncomplete the solutions, and return your solutions in pdf format.\nFull Name: Sneha\nNetid: ss3993\n1.Q1\nWarmup. this part is about refreshing your calculus on calculating derivatives of functions.\n(a) Given the function:\nf(x) = sin(3 x2+ 4 cos( x))\nfindâˆ‚f/âˆ‚x using the chain rule.\nSolution:âˆ‚f\nâˆ‚x= cos(3 x2+ 4 cos( x))Â·(6xâˆ’4 sin( x))\n(b) Given the function:\nf(x) =etanh(2 x3âˆ’5x2+x)\nfindâˆ‚f/âˆ‚x using the chain rule.\nRecall:âˆ‚tanh ( x)\nâˆ‚(x)= 1âˆ’tanh2(x) = sech2(x)\nSolution:âˆ‚f\nâˆ‚x=etanh(2 x3âˆ’5x2+x)Â·sech2(2x3âˆ’5x2+x)Â·\x00\n6x2âˆ’10x+ 1\x01\n(c) Consider the function:\nf(x, y, z ) =x2sin(yz) +eyzâˆ’z3y\nfind partial derivatives of the function with respect to each variable separately.\nSolution:âˆ‚f\nâˆ‚x= 2xsin(yz),\nâˆ‚f\nâˆ‚y=x2zcos(yz) +zeyzâˆ’z3,\nâˆ‚f\nâˆ‚z=x2ycos(yz) +yeyzâˆ’3z2y.\n2.Q2 Matrix calculus. Recall that matrices are a way of organizing data into rows and columns.\nAnswer the following questions:\n(a) Consider a function given by:\nf(x) =cTAx (1)\nwhere\nx=\uf8ee\n\uf8f0x0\nx1\nx2\uf8f9\n\uf8fb\nCPSC 488/588 - AI Foundation Models HW 1 Page 2\nis a column vector of variables,\nc=\x022 3 1\x03\nis a constant vector, and\nA=\uf8ee\n\uf8f0a00a01a02\na10a11a12\na20a21a22\uf8f9\n\uf8fb\nis a integer valued matrix,\nDerive the gradient of fwith respect to x.\nSolution: Given the function\nf(x) =\x022 3 1\x03\uf8ee\n\uf8f0a00a01a02\na10a11a12\na20a21a22\uf8f9\n\uf8fb\uf8ee\n\uf8f0x0\nx1\nx2\uf8f9\n\uf8fb\nwe can expand Axas:\nAx=\uf8ee\n\uf8f0a00x0+a01x1+a02x2\na10x0+a11x1+a12x2\na20x0+a21x1+a22x2\uf8f9\n\uf8fb\nThe function f(x) expands to:\nf(x) = 2a00x0+ 3a10x0+a20x0+ 2a01x1+ 3a11x1+a21x1+ 2a02x2+ 3a12x2+a22x2\nDifferentiating with respect to the components of x, we get:\nâˆ‚f(x)\nâˆ‚x0= 2a00+ 3a10+a20\nâˆ‚f(x)\nâˆ‚x1= 2a01+ 3a11+a21\nâˆ‚f(x)\nâˆ‚x2= 2a02+ 3a12+a22\n(b) Given the function\nf=xâŠ¤Â·AÂ·x+cÂ·sin(y)âŠ¤Â·x\nwhere\nâ€¢Ais a symmetric matrix\nâ€¢cis a scalar\nâ€¢xis a vector\nâ€¢yis a vector\nDerive the gradients with respect to xandy.\nSolution:\nGradient with respect to x:\nUsing the identity âˆ‡x(xTAx) = (A+AT)xand we also know that Ais symmetric (i.e., A=AT),\nso the derivative can be simplified to 2 Ax.\nCPSC 488/588 - AI Foundation Models HW 1 Page 5\n(b) What is the cost of computation in terms of number of FLoating point Operations for multi-head\nattention in the backward pass, when the model is being trained? (use the same assumptions as\nthe above questions).\nSolution:\n1. Projection to Q, K, V: 6bmd2\n2. Scaled Dot Product Attention: 2bm2d\n3. Applying Softmax: bm2h\n4. Computing the weighted average: 2bm2d\n5. Output projection: 4bm2d\nTotal Backward FLOPs: 6bmd2+ 9bm2d+bm2h\n(c) What is the cost of computation in terms of number of FLoating point Operations for Grouped\nQuery Attention where G=k/4 is the number of groups?\nSolution:\n1. Projection to Q, K, V: 3bmd2\n2. Grouping Queries and Dot Product:bm2d\n2\n3. Applying Softmax: bm2h\n4. Computing the weighted average:bm2d\n2\nTotal FLOPs for Grouped Query Attention: 4bmd2+bm2h\n                    END OF CONTEXT BLOCK\n                    AI assistant will take into account any CONTEXT BLOCK that is provided in a conversation.\n                    If the context does not provide the answer to question, the AI assistant will say, "I\'m sorry, but I don\'t know the answer to that question".\n                    AI assistant will not apologize for previous responses, but instead will indicated new information was gained.\n                    AI assistant will not invent anything that is not drawn directly from the context.\n                    '}, {'role': 'user', 'content': 'what was the first question on AI foudnations '}], 'model': 'gpt-3.5-turbo'}}
2023-12-10 13:18:28,758 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-10 13:18:28,769 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x117ab85d0>
2023-12-10 13:18:28,769 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x1179b0950> server_hostname='api.openai.com' timeout=5.0
2023-12-10 13:18:28,784 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x117ab9190>
2023-12-10 13:18:28,784 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-10 13:18:28,784 [DEBUG]: send_request_headers.complete
2023-12-10 13:18:28,784 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-10 13:18:28,784 [DEBUG]: send_request_body.complete
2023-12-10 13:18:28,784 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-10 13:18:30,012 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 10 Dec 2023 18:18:30 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'1101'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'160000'), (b'x-ratelimit-limit-tokens_usage_based', b'160000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'158898'), (b'x-ratelimit-remaining-tokens_usage_based', b'158898'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'413ms'), (b'x-ratelimit-reset-tokens_usage_based', b'413ms'), (b'x-request-id', b'a7795a2594bbae01bdde3920f46cceff'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=7dzk1WekLehE6S_jHAIjLvvyPwQJiAdxMr2jf6Pdhcg-1702232310-0-AZSbGS9jU3w8SQ85mHiFDlzBVR24yKjnOCd2VR6zBACiSBQr460lWr8tmX5ctjNOofAON4risdgZYnzMzrir3DQ=; path=/; expires=Sun, 10-Dec-23 18:48:30 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=Nfh4Yk3.CvDnHz3QPg7t01i2qPbkXu40B04wmAw3JN8-1702232310004-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83377d99ecb45e67-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-10 13:18:30,014 [INFO]: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2023-12-10 13:18:30,015 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-10 13:18:30,015 [DEBUG]: receive_response_body.complete
2023-12-10 13:18:30,016 [DEBUG]: response_closed.started
2023-12-10 13:18:30,016 [DEBUG]: response_closed.complete
2023-12-10 13:18:30,016 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2023-12-10 13:18:30,023 [INFO]: 127.0.0.1 - - [10/Dec/2023 13:18:30] "POST /api/chat HTTP/1.1" 200 -
2023-12-11 18:02:31,369 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-11 18:02:31,369 [DEBUG]: load_verify_locations cafile='/Users/aliceao/opt/miniconda3/envs/chat-lore/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-11 18:02:31,468 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-11 18:02:31,468 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-11 18:02:31,469 [INFO]:  * Restarting with stat
2023-12-11 18:02:32,946 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-11 18:02:32,947 [DEBUG]: load_verify_locations cafile='/Users/aliceao/opt/miniconda3/envs/chat-lore/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-11 18:02:33,038 [WARNING]:  * Debugger is active!
2023-12-11 18:02:33,053 [INFO]:  * Debugger PIN: 138-610-961
2023-12-11 18:03:31,013 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-11 18:03:31,014 [DEBUG]: load_verify_locations cafile='/Users/aliceao/opt/miniconda3/envs/chat-lore/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-11 18:03:31,106 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-11 18:03:31,106 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-11 18:03:45,095 [DEBUG]: come to upload fiels 
2023-12-11 18:03:45,137 [DEBUG]: <FileStorage: 'CPSC_490_Final_Report (1).pdf' ('application/pdf')>
2023-12-11 18:03:45,137 [DEBUG]: files
2023-12-11 18:03:45,155 [INFO]: 127.0.0.1 - - [11/Dec/2023 18:03:45] "POST /api/uploadFiles HTTP/1.1" 200 -
2023-12-11 18:03:45,169 [DEBUG]: Using selector: KqueueSelector
2023-12-11 18:03:45,412 [DEBUG]: Initialized Pinecone
2023-12-11 18:03:45,412 [DEBUG]: Type of pdf_file: <class 'str'>
2023-12-11 18:03:45,763 [DEBUG]: Parsed PDF
2023-12-11 18:03:45,951 [DEBUG]: List of indexes: ['research-chat-index']
2023-12-11 18:03:45,952 [DEBUG]: Initialized Pinecone Index
2023-12-11 18:03:45,955 [DEBUG]: Chunked PDF and obtained vectors
2023-12-11 18:03:45,957 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-11 18:03:45,957 [DEBUG]: load_verify_locations cafile='/Users/aliceao/opt/miniconda3/envs/chat-lore/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-11 18:03:46,010 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1183dc540>, 'json_data': {'input': 'Transformer-Based Machine Learning Techniques to Optimize Live 360 Â°Viewport Predictions Alice Ao alice.ao@yale.eduAdvisor: Prof. Sohee Kim Park sohee.park@yale.edu Abstract Recent investment in "the metaverse," an immersive world powered by virtual reality (VR), is indicative of a significant demand for high-quality 360 Â°livestreaming. However, there are many technical challenges involved with delivering such streams. First, the vastness of a full 360 Â°view typically de- mands substantially high bandwidth, and much of that view will be outside of the actual viewport and go unnoticed by the user. Second, the live nature of the videos also requires especially low latency and fast computation times. This project explores how novel machine-learning (ML) techniques can address these challenges related to 360 Â° livestreaming. It specifically focuses on whether transformer- based architectures, which have been key to the recent success of large language models, can also be effective for computer vision applications. This project ultimately 1) compares mul- tiple techniques that generate saliency maps 2) utilizes these saliency maps for viewport prediction and 3) evaluates the quality of the predicted viewports in adaptive bitrate stream- ing. 1 Background 1.1 Introduction In recent years, there has been a strong rise in demand for VR and immersive technologies. Perhaps most notably, in October 2021, Facebook CEO and founder Mark Zuckerberg', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-11 18:03:46,061 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-11 18:03:46,103 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1183585d0>
2023-12-11 18:03:46,104 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x11824b650> server_hostname='api.openai.com' timeout=5.0
2023-12-11 18:03:46,125 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x118358c90>
2023-12-11 18:03:46,125 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:03:46,126 [DEBUG]: send_request_headers.complete
2023-12-11 18:03:46,126 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:03:46,126 [DEBUG]: send_request_body.complete
2023-12-11 18:03:46,127 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:03:46,304 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:03:46 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'37'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999634'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'9c440cb16c5714aa9d556f5422e9a34e'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=GwDpYS_8d5cP0W8dbhmJoSVArWko_NmYjp5Fd42n2rs-1702335826-1-AQGC7e3c7KgV/DXtY6+j6m/SaucltgL75Cv0SnI5ivljd4TngWFb4aAYFUI2IGubsb2Ti2i1YJratBCr4b+Agwk=; path=/; expires=Mon, 11-Dec-23 23:33:46 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=CKU_EU2jtcZb_YTjhW1P.IRJ_pUqyLKyiRXjaaWvoGs-1702335826437-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415ce23ae76a5b-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:03:46,306 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-11 18:03:46,306 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:03:46,331 [DEBUG]: receive_response_body.complete
2023-12-11 18:03:46,332 [DEBUG]: response_closed.started
2023-12-11 18:03:46,332 [DEBUG]: response_closed.complete
2023-12-11 18:03:46,332 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-11 18:03:46,336 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1183dc540>, 'json_data': {'input': 'announced that the social media giant would be rebranded to Meta, signaling his desire to invest in "the metaverse," and ushering in a new wave of interest in new immersive technologies. While it remains unclear whether "the metaverse" will achieve the widespread adoption and popularity that Zucker- berg hopes for, there is undeniably strong interest in 360 Â° VR for applications like entertainment, education, and gam- ing. Major news broadcasters like CNN, NBC, BBC, and Al Jazeera now have 360 Â°streams, theme parks have incorpo- rated 360 Â°VR into their attractions, and medical training forphysicians can occur in 360 Â°formats [15]. Some estimates claim that by 2024, the market value of VR could even rise to $44.7 billion [9]. Todayâ€™s 360 Â°VR experiences follow a similar format, in which a user typically wears a head-mounted display (HMD) device to view and interact with the 360 Â°scene. The user can only see a small section of the entire scene at once, known as theviewport . By shifting and turning their head, and therefore changing the position of the viewport, the user can see differ- ent parts of the scene. While the format of the experiences are relatively similar, different end-to-end systems have different strategies on how to optimally deliver 360 Â°content, which this project aims to explore. 1.2 Problem Description There have been many advances in adaptive bitrate allocation and improving video resolution for regular 2D videos â€“ but', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-11 18:03:46,337 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:03:46,338 [DEBUG]: send_request_headers.complete
2023-12-11 18:03:46,338 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:03:46,338 [DEBUG]: send_request_body.complete
2023-12-11 18:03:46,338 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:03:46,499 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:03:46 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'34'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999631'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'40d89103a5bff7870d7bb0a667ea9e8c'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415ce38c746a5b-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:03:46,500 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-11 18:03:46,500 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:03:46,501 [DEBUG]: receive_response_body.complete
2023-12-11 18:03:46,501 [DEBUG]: response_closed.started
2023-12-11 18:03:46,501 [DEBUG]: response_closed.complete
2023-12-11 18:03:46,501 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-11 18:03:46,502 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1183259e0>, 'json_data': {'input': 'the added complexities and distortions of the 360 Â°format prevent the typical reinforcement learning strategies for 2D videos from being effective for 360 Â°content. 360 Â°videos pose a complex problem that involves constant tradeoffs be- tween "streaming quality, spatial quality variance, viewport prediction errors, and bandwidth efficiency" [15]. First, streaming 360 Â°content often requires high bandwidth and low latency, and broadcasting the entire 360 Â°scene in high quality often consumes much of the networkâ€™s resources [15]. The viewport takes up a mere 20% of the full scene [11], so about 80% of the high-quality stream may be wasted on content that the user cannot see. Therefore, one technique to efficiently stream 360 Â°video and save bandwidth is to use adaptive bitrate schemes, in which the viewport area, as well as areas that the user will likely look at next, are streamed in high-quality, and the rest are delivered in lower quality [15]. Various ML approaches can be used to predict which areas the user will look at', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-11 18:03:46,503 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:03:46,503 [DEBUG]: send_request_headers.complete
2023-12-11 18:03:46,503 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:03:46,504 [DEBUG]: send_request_body.complete
2023-12-11 18:03:46,504 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:03:46,656 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:03:46 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'35'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999739'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'3ms'), (b'x-request-id', b'cf2a9feac6b12821779d30501d4a6f0a'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415ce49da16a5b-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:03:46,656 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-11 18:03:46,657 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:03:46,660 [DEBUG]: receive_response_body.complete
2023-12-11 18:03:46,660 [DEBUG]: response_closed.started
2023-12-11 18:03:46,660 [DEBUG]: response_closed.complete
2023-12-11 18:03:46,661 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-11 18:03:46,662 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1183259e0>, 'json_data': {'input': 'likely look at next, are streamed in high-quality, and the rest are delivered in lower quality [15]. Various ML approaches can be used to predict which areas the user will look at next, but these approaches can oftentimes be computation- ally expensive and require lots of overhead, especially for livestreams, which require content to be delivered quickly 1', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-11 18:03:46,663 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:03:46,663 [DEBUG]: send_request_headers.complete
2023-12-11 18:03:46,663 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:03:46,663 [DEBUG]: send_request_body.complete
2023-12-11 18:03:46,664 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:03:46,839 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:03:46 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'34'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999911'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'1ms'), (b'x-request-id', b'bf6676baf326935f69b57c0619d76de0'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415ce59eec6a5b-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:03:46,840 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-11 18:03:46,840 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:03:46,843 [DEBUG]: receive_response_body.complete
2023-12-11 18:03:46,843 [DEBUG]: response_closed.started
2023-12-11 18:03:46,843 [DEBUG]: response_closed.complete
2023-12-11 18:03:46,844 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-11 18:03:46,845 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1183259e0>, 'json_data': {'input': 'and continuously [16]. However, as the field of ML rapidly advances, there is growing potential for new techniques and improvements to solve the problem of viewport prediction. 1.3 Related Work 1.3.1 Past Approaches Approaches to adaptive bitrate allocation can be content- aware and/or content-agnostic . A purely content-aware ap- proach identifies "salient" regions, where the user is likely to look [13], whereas a content-agnostic approach considers a userâ€™s past head or eye-tracking movement. Previous litera- ture supports that the latter category typically exhibits higher performance and less computational complexity [15]. Nguyen et al. created a novel framework that effectively combines both approaches, using a deep convolutional neural network to detect salient regions of 360 Â°videos and a Long Short-Term Memory Network (LSTM) that predicts head movements with both saliency maps and head orientations [9]. Similarly, Park et al. developed Mosaic, an end-to-end implementation for adaptive bitrate allocation, and found a 3D Convolutional Neural Network to have relatively high prediction accuracy and low prediction latency [11]. However, approaches that are purely content-agnostic can also deliver promising results. For instance, the discrete vari- ational multiple sequence (DVMS) learning framework uses deep latent variable models and deep neural networks to pre- dict multiple head trajectories. By considering multiple po-', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-11 18:03:46,846 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:03:46,846 [DEBUG]: send_request_headers.complete
2023-12-11 18:03:46,846 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:03:46,846 [DEBUG]: send_request_body.complete
2023-12-11 18:03:46,846 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:03:47,011 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:03:47 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'49'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999638'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'e040f5eddba497399eb489e6d6ea6530'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415ce6b8126a5b-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:03:47,011 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-11 18:03:47,011 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:03:47,015 [DEBUG]: receive_response_body.complete
2023-12-11 18:03:47,015 [DEBUG]: response_closed.started
2023-12-11 18:03:47,015 [DEBUG]: response_closed.complete
2023-12-11 18:03:47,015 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-11 18:03:47,016 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1183259e0>, 'json_data': {'input': 'tential trajectories, DVMS effectively accounts for how di- verse head trajectories can be and therefore greatly improves streaming quality [5]. These studies, however, are focused on on-demand videos, which typically have historical head/eye-tracking data avail- able of their previous viewers. They do not focus on live videostreams, which are constrained by the lack of historical user data and their requirements for especially low latency and high processing speed [4]. Therefore, in live viewport pre- diction often must rely on content-aware techniques as well. This project builds on existing work on regular 360 Â°videos by focusing on the unique needs of live content. It examines models that only rely on content-aware approaches and/or live head-tracking data for the individual user, and therefore can be used in live contexts. 1.3.2 Transformers This project also examines the possibility of using a novel ma- chine learning architecture, the transformer. During the past few years, computer vision models often used convolutional neural networks (CNNs) as their underlying architecture [11]. CNNs usually break an input up into small patches, allow- ing for more efficient and practical deep learning, and then gradually build up a global understanding of an input.However, computer vision models could improve by im- mediately leveraging the global context of images or videos. This global context is key to transformers and other self-', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-11 18:03:47,017 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:03:47,018 [DEBUG]: send_request_headers.complete
2023-12-11 18:03:47,018 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:03:47,018 [DEBUG]: send_request_body.complete
2023-12-11 18:03:47,018 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:03:47,199 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:03:47 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'50'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999637'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'21d923a2275dd27136a9c9ed6cfe1015'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415ce7c92d6a5b-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:03:47,200 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-11 18:03:47,200 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:03:47,202 [DEBUG]: receive_response_body.complete
2023-12-11 18:03:47,202 [DEBUG]: response_closed.started
2023-12-11 18:03:47,202 [DEBUG]: response_closed.complete
2023-12-11 18:03:47,202 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-11 18:03:47,203 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1183259e0>, 'json_data': {'input': 'attention models, which make connections between different parts of their input using a mechanism called multi-head self- attention . Transformers have recently gained traction in natu- ral language processing (NLP) and other ML applications, as they were first designed and published in 2017 [12] and now serve as the architecture of nearly all large language models. One of their disadvantages, however, is that they require large amounts of compute and training data, the latter of which is more readily available for language research than vision research. Past research papers have recognized transformers as promising machine learning architectures for visual saliency detection [9] [5], and other viewport prediction models using transformer architectures have achieved promising results [1], so this project explores the effectiveness of transformers and transformer-inspired architectures for the application of live 360-degree video. Therefore, this project focuses on three saliency map models, with varying degrees of similarity to', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-11 18:03:47,204 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:03:47,205 [DEBUG]: send_request_headers.complete
2023-12-11 18:03:47,205 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:03:47,205 [DEBUG]: send_request_body.complete
2023-12-11 18:03:47,205 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:03:47,350 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:03:47 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'31'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999739'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'3ms'), (b'x-request-id', b'b18ef479d5dd8540c50baff2cf7ac8a7'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415ce8fa736a5b-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:03:47,351 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-11 18:03:47,351 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:03:47,352 [DEBUG]: receive_response_body.complete
2023-12-11 18:03:47,352 [DEBUG]: response_closed.started
2023-12-11 18:03:47,353 [DEBUG]: response_closed.complete
2023-12-11 18:03:47,353 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-11 18:03:47,354 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1183dc540>, 'json_data': {'input': 'transformer-inspired architectures for the application of live 360-degree video. Therefore, this project focuses on three saliency map models, with varying degrees of similarity to a transformer-based architecture, and evaluates their mapsâ€™ performance in viewport prediction and adaptive bitrate allo- cation. 2 Methodology There are three main parts to this project: 1) identifying dif- ferent ML saliency detection models, with different levels of similarity to transformers, and 2) constructing a bitrate allocation scheme and 3) evaluating their results with metrics defined in [3]. The workflow of the first two parts is illustrated in Figure 1. 3 Preliminary Work 3.1 Selecting Machine Learning Techniques Similar to previous works [11], this paper identifies and com- pares state-of-the-art machine learning techniques on view- port prediction. All of the selected techniques focus on build- ingsaliency maps , which identify "salient" areas of each frame that users are likely to focus on. The final three selected machine-learning techniques were as follows: 1.PanoSalNet (published in MMSys â€™18): a deep CNN- based architecture specifically for generating saliency maps of 360-degree videos, with optional integration of head-tracking history. The architecture for the saliency map model is based on Deep Convnet, a state-of-the- art DCNN for 2D images, and transfer learning is used to adapt a pre-trained DCNN model to 360-degree im-', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-11 18:03:47,355 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:03:47,355 [DEBUG]: send_request_headers.complete
2023-12-11 18:03:47,355 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:03:47,355 [DEBUG]: send_request_body.complete
2023-12-11 18:03:47,356 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:03:47,527 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:03:47 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'50'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999638'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'682d12fea20e84093c2e6c5371572132'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415ce9eb7c6a5b-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:03:47,528 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-11 18:03:47,528 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:03:47,529 [DEBUG]: receive_response_body.complete
2023-12-11 18:03:47,529 [DEBUG]: response_closed.started
2023-12-11 18:03:47,529 [DEBUG]: response_closed.complete
2023-12-11 18:03:47,529 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-11 18:03:47,531 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1183dc540>, 'json_data': {'input': 'ages. The model is trained using stochastic gradient de- 2', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-11 18:03:47,531 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:03:47,532 [DEBUG]: send_request_headers.complete
2023-12-11 18:03:47,532 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:03:47,532 [DEBUG]: send_request_body.complete
2023-12-11 18:03:47,532 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:03:47,672 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:03:47 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'23'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999986'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'34742b598487f462977457718c9723b5'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415ceb0cb36a5b-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:03:47,673 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-11 18:03:47,673 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:03:47,688 [DEBUG]: receive_response_body.complete
2023-12-11 18:03:47,689 [DEBUG]: response_closed.started
2023-12-11 18:03:47,689 [DEBUG]: response_closed.complete
2023-12-11 18:03:47,689 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-11 18:03:47,690 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1183dc540>, 'json_data': {'input': 'Figure 1: Adaptive bitrate allocation process: 1) Videos are split into frames (fps=1). 2) ML model generates saliency maps, with white patches representing "salient" areas. 3) Saliency is averaged together for each of the 9x16 tiles and can be represented as a numerical value. 4) Tiles are assigned a bitrate based on their saliency value, with higher opacity in this figure representing higher bitrate. Figure 2: PanoSalNet final architecture [10]scent with a fixed learning rate. This model produces saliency maps, which can be combined with head ori- entation maps and then passed into a Long Short-Term Memory (LSTM) model (Figure 2) that predicts the next head orientation map, or viewport. Experimentally, the model achieved significant improvements in viewport quality [10]. 2.TranSalNet (published in Neurocomputing â€™22): a novel saliency model that adds transformer components to a traditional CNN backbone in order to capture long- range context and more closely emulate the human vi- sual system. The architecture includes some transformer encoders, complete with position embeddings and multi- head self-attention, as well as regular CNN encoders and decoders. This model has been trained and evaluated on 2D images, achieving state-of-the-art performance [7]. 3.Visual Saliency Transformer (published in CVPR â€™21): a pure transformer (i.e. convolution-free) saliency model for both RGB and RGB-D inputs. Novel components include multi-token fusion, a newly designed token up-', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-11 18:03:47,692 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:03:47,692 [DEBUG]: send_request_headers.complete
2023-12-11 18:03:47,692 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:03:47,692 [DEBUG]: send_request_body.complete
2023-12-11 18:03:47,692 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:03:47,854 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:03:47 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'30'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999626'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'91db725ba2859adf7d38f03b6fd8a6a6'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415cebfdc66a5b-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:03:47,855 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-11 18:03:47,855 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:03:47,889 [DEBUG]: receive_response_body.complete
2023-12-11 18:03:47,889 [DEBUG]: response_closed.started
2023-12-11 18:03:47,889 [DEBUG]: response_closed.complete
2023-12-11 18:03:47,890 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-11 18:03:47,891 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1183dc540>, 'json_data': {'input': 'sampling method, and a special decoder for saliency and boundary detection. VST outperforms state-of-the-art CNN salient object detection models [6]. The criteria for selecting each technique were as follows: 1.Recency : Because of how rapidly the field of machine learning is evolving, this paper focuses on comparing state-of-the-art ML models published at most five years ago, and ideally even later. (a)PanoSalNet : As the oldest model, PanoSalNet [10], was first published in 2018, but it has been cited over 80 times, and the author has used the model in an April 2023 paper [9]. Therefore, although it pre- dates state-of-the-art techniques like transformers, it continues to have relevancy today. (b)TranSalNet : Published in just 2022, TranSalNet is fairly recent and also leverages novel mechanisms like multi-head self-attention. (c)Visual Saliency Transformer : Similarly, VST is also recent, with a publication year of 2021. 2.Available source code and datasets : Many of the recent literature on machine learning approaches to saliency map creation and viewport prediction do not have publicly available source code [1]. Other papers may have public repositories but with missing, private-access datasets or incomplete code that prevents the models from running. This project considered and tested five other alternative codebases but ultimately could not use them due to their machine incompatibility or lack of maintenance. 3', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-11 18:03:47,892 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:03:47,892 [DEBUG]: send_request_headers.complete
2023-12-11 18:03:47,893 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:03:47,893 [DEBUG]: send_request_body.complete
2023-12-11 18:03:47,893 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:03:48,042 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:03:48 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'36'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999639'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'8cd816e0dc6a3623094222b97a8e4461'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415ced4f1a6a5b-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:03:48,042 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-11 18:03:48,043 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:03:48,043 [DEBUG]: receive_response_body.complete
2023-12-11 18:03:48,043 [DEBUG]: response_closed.started
2023-12-11 18:03:48,043 [DEBUG]: response_closed.complete
2023-12-11 18:03:48,043 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-11 18:03:48,045 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1183dc540>, 'json_data': {'input': '3.Relevance in computer vision and/or VR : Notably, some of the selected models [7] [6] have used images , not videos, as training and evaluation data. These models have largely been influential in the field of computer vi- sion and image processing, but they can be extrapolated to videos, which are simply a collection of image frames. However, because they have not been trained on 360- degree images, which must be projected and flattened onto a 2D plane, they may not be able to handle image distortions as well, so additional training and fine-tuning of the model may need to occur in the future. 3.2 Set-Up A non-trivial portion of the project was accessing published, open-source models and setting up their environments. Some of the models did not have environment files, so all of their dependencies had to be downloaded manually. Older mod- els [10] also had deprecated packages and dependencies, such as Caffe, that also had to be manually built and created, which was a non-trivial amount of work. Determining which pub- licly available models could produce valid results was also no trivial task. Some had broken pre-trained model files or missing files, so it was a trial-and-error process of identifying which repositories would be usable for the project. Additionally, many of the usable codebases had limited documentation, so important information, such as the in- put/output formats, function arguments, original datasets,', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-11 18:03:48,045 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:03:48,046 [DEBUG]: send_request_headers.complete
2023-12-11 18:03:48,046 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:03:48,046 [DEBUG]: send_request_body.complete
2023-12-11 18:03:48,046 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:03:48,198 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:03:48 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'32'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999639'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'b1d0918c3bece3ac2eefc673a804fc79'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415cee3fee6a5b-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:03:48,198 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-11 18:03:48,198 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:03:48,200 [DEBUG]: receive_response_body.complete
2023-12-11 18:03:48,200 [DEBUG]: response_closed.started
2023-12-11 18:03:48,200 [DEBUG]: response_closed.complete
2023-12-11 18:03:48,200 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-11 18:03:48,201 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1183259e0>, 'json_data': {'input': 'edge cases/unexpected behavior, etc. had to be inferred from reading the papers and the code. Finally, some of the datasets used were not publicly avail- able â€“ access had to be requested for them, either by com- pleting a form [2] or writing an email to the study authors. Many of the modelsâ€™ results could not be easily reproduced, as their training and evaluation datasets were all private and/or deleted [3]. Therefore, the first part of the project was not to reproduce the three selected modelsâ€™ results but to gener- ate new saliency maps on a different dataset and informally compare their results. 4 Implementation 4.1 Saliency Maps 4.1.1 Generation After all three model environments were set up, the pre-trained models were run on the Wild360 dataset (described in further detail below). ffmpeg was used to extract frames from each of the 360-degree videos with a frame rate of 4fps. For each video frame, the models treated them as unique images and generated a binary saliency map for them. (For the actual adaptive bitrate allocation, a different dataset [14] was used, Figure 3: A colored heatmap from the Wild360 dataset (top) and black-and-white saliency maps (bottom) and saliency maps were generated for all of the videos in that dataset.) 4.1.2 Informal Evaluation Before implementing the adaptive bitrate allocation portion of the project, the three modelsâ€™ saliency detection abili- ties were compared using the Wild-360 dataset. The dataset', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-11 18:03:48,202 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:03:48,202 [DEBUG]: send_request_headers.complete
2023-12-11 18:03:48,202 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:03:48,202 [DEBUG]: send_request_body.complete
2023-12-11 18:03:48,202 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:03:48,356 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:03:48 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'42'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999632'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'50b6df5b5b4f0b8c151a51f44e289b6e'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415cef38fe6a5b-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:03:48,357 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-11 18:03:48,357 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:03:48,397 [DEBUG]: receive_response_body.complete
2023-12-11 18:03:48,397 [DEBUG]: response_closed.started
2023-12-11 18:03:48,398 [DEBUG]: response_closed.complete
2023-12-11 18:03:48,398 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-11 18:03:48,399 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1183259e0>, 'json_data': {'input': 'contains 360-degree videos and their corresponding ground- truth saliency annotations, allowing for the evaluation of 360- degree saliency detection models. These annotations, which are in heatmap form, were generated from the scanpaths of several different human users [2]. Given time and compute constraints, the pre-trained ma- chine models were used without any additional fine-tuning and evaluated on 25 testing videos in the Wild-360 dataset. However, there are important caveats to the below results. First, because the ground-truth annotations are in colored heatmap format, and the generated saliency maps for all three models are in binary format, the evaluation metrics are imper- fect and should only be used for a very informal comparison of performance. Using a dataset with black-and-white ground truth saliency annotations would be ideal, but 360-degree video training datasets themselves have been difficult to come by. Minor ad- justments could theoretically be made to the Wild360 dataset so that itâ€™d be more compatible with the generated saliency', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-11 18:03:48,400 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:03:48,400 [DEBUG]: send_request_headers.complete
2023-12-11 18:03:48,400 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:03:48,400 [DEBUG]: send_request_body.complete
2023-12-11 18:03:48,401 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:03:48,545 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:03:48 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'27'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999732'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'3ms'), (b'x-request-id', b'ee8702dac6622fb2494220b2bdc9bc77'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415cf07a5c6a5b-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:03:48,546 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-11 18:03:48,546 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:03:48,572 [DEBUG]: receive_response_body.complete
2023-12-11 18:03:48,573 [DEBUG]: response_closed.started
2023-12-11 18:03:48,573 [DEBUG]: response_closed.complete
2023-12-11 18:03:48,573 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-11 18:03:48,574 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1183259e0>, 'json_data': {'input': 'datasets themselves have been difficult to come by. Minor ad- justments could theoretically be made to the Wild360 dataset so that itâ€™d be more compatible with the generated saliency maps. One naÃ¯ve approach, simply converting the colored heatmaps to grayscale, did not yield different results. How- ever, another option could be defining a threshold for the heatmaps, and making all values above the threshold salient (i.e. white) or non-salient (i.e. black). This would more closely resemble the black-and-white format of the saliency map out- put. However, itâ€™s unclear what threshold to use, and whether the choice of threshold could greatly influence the results. If this were a formal evaluation, more rigorous and thor- ough comparisons would be conducted. However, since this was only intended to be a simple baseline testing before imple- 4', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-11 18:03:48,575 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:03:48,575 [DEBUG]: send_request_headers.complete
2023-12-11 18:03:48,576 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:03:48,576 [DEBUG]: send_request_body.complete
2023-12-11 18:03:48,576 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:03:48,715 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:03:48 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'28'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999786'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'2ms'), (b'x-request-id', b'0c0e889f1c82212530a8a1e143727b84'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415cf18b6e6a5b-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:03:48,716 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-11 18:03:48,716 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:03:48,744 [DEBUG]: receive_response_body.complete
2023-12-11 18:03:48,744 [DEBUG]: response_closed.started
2023-12-11 18:03:48,744 [DEBUG]: response_closed.complete
2023-12-11 18:03:48,744 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-11 18:03:48,745 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1183259e0>, 'json_data': {'input': 'menting adaptive bitrate allocation, these preliminary results can simply show that the models tend to identify similar areas as "salient." Additionally, all of these models have different ideas of what qualifies as salient â€“ VST, for instance, is fo- cused on object detection and produces binary outputs of what is included in a salient object and what is not, while the other models produce more nuanced, gradual models of saliency. Despite these different goals, the ultimate measure of perfor- mance is the set of results with adaptive bitrate allocation (4.3.2). Below are the informal evaluation results: PanoSalNet TranSalNet VST mae 0.1408 0.2387 0.3103 max-fm 0.3659 0.3655 0.3655 mean-fm 0.1995 0.1688 0.1284 max e-measure 0.4300 0.4203 0.4275 mean e-measure 0.3436 0.3688 0.4094 s measure 0.4305 0.4776 0.4649 AP 0.3213 0.3318 0.3165 AUC 0.5112 0.5161 0.5127 Table 1: Saliency detection results These evaluation metrics are the same as those used in [6], and better results among the three are depicted in bold. 4.2 Adaptive Bitrate Allocation Adaptive bitrate schemes typically use a tile-based approach [11] [15], in which video frames are split into rectangular tiles, for computational efficiency. Computing and assigning a unique bitrate to each pixel in the frame would be impractical, so tiling offers a feasible, yet still effective alternative. With tile-based approaches, tiles likely to be in or near the viewport area will be streamed in high quality, and others will be lower', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-11 18:03:48,747 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:03:48,748 [DEBUG]: send_request_headers.complete
2023-12-11 18:03:48,748 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:03:48,748 [DEBUG]: send_request_body.complete
2023-12-11 18:03:48,748 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:03:48,922 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:03:49 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'53'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999625'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'91cc2a2c290316737b7e9bdad81f68eb'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415cf29ccb6a5b-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:03:48,922 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-11 18:03:48,922 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:03:48,923 [DEBUG]: receive_response_body.complete
2023-12-11 18:03:48,923 [DEBUG]: response_closed.started
2023-12-11 18:03:48,923 [DEBUG]: response_closed.complete
2023-12-11 18:03:48,924 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-11 18:03:48,925 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1183259e0>, 'json_data': {'input': 'quality. In [3], various content-agnostic models, such as PanoS- alNet [10] with head-movement maps added, are used in adaptive bitrate schemes and evaluated in terms of quality- of-experience and viewport prediction accuracy. This project utilizes the source code and same basic bitrate allocation of [3], as well as one of the datasets they used [14]. Unlike Wild360, the dataset in [14] contains additional head move- ment data taken from dozens of user testers that can serve as ground truth annotations, or representations of actual user viewports. The original tiling scheme that [3] used with PanoSalNet had9Ã—16tiles, so all three models also used 9Ã—16tiles and the same bitrate allocation algorithm as in [3]. Figure 4: Bitrate allocation and Manhattan Tile Error algo- rithm Figure 5: Example tiling schemes [15] 5', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-11 18:03:48,926 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:03:48,926 [DEBUG]: send_request_headers.complete
2023-12-11 18:03:48,926 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:03:48,926 [DEBUG]: send_request_body.complete
2023-12-11 18:03:48,926 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:03:49,074 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:03:49 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'31'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999794'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'2ms'), (b'x-request-id', b'f8e2c36b34e625a3442f97c4c0c2e6de'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415cf3be2a6a5b-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:03:49,074 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-11 18:03:49,075 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:03:49,075 [DEBUG]: receive_response_body.complete
2023-12-11 18:03:49,075 [DEBUG]: response_closed.started
2023-12-11 18:03:49,075 [DEBUG]: response_closed.complete
2023-12-11 18:03:49,076 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-11 18:03:49,077 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1183dc540>, 'json_data': {'input': 'Figure 6: QoE aggregate score from [3] 4.3 Evaluation 4.3.1 Defining Quality-of-Experience 360-degree streaming projects typically define their own quality-of-experience (QoE) metric, which ideally allows them to quantify and compare the effectiveness of various viewport prediction methods. There is no universally agreed- upon standard for QoE, leaving room for subjectivity and author discretion. Therefore, a significant part of this project was also selecting and defining an appropriate QoE metric. The initial approach was to use the same QoE metric as defined in [3], as it considered four different aspects of a 360-degree viewing experience: 1) the average bitrate in a userâ€™s viewport for each frame 2) the variation of the bitrate in the viewport for each frame 3) the variation of bitrate across frames and 4) the variation of bitrate across successive chunks, or tiles. These four metrics are combined into an aggregate QoE score, as shown in Figure 6. However, examining the published code repository and running some small experiments demonstrated that it was difficult to compare models using this aggregated QoE score. Sometimes, QoE could be less than 0, and the results became less meaningful and interpretable. Therefore, this project ex- amines the four QOE metrics separately to extract insights of video quality. Since the frame rate was 1, QoE 3 is 0for every method, QoE 3 is omitted from the table of results. Furthermore, since the QoE score was the sum of all cal-', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-11 18:03:49,077 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:03:49,078 [DEBUG]: send_request_headers.complete
2023-12-11 18:03:49,078 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:03:49,078 [DEBUG]: send_request_body.complete
2023-12-11 18:03:49,078 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:03:49,220 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:03:49 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'29'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999626'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'ccc24e136e74d57cdbec415c3ead012a'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415cf4af196a5b-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:03:49,221 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-11 18:03:49,221 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:03:49,244 [DEBUG]: receive_response_body.complete
2023-12-11 18:03:49,244 [DEBUG]: response_closed.started
2023-12-11 18:03:49,244 [DEBUG]: response_closed.complete
2023-12-11 18:03:49,244 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-11 18:03:49,245 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1183dc540>, 'json_data': {'input': 'culated QoEs for all frames, it seemed that videos with more frames, as long as their QoEs were positive, would achieve better results. This makes it difficult to compare videos of different lengths, so in this project, QoE is divided by the number of frames in the end to achieve a fairer comparison. 4.3.2 Results As anticipated, the evaluation results of these three content- aware models are lower than the content-agnostic ones evalu- ated in [3]. Since content-agnostic approaches have access to the userâ€™s current viewport, they can assume that the userâ€™s next viewport will not deviate much from the current one, an assumption that tends to be correct. Content-agnostic meth- ods can also generate a unique predicted viewport for each user based on their individual head movement data, whereas for the content-aware ones, each userâ€™s individual movement was compared against the same model-generated prediction. The QoE results, as shown in Table 2 are significantly lower than those in [3]. However, as previously mentioned, this is to be expected, as the QoE results in this project are averaged per second, and the viewport accuracy is lower due Figure 7: QoE 1 Scores by Topic/Video to no head-movement data. We can also see that there is a very clear outlier for Topic 6, which VST achieves very high average quality for, which is explained more in the discussion section. VST TranSalNet QoE 1 0.00757 0.00873 QoE 2 0.01755 0.0122 QoE 4 0.00649 0.00200', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-11 18:03:49,246 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:03:49,247 [DEBUG]: send_request_headers.complete
2023-12-11 18:03:49,247 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:03:49,247 [DEBUG]: send_request_body.complete
2023-12-11 18:03:49,247 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:03:49,390 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:03:49 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'30'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999632'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'1b5ab06d53292542256e0bbabf819816'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415cf5b8506a5b-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:03:49,391 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-11 18:03:49,391 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:03:49,392 [DEBUG]: receive_response_body.complete
2023-12-11 18:03:49,392 [DEBUG]: response_closed.started
2023-12-11 18:03:49,392 [DEBUG]: response_closed.complete
2023-12-11 18:03:49,392 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-11 18:03:49,393 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1183dc540>, 'json_data': {'input': 'QoE 2 0.01755 0.0122 QoE 4 0.00649 0.00200 Table 2: Median QoE scores across all videos. QoE 1: average bitrate in viewport QoE 2: variation of bitrate in viewport QoE 4: variation of bitrate across chunks However, the Manhattan Tile Error, a measure of viewport prediction accuracy, is promising. Figure 8 illustrates how the Manhattan distance, is typically calculated. The Manhattan Tile Error is then the minimum tile distance between the actual viewport tile and the predicted viewport tile. Table 3 displays the Manhattan Tile Error for the selected models, TranSalNet and VST, and Table 4 displays the Manhattan Tile Error for two models compared in [3]. Again, given that they have access to past head movement data, the models from [3] have predictably better Manhattan Tile Errors than saliency-only models. Furthermore, PARIMA understandably [3] outperforms PanoSalNet with head move- ment data because its authors defined the exact bitrate alloca- tion scheme and conducted the evaluations themselves. 5 Discussion A few major takeaways can be gleaned from these results,', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-11 18:03:49,394 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:03:49,395 [DEBUG]: send_request_headers.complete
2023-12-11 18:03:49,395 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:03:49,395 [DEBUG]: send_request_body.complete
2023-12-11 18:03:49,395 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:03:49,546 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:03:49 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'30'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999730'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'3ms'), (b'x-request-id', b'24dccb825c32bb06a151c10532d8c72d'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415cf6a9516a5b-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:03:49,546 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-11 18:03:49,546 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:03:49,551 [DEBUG]: receive_response_body.complete
2023-12-11 18:03:49,552 [DEBUG]: response_closed.started
2023-12-11 18:03:49,552 [DEBUG]: response_closed.complete
2023-12-11 18:03:49,552 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-11 18:03:49,553 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1183dc540>, 'json_data': {'input': 'ment data because its authors defined the exact bitrate alloca- tion scheme and conducted the evaluations themselves. 5 Discussion A few major takeaways can be gleaned from these results, mainly the applicability of pre-trained models in computer vision and limitations and gaps in the field of 360-degree livestreaming. This discussion aims to provide explanations and theories for notable areas in the results, as well as guide future work and ideas for exploration. 6', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-11 18:03:49,554 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:03:49,555 [DEBUG]: send_request_headers.complete
2023-12-11 18:03:49,555 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:03:49,555 [DEBUG]: send_request_body.complete
2023-12-11 18:03:49,555 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:03:49,703 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:03:49 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'22'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999882'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'1ms'), (b'x-request-id', b'492e0f56f65985bfdfd101bb445c21c2'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415cf7aa7e6a5b-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:03:49,704 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-11 18:03:49,704 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:03:49,710 [DEBUG]: receive_response_body.complete
2023-12-11 18:03:49,711 [DEBUG]: response_closed.started
2023-12-11 18:03:49,711 [DEBUG]: response_closed.complete
2023-12-11 18:03:49,711 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-11 18:03:49,712 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1183dc540>, 'json_data': {'input': 'Figure 8: Manhattan vs. Euclidean distance [8] VST TranSalNet 0 5.53 7.18 1 6.71 7.55 2 7.69 8.43 3 6.61 9.38 4 8.01 8.13 5 7.18 7.71 6 5.93 8.66 7 8.20 8.26 8 8.18 8.35 Table 3: Manhattan Tile Error for tested models (fps=1) PARIMA PanoSalNet 0 0.14 1.65 1 0.16 2.69 2 0.15 2.35 3 0.13 2.25 4 0.18 1.63 5 0.18 1.66 6 0.16 1.36 7 0.18 2.51 8 0.10 2.42 Table 4: Original Manhattan Tile Error for models with HMD information [3] (fps=1) 5.1 Applicability of VST/TranSalNet One goal of this project was to determine whether pre-trained, open-source transformers [6] or transformer-like models [7] could accurately identify salient areas of a 360-degree video frame. The evaluation results demonstrate that "out-of-the- box" VST or TranSalNet, without any further training or mod- ifications, cannot be effectively applied to 360-degree video use cases. This can be partly attributed to the fact that VSTâ€™s and Figure 9: A 360-degree frame, and its corresponding VST- generated saliency map TranSalNetâ€™s training data of 2D images differs greatly from the format of 360-degree video frames. Visually examining some of VSTâ€™s generated saliency maps shows that at times, the model struggles with the distortion of these 360-degree frames. As demonstrated by Figure 9, the model sometimes selects extremely distorted objects, such as the aircraftâ€™s wing, even though humans may not deem it a visually salient feature. Another barrier to using VST, and to some extent, TranSal-', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-11 18:03:49,713 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:03:49,714 [DEBUG]: send_request_headers.complete
2023-12-11 18:03:49,714 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:03:49,714 [DEBUG]: send_request_body.complete
2023-12-11 18:03:49,714 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:03:49,894 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:03:50 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'69'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999631'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'f68240f907775656a150ed55a8e6712a'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415cf8ab7f6a5b-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:03:49,894 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-11 18:03:49,895 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:03:49,924 [DEBUG]: receive_response_body.complete
2023-12-11 18:03:49,924 [DEBUG]: response_closed.started
2023-12-11 18:03:49,924 [DEBUG]: response_closed.complete
2023-12-11 18:03:49,924 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-11 18:03:49,925 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1183259e0>, 'json_data': {'input': 'Net as well, for adaptive bitrate allocation is that the saliency maps they generate are mostly binary. Pixels are either white (salient) or black (not salient), which works for saliency evalu- ation but not nuanced head movement prediction and adaptive bitrate allocation, in which differences in bitrates should ide- ally be gradual. VST especially generates saliency maps with regions that are either sharply white or sharply black. TranSalNet theoretically achieves better QoE results than VST in terms of median average viewport bitrate, variation of bitrate in the viewport, and variation of bitrate between nearby chunks â€“ despite having lower Manhattan Tile Errors on nearly every video. Visually examining the differences between the two modelsâ€™ saliency maps may also explain why. TranSalNet tends to generate more subtle saliency maps that gradually shift from white to black, and therefore ends up allocating more medium bitrates to moderately salient/salient- adjacent regions, which leads to better quality streaming. Purely content-aware approaches are also more suited to 2D static images, as they typically assume that users will always be drawn to the most salient object in an image. However, with a 360-degree video, users are more likely to explore and gradually glance around, sometimes examining less salient details. Additionally, because they do not have the full context of the scene, they also may be more drawn to these less salient', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-11 18:03:49,926 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:03:49,927 [DEBUG]: send_request_headers.complete
2023-12-11 18:03:49,927 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:03:49,927 [DEBUG]: send_request_body.complete
2023-12-11 18:03:49,927 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:03:50,073 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:03:50 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'32'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999633'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'13dfb5306daac2b9eae9880140665b20'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415cf9fd2c6a5b-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:03:50,073 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-11 18:03:50,074 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:03:50,074 [DEBUG]: receive_response_body.complete
2023-12-11 18:03:50,074 [DEBUG]: response_closed.started
2023-12-11 18:03:50,074 [DEBUG]: response_closed.complete
2023-12-11 18:03:50,074 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-11 18:03:50,075 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1183259e0>, 'json_data': {'input': 'details. Therefore, integrating head and eye-tracking data is crucial to improving viewport prediction. Finetuning or training both models from scratch could also improve performance, especially if 360-degree video data is used. It is unclear how much of the better performance of PanoSalNet and PARIMA can be attributed to their use of head-movement data, and how much is due to their training on 360-degree video. 7', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-11 18:03:50,076 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:03:50,076 [DEBUG]: send_request_headers.complete
2023-12-11 18:03:50,077 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:03:50,077 [DEBUG]: send_request_body.complete
2023-12-11 18:03:50,077 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:03:50,219 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:03:50 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'23'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999895'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'1ms'), (b'x-request-id', b'e0233dc6f54eea83a3954bb4945bf1d9'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415cfaee676a5b-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:03:50,220 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-11 18:03:50,220 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:03:50,254 [DEBUG]: receive_response_body.complete
2023-12-11 18:03:50,254 [DEBUG]: response_closed.started
2023-12-11 18:03:50,254 [DEBUG]: response_closed.complete
2023-12-11 18:03:50,254 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-11 18:03:50,256 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1183259e0>, 'json_data': {'input': 'Figure 10: Frame from Topic 6 (top) and generated saliency maps from VST (bottom left) and TranSalNet (bottom right) 5.2 Other Takeaways One interesting takeaway from the results is that VST per- formed especially well on one specific video, Topic 6, which is shown in Figure 10. Intuitively, one would expect the saliency maps from TranSalNet to be better representations of where users are likely to gaze, as they identify the figures, tables, and middle of the scene as the more salient areas of the frame. VST, on the other hand, seems to mark the distorted wooden beams at the very top of the frame, as salient. However, VST achieves far higher average viewport quality than TranSalNet, as shown in Figure 7. Video 6 is notable in that it is one of the few videos in the dataset that takes place indoors and has extremely noticeable ceiling distortions. Therefore, it is possible that VST is better at generating pre- dictions for distorted indoor videos, and perhaps users do tend to look at higher areas in these videos and find them more salient. However, more sample videos would be needed to conclude that VST is significantly better at these videos. Another important note is that given the relative speed of the saliency map generation and adaptive bitrate allocation, as well as how the model would be pre-trained and ready to make inferences, these techniques would be feasible in livestream- ing applications. For instance, these processes typically take', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-11 18:03:50,257 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:03:50,257 [DEBUG]: send_request_headers.complete
2023-12-11 18:03:50,257 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:03:50,258 [DEBUG]: send_request_body.complete
2023-12-11 18:03:50,258 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:03:50,397 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:03:50 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'27'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999632'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'31cb7bd7009a42cc68443bff9b3b3aff'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415cfc0f926a5b-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:03:50,398 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-11 18:03:50,398 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:03:50,398 [DEBUG]: receive_response_body.complete
2023-12-11 18:03:50,398 [DEBUG]: response_closed.started
2023-12-11 18:03:50,399 [DEBUG]: response_closed.complete
2023-12-11 18:03:50,399 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-11 18:03:50,400 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1183259e0>, 'json_data': {'input': 'only a few milliseconds per frame, and this project tested a frame rate of 1fps. Therefore, it would be possible to test this workflow with a live, real-world streaming system and human users. 6 Future Work While the basic evaluation of content-aware adaptive bitrate allocation methods has been completed, this project will be further extended into the spring semester to allow for further work in quality optimization. The primary goals of future work will be to address the limited applicability of VST andother pre-trained 2D image models, as mentioned in the dis- cussion. 6.1 Main Goals 6.1.1 Saliency-Only PanoSalNet The code for PanoSalNet is open-source and was used to produce the saliency results in Table 1. However, due to de- pendency issues and outdated packages, it was difficult to generate more saliency maps for the new dataset and compare them to VST and TranSalNet. This paper was therefore only able to compare those two models to published PanoSalNet results with head-movement data. It would be helpful to then find a way to fix the dependency and compatibility issues to have a more direct comparison of saliency-only models and their performance on adaptive bitrate allocation. 6.1.2 Transfer Learning, Finetuning, and Training Given the high compute required to train transformers and transformer-like models, this project did not include any fine- tuning or additional training. However, in a future semester, this project could explore fine-tuning the pre-trained two', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-11 18:03:50,400 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:03:50,401 [DEBUG]: send_request_headers.complete
2023-12-11 18:03:50,401 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:03:50,401 [DEBUG]: send_request_body.complete
2023-12-11 18:03:50,401 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:03:50,589 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:03:50 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'38'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999625'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'2d024501e49380361ec3fa29525fe125'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415cfcf8816a5b-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:03:50,589 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-11 18:03:50,590 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:03:50,593 [DEBUG]: receive_response_body.complete
2023-12-11 18:03:50,593 [DEBUG]: response_closed.started
2023-12-11 18:03:50,593 [DEBUG]: response_closed.complete
2023-12-11 18:03:50,593 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-11 18:03:50,594 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1183259e0>, 'json_data': {'input': 'transformer-like models, VST and TranSalNet, on 360-degree video frames, since their original inputs are 2D images. This would be similar to the work completed in PanoSalNet [10] and could likely reduce some of the saliency detection errors involved with these models. Another option could be fully training the initial models on 360-degree videos only, as the codebases provide the option of training from scratch. 6.1.3 Adding Content-Agnostic Methods Given the effectiveness of past content-agnostic methods on viewport prediction and adaptive bitrate allocation [15] [3], as well as the enhanced performance provided by combined content-agnostic and content-aware methods [9], the QoE could be greatly improved by adding head-movement infor- mation. This is one of the first priorities of the next semester, and it should be easy to integrate the LSTM architecture published in [10]. 6.1.4 Testing Different Tiling Schemes and Frame Rates Currently, the tiling scheme used was 9Ã—16, but these dimen-', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-11 18:03:50,595 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:03:50,596 [DEBUG]: send_request_headers.complete
2023-12-11 18:03:50,596 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:03:50,596 [DEBUG]: send_request_body.complete
2023-12-11 18:03:50,597 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:03:50,760 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:03:50 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'34'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999749'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'3ms'), (b'x-request-id', b'042a71251a3fca269e5fbcefc07715ff'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415cfe29d26a5b-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:03:50,761 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-11 18:03:50,761 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:03:50,783 [DEBUG]: receive_response_body.complete
2023-12-11 18:03:50,783 [DEBUG]: response_closed.started
2023-12-11 18:03:50,783 [DEBUG]: response_closed.complete
2023-12-11 18:03:50,783 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-11 18:03:50,784 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1183dc540>, 'json_data': {'input': 'should be easy to integrate the LSTM architecture published in [10]. 6.1.4 Testing Different Tiling Schemes and Frame Rates Currently, the tiling scheme used was 9Ã—16, but these dimen- sions are slightly larger than those of other works [11] [3]. Therefore, it may be better to try out schemes with fewer tiles, such as an 8Ã—8one like [3] did, and compare the results in future work. Testing smaller and larger frame rates could also yield interesting takeaways for QoE analysis. 8', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-11 18:03:50,785 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:03:50,785 [DEBUG]: send_request_headers.complete
2023-12-11 18:03:50,785 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:03:50,786 [DEBUG]: send_request_body.complete
2023-12-11 18:03:50,786 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:03:50,969 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:03:51 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'53'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999880'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'1ms'), (b'x-request-id', b'018eddbbd090c58de3a8c10111996315'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415cff5b256a5b-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:03:50,969 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-11 18:03:50,970 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:03:50,995 [DEBUG]: receive_response_body.complete
2023-12-11 18:03:50,996 [DEBUG]: response_closed.started
2023-12-11 18:03:50,996 [DEBUG]: response_closed.complete
2023-12-11 18:03:50,996 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-11 18:03:50,997 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1183dc540>, 'json_data': {'input': '6.2 Stretch Goals One major stretch goal for the following semester would be to deploy an end-to-end system using the finalized adaptive bitrate allocation scheme. Other objectives include defining a subjective QoE metric and conducting real-world testing with existing open-source tools for VR surveys and tests [15]. Participants could use a VR headset to interact with a sample of videos and rate and qualitatively describe their experience. Measuring QoE through participantsâ€™ subjective assessments could capture the real, human experience of viewing a 360 Â°livestream and gauge the effectiveness of the new adaptive bitrate scheme in a real-world setting. 7 Acknowledgements I would like to thank my advisor, Dr. Sohee Park, and her mentorship and guidance throughout this semester. I began this project without any knowledge of 360-degree streaming, and I am grateful for her continuous support and patience. I would also like to acknowledge Rachel Liang, the 490 teach- ing assistant, as well as the many computer science professors and friends who have helped encourage me throughout my time here at Yale. I cannot imagine being able to make it to the senior thesis without their help and kindness! References [1]F. Y . Chao, C. Ozcinar, and A. Smolic. Transformer- based long-term viewport prediction in 360 Â°video: Scanpath is all you need. In IEEE 23nd International Workshop on Multimedia Signal Processing (MMSP) , 2021. [2]H. Cheng, C. Chao, J. Dong, H. Wen, T. Liu, and M. Sun.', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-11 18:03:50,998 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:03:50,998 [DEBUG]: send_request_headers.complete
2023-12-11 18:03:50,998 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:03:50,998 [DEBUG]: send_request_body.complete
2023-12-11 18:03:50,998 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:03:51,152 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:03:51 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'28'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999625'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'5daa47a131bbf0385590f1dbc253bb8d'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415d00aca36a5b-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:03:51,152 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-11 18:03:51,153 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:03:51,166 [DEBUG]: receive_response_body.complete
2023-12-11 18:03:51,166 [DEBUG]: response_closed.started
2023-12-11 18:03:51,166 [DEBUG]: response_closed.complete
2023-12-11 18:03:51,167 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-11 18:03:51,168 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1183dc540>, 'json_data': {'input': 'Cube padding for weakly-supervised saliency prediction in 360 Â°videos. In 2018 IEEE/CVF Conference on Com- puter Vision and Pattern Recognition (CVPR) , pages 1420â€“1429, Los Alamitos, CA, USA, jun 2018. IEEE Computer Society. [3]Lovish Chopra, Sarthak Chakraborty, Abhijit Mondal, and Sandip Chakraborty. Parima: Viewport adaptive 360-degree video streaming. In Proceedings of the Web Conference 2021 , WWW â€™21. ACM, April 2021. [4]Xianglong Feng, Yao Liu, and Sheng Wei. Livedeep: On- line viewport prediction for live virtual reality streaming using lifelong deep learning. In 2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR) , pages 800â€“808, 2020. [5]Quentin Guimard, Lucile Sassatelli, Francesco Marchetti, Federico Becattini, Lorenzo Seidenari, andAlberto Del Bimbo. Deep variational learning for multiple trajectory prediction of 360 Â°head movements. InProceedings of the 13th ACM Multimedia Systems Conference , MMSys â€™22, page 12â€“26, New York, NY , USA, 2022. Association for Computing Machinery. [6]Nian Liu, Ni Zhang, Kaiyuan Wan, Junwei Han, and Ling Shao. Visual saliency transformer. CoRR , abs/2104.12099, 2021. [7]Jianxun Lou, Hanhe Lin, David Marshall, Dietmar Saupe, and Hantao Liu. Transalnet: Towards perceptu- ally relevant visual saliency prediction. Neurocomput- ing, 494:455â€“467, 2022. [8]Vinh-Trung Luu, Germain Forestier, Jonathan Weber, Paul Bourgeois, Fahima Djelil, and Pierre-Alain Muller. A review of alignment based similarity measures for', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-11 18:03:51,168 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:03:51,169 [DEBUG]: send_request_headers.complete
2023-12-11 18:03:51,169 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:03:51,169 [DEBUG]: send_request_body.complete
2023-12-11 18:03:51,169 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:03:51,339 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:03:51 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'32'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999625'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'33a3b9b9aa602f3bf77953db43e387c5'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415d01cdf36a5b-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:03:51,339 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-11 18:03:51,340 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:03:51,340 [DEBUG]: receive_response_body.complete
2023-12-11 18:03:51,340 [DEBUG]: response_closed.started
2023-12-11 18:03:51,340 [DEBUG]: response_closed.complete
2023-12-11 18:03:51,341 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-11 18:03:51,341 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1183dc540>, 'json_data': {'input': 'web usage mining. Artificial Intelligence Review , 53, 03 2020. [9]Anh Nguyen and Zhisheng Yan. Enhancing 360 video streaming through salient content in head-mounted dis- plays. Sensors , 23(8), 2023. [10] Anh Nguyen, Zhisheng Yan, and Klara Nahrstedt. Your attention is unique: Detecting 360-degree video saliency in head-mounted display for head movement prediction. pages 1190â€“1198, 10 2018. [11] Sohee Park, Arani Bhattacharya, Zhibo Yang, Samir R. Das, and Dimitris Samaras. Mosaic: Advancing user quality of experience in 360-degree video streaming with machine learning. IEEE Transactions on Network and Service Management , 18(1):1000â€“1015, 2021. [12] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023. [13] Shibo Wang, Shusen Yang, Hailiang Li, Xiaodan Zhang, Chen Zhou, Chenren Xu, Feng Qian, Nanbin Wang, and Zongben Xu. Salientvr: Saliency-driven mobile 360-', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-11 18:03:51,342 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:03:51,343 [DEBUG]: send_request_headers.complete
2023-12-11 18:03:51,343 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:03:51,343 [DEBUG]: send_request_body.complete
2023-12-11 18:03:51,343 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:03:51,508 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:03:51 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'34'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999755'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'2ms'), (b'x-request-id', b'78ca2b5d7b70a0ad853b2599f41cf5dd'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415d02ef5a6a5b-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:03:51,508 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-11 18:03:51,509 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:03:51,530 [DEBUG]: receive_response_body.complete
2023-12-11 18:03:51,530 [DEBUG]: response_closed.started
2023-12-11 18:03:51,530 [DEBUG]: response_closed.complete
2023-12-11 18:03:51,531 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-11 18:03:51,532 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1183dc540>, 'json_data': {'input': '[13] Shibo Wang, Shusen Yang, Hailiang Li, Xiaodan Zhang, Chen Zhou, Chenren Xu, Feng Qian, Nanbin Wang, and Zongben Xu. Salientvr: Saliency-driven mobile 360- degree video streaming with gaze information. In Pro- ceedings of the 28th Annual International Conference on Mobile Computing And Networking , MobiCom â€™22, page 542â€“555, New York, NY , USA, 2022. Association for Computing Machinery. [14] Chenglei Wu, Zhihao Tan, Zhi Wang, and Shiqiang Yang. A dataset for exploring user behaviors in vr spherical video streaming. Proceedings of the 8th ACM on Multi- media Systems Conference , 2017. [15] Abid Yaqoob, Ting Bi, and Gabriel-Miro Muntean. A survey on adaptive 360 Â°video streaming: Solutions, challenges and opportunities. IEEE Communications Surveys Tutorials , 22(4):2801â€“2838, 2020. 9', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-11 18:03:51,533 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:03:51,533 [DEBUG]: send_request_headers.complete
2023-12-11 18:03:51,533 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:03:51,533 [DEBUG]: send_request_body.complete
2023-12-11 18:03:51,533 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:03:51,685 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:03:51 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'37'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999799'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'2ms'), (b'x-request-id', b'376953cdc12f7fced30eeb5a9c7f4dcd'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415d0408a36a5b-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:03:51,686 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-11 18:03:51,686 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:03:51,711 [DEBUG]: receive_response_body.complete
2023-12-11 18:03:51,711 [DEBUG]: response_closed.started
2023-12-11 18:03:51,711 [DEBUG]: response_closed.complete
2023-12-11 18:03:51,712 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-11 18:03:51,713 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1183259e0>, 'json_data': {'input': '[16] Hyunho Yeo, Hwijoon Lim, Jaehong Kim, Youngmok Jung, Juncheol Ye, and Dongsu Han. Neuroscaler: Neu- ral video enhancement at scale. In Proceedings of the ACM SIGCOMM 2022 Conference , SIGCOMM â€™22, page 795â€“811, New York, NY , USA, 2022. Association for Computing Machinery. 10', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-11 18:03:51,713 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:03:51,714 [DEBUG]: send_request_headers.complete
2023-12-11 18:03:51,714 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:03:51,714 [DEBUG]: send_request_body.complete
2023-12-11 18:03:51,714 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:03:51,856 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:03:51 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'22'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999928'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'66ed64184f6d6e5e7a759dd5e1aedda5'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415d0529c96a5b-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:03:51,857 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-11 18:03:51,857 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:03:51,857 [DEBUG]: receive_response_body.complete
2023-12-11 18:03:51,858 [DEBUG]: response_closed.started
2023-12-11 18:03:51,858 [DEBUG]: response_closed.complete
2023-12-11 18:03:51,858 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-11 18:03:51,859 [DEBUG]: Embedded chunks
2023-12-11 18:03:52,957 [DEBUG]: Upserted vectors into index
2023-12-11 18:03:52,959 [INFO]: 127.0.0.1 - - [11/Dec/2023 18:03:52] "POST /api/generate_embeddings HTTP/1.1" 200 -
2023-12-11 18:03:53,737 [INFO]: 127.0.0.1 - - [11/Dec/2023 18:03:53] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-11 18:03:58,986 [DEBUG]: Using selector: KqueueSelector
2023-12-11 18:03:58,987 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-11 18:03:58,988 [DEBUG]: load_verify_locations cafile='/Users/aliceao/opt/miniconda3/envs/chat-lore/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-11 18:03:59,042 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x11840e8e0>, 'json_data': {'input': 'What are some common limitations found in these research papers?', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-11 18:03:59,043 [DEBUG]: close.started
2023-12-11 18:03:59,043 [DEBUG]: close.complete
2023-12-11 18:03:59,043 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-11 18:03:59,058 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x11835ad10>
2023-12-11 18:03:59,058 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x11824b650> server_hostname='api.openai.com' timeout=5.0
2023-12-11 18:03:59,070 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1156e6590>
2023-12-11 18:03:59,070 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:03:59,071 [DEBUG]: send_request_headers.complete
2023-12-11 18:03:59,071 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:03:59,071 [DEBUG]: send_request_body.complete
2023-12-11 18:03:59,071 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:03:59,201 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:03:59 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'20'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999983'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'70c74f19629a71fd1491d8682e4b0a40'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415d331bd41921-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:03:59,201 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-11 18:03:59,202 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:03:59,209 [DEBUG]: receive_response_body.complete
2023-12-11 18:03:59,209 [DEBUG]: response_closed.started
2023-12-11 18:03:59,210 [DEBUG]: response_closed.complete
2023-12-11 18:03:59,210 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-11 18:04:00,220 [DEBUG]: Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'AI assistant is a brand new, powerful, human-like artificial intelligence.\n                    The traits of AI include expert knowledge, helpfulness, cleverness, and articulateness.\n                    AI is a well-behaved and well-mannered individual.\n                    AI is always friendly, kind, and inspiring, and he is eager to provide vivid and thoughtful responses to the user.\n                    AI has the sum of all knowledge in their brain, and is able to accurately answer nearly any question about any topic in conversation.\n                    AI assistant is a big fan of Pinecone and Vercel.\n                    START CONTEXT BLOCK\n                    2-ThisisasupercoolideaandIlikehowitclearlyalignswithyourinterests!Ijustwonder1)ifyoucanmodifytheoutputtomakeitalittlebitmorereadable,maybeplaywithASCIIcharactersandmakeitmoregame-like?and2)thereasoningbehinddesignchoicesyoumake,footballisobviouslysupercomplexandIâ€™massumingyouâ€™resimplifyingitforyourgame,whatandwhyareyouchoosingtoleaveout?Itseemslikeyouâ€™reofftoagoodstartthough!\n4-Again,lovetheideaandcandeï¬nitelyseethepassionbehindit!100+coldemailsiscrazyandIreallyrespecttheamountofworkthatâ€™salreadygoneintothisproject.Ijustwouldlovetoknowmoredetailsabouttheregressionmodeling/transferlearningpartofthisproject\n6-Thisissuperinteresting,Iâ€™veneverreallythoughtabouthowcomplicatedadishwashinggamecouldbe,butIhonestlylovehowout-of-the-boxthisisandhowyouâ€™veputatonofthoughtintoit.Mymainsuggestionwouldbetojustaddmoretexttotheslidesâ€”Iknowthattheuseofvisualsisintentionalbutfororganizationalpurposes,itcouldbegoodtostartwithatextoverviewandsupplementvisualswithcaptions.\n8-CouldbebecauseImissedtheproposal(andmaybetherewasalongeroverviewthere),butIwouldâ€™veappreciatedalongerhigh-leveloverview/statementofpurpose.However,Ireallyrespecthowtechnicalthisprojectisandhowitâ€™sdeï¬nitelystronglyrootedintheory\n10-Ilikethebackgroundandoverview,eventhoughIâ€™mnotfamiliarwiththetopic,theslidesreallyhelpillustratethepurposeoftheproject.Myonlysuggestionwouldbetocutdownsomeofthetextontheslides.\nCPSC 488/588 - AI Foundation Models HW 3 Page 3\nproposed system level method to improve the efficiency of inference in Transformers. We did not cover\nthis in class. Please read the paper and answer the following questions.\n(a) (5 points) Summarize how the proposed approach works on high-level (no more than 5 sentences).\nDo you think such approach can be combined with FlashAttention?\nSolution:\n5-sentence summary: Prompt Cache reduces the computational overhead of, and thus acceler-\nates, LLM inference by reusing attention states from different input prompts and supporting\nmodularity to maximize reuse. It takes advantage of the fact that prompts often have signifi-\ncant text overlap, sharing frequently occurring phrases/segments of text (e.g. system messages,\ndomain-specific documents, templates). Prompt Cache pre-computes the attention states of\nthese frequent text segments as prompt modules and stores them in a modular key-value cache,\nso that when an LLM encounters a frequent text segment, it does not need to re-compute full\nattention for its tokens. It requires users to write prompts in Prompt Markup Language (PML),\nin which they use a schema to define prompt modules, which can be parameterized to maximize\nreuse and allow for customization. Discontinuous position IDs are key to caching prompts, so\nmodifications must be made to some transformer architectures to support these position IDs.\nwindow size from bottom to top, which the LongFormer paper shows achieves good experimental\nresults. Iâ€™d start with a small window size of 32 on the lowest layer, which focuses on local\ncontext and is computatio\n                    END OF CONTEXT BLOCK\n                    AI assistant will take into account any CONTEXT BLOCK that is provided in a conversation.\n                    If the context does not provide the answer to question, the AI assistant will say, "I\'m sorry, but I don\'t know the answer to that question".\n                    AI assistant will not apologize for previous responses, but instead will indicated new information was gained.\n                    AI assistant will not invent anything that is not drawn directly from the context.\n                    '}, {'role': 'user', 'content': 'What are some common limitations found in these research papers?'}], 'model': 'gpt-3.5-turbo'}}
2023-12-11 18:04:00,221 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-11 18:04:00,231 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x11839a210>
2023-12-11 18:04:00,232 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x11824bb60> server_hostname='api.openai.com' timeout=5.0
2023-12-11 18:04:00,246 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x11839b410>
2023-12-11 18:04:00,246 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:04:00,246 [DEBUG]: send_request_headers.complete
2023-12-11 18:04:00,247 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:04:00,247 [DEBUG]: send_request_body.complete
2023-12-11 18:04:00,247 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:04:05,930 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:04:06 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'5308'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'160000'), (b'x-ratelimit-limit-tokens_usage_based', b'160000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'158899'), (b'x-ratelimit-remaining-tokens_usage_based', b'158899'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'412ms'), (b'x-ratelimit-reset-tokens_usage_based', b'412ms'), (b'x-request-id', b'd37bb6ef1dcca311ebfd380ca98effab'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=XL_hqDeR5SnJrD7BaoHIpXELDrCNktM7IUS04hN0gcA-1702335846-1-AXPRPlrUHErojlpgyi9bdED1dfsBJOnTwSC5UStFWGUOswCTxx4MH/c2B5n3Lf3ttsHYAiNTU7P09Fe/XZFe1dk=; path=/; expires=Mon, 11-Dec-23 23:34:06 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=HfR3.D0.qaJQfSbKMNGYmrpTYdR4xbgoAdXQp5HUe0c-1702335846067-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415d3a8a2942c4-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:04:05,930 [INFO]: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2023-12-11 18:04:05,931 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:04:05,931 [DEBUG]: receive_response_body.complete
2023-12-11 18:04:05,931 [DEBUG]: response_closed.started
2023-12-11 18:04:05,931 [DEBUG]: response_closed.complete
2023-12-11 18:04:05,931 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2023-12-11 18:04:05,933 [INFO]: 127.0.0.1 - - [11/Dec/2023 18:04:05] "POST /api/chat HTTP/1.1" 200 -
2023-12-11 18:04:05,944 [DEBUG]: Request received for answer evaluation
2023-12-11 18:04:05,947 [DEBUG]: Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'text-davinci-003', 'prompt': "Please rate the faithfulness of the answers to their contexts on a scale from 0 to 1, where 0 is completely unfaithful and 1 is completely faithful.\nExample 1:\nContext: The sky is blue.\nAnswer: The sky is blue.\nRating: 1\nExample 2:\nContext: The sky is blue.\nAnswer: The sky is often cloudy.\nRating: 0.5\nExample 3:\nContext: Apples are usually red.\nAnswer: Oranges are orange.\nRating: 0\nYour Turn:\nContext: 2-ThisisasupercoolideaandIlikehowitclearlyalignswithyourinterests!Ijustwonder1)ifyoucanmodifytheoutputtomakeitalittlebitmorereadable,maybeplaywithASCIIcharactersandmakeitmoregame-like?and2)thereasoningbehinddesignchoicesyoumake,footballisobviouslysupercomplexandIâ€™massumingyouâ€™resimplifyingitforyourgame,whatandwhyareyouchoosingtoleaveout?Itseemslikeyouâ€™reofftoagoodstartthough!\n4-Again,lovetheideaandcandeï¬nitelyseethepassionbehindit!100+coldemailsiscrazyandIreallyrespecttheamountofworkthatâ€™salreadygoneintothisproject.Ijustwouldlovetoknowmoredetailsabouttheregressionmodeling/transferlearningpartofthisproject\n6-Thisissuperinteresting,Iâ€™veneverreallythoughtabouthowcomplicatedadishwashinggamecouldbe,butIhonestlylovehowout-of-the-boxthisisandhowyouâ€™veputatonofthoughtintoit.Mymainsuggestionwouldbetojustaddmoretexttotheslidesâ€”Iknowthattheuseofvisualsisintentionalbutfororganizationalpurposes,itcouldbegoodtostartwithatextoverviewandsupplementvisualswithcaptions.\n8-CouldbebecauseImissedtheproposal(andmaybetherewasalongeroverviewthere),butIwouldâ€™veappreciatedalongerhigh-leveloverview/statementofpurpose.However,Ireallyrespecthowtechnicalthisprojectisandhowitâ€™sdeï¬nitelystronglyrootedintheory\n10-Ilikethebackgroundandoverview,eventhoughIâ€™mnotfamiliarwiththetopic,theslidesreallyhelpillustratethepurposeoftheproject.Myonlysuggestionwouldbetocutdownsomeofthetextontheslides.\nCPSC 488/588 - AI Foundation Models HW 3 Page 3\nproposed system level method to improve the efficiency of inference in Transformers. We did not cover\nthis in class. Please read the paper and answer the following questions.\n(a) (5 points) Summarize how the proposed approach works on high-level (no more than 5 sentences).\nDo you think such approach can be combined with FlashAttention?\nSolution:\n5-sentence summary: Prompt Cache reduces the computational overhead of, and thus acceler-\nates, LLM inference by reusing attention states from different input prompts and supporting\nmodularity to maximize reuse. It takes advantage of the fact that prompts often have signifi-\ncant text overlap, sharing frequently occurring phrases/segments of text (e.g. system messages,\ndomain-specific documents, templates). Prompt Cache pre-computes the attention states of\nthese frequent text segments as prompt modules and stores them in a modular key-value cache,\nso that when an LLM encounters a frequent text segment, it does not need to re-compute full\nattention for its tokens. It requires users to write prompts in Prompt Markup Language (PML),\nin which they use a schema to define prompt modules, which can be parameterized to maximize\nreuse and allow for customization. Discontinuous position IDs are key to caching prompts, so\nmodifications must be made to some transformer architectures to support these position IDs.\nwindow size from bottom to top, which the LongFormer paper shows achieves good experimental\nresults. Iâ€™d start with a small window size of 32 on the lowest layer, which focuses on local\ncontext and is computatio\nAnswer: Based on the given context, I can provide some common limitations found in research papers in general. However, without specific information about the research papers mentioned in the context, I cannot provide specific limitations for those papers. Here are some common limitations found in research papers:\n\n1. Small Sample Size: Research papers often have a limited sample size, which can limit the generalizability of the findings.\n\n2. Limited Scope: Research papers may focus on a specific aspect or phenomenon, which may not fully capture the complexity of the topic.\n\n3. Methodological Limitations: Research papers may have limitations in their research design or methodology, such as biased sampling, measurement errors, or confounding variables. These limitations can affect the validity and reliability of the results.\n\n4. Generalizability: The findings of research papers may not be applicable to other populations, contexts, or timeframes. This limits the generalizability of the findings.\n\n5. Publication Bias: Research papers that report statistically significant or positive results are more likely to be published, while studies with null or negative findings may go unpublished. This can result in an overrepresentation of certain findings and limit the overall understanding of a topic.\n\nIt's important to note that these limitations can vary depending on the specific research papers being discussed.\nRating:", 'max_tokens': 60}}
2023-12-11 18:04:05,948 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-11 18:04:05,966 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1183562d0>
2023-12-11 18:04:05,966 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x1157a5370> server_hostname='api.openai.com' timeout=5.0
2023-12-11 18:04:05,980 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x11839a8d0>
2023-12-11 18:04:05,980 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:04:05,980 [DEBUG]: send_request_headers.complete
2023-12-11 18:04:05,980 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:04:05,981 [DEBUG]: send_request_body.complete
2023-12-11 18:04:05,981 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:04:06,380 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:04:06 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'text-davinci-003'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'275'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-limit-tokens_usage_based', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'249940'), (b'x-ratelimit-remaining-tokens_usage_based', b'249940'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'14ms'), (b'x-ratelimit-reset-tokens_usage_based', b'14ms'), (b'x-request-id', b'59dc4b3aba67a6252aeb1c5893450362'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=c.eopV24eO.XbEVdJGve_JYaIt8ETqTIpwwZrs6yqkI-1702335846-1-AYPf25EZi2lIr4nYFdhq7EE6qfJv4TlvrUQa+MD9r/A7M5O/b0lpXFXjgWi+rY/sBwNTkB6OdaD/YUZXhLM/pao=; path=/; expires=Mon, 11-Dec-23 23:34:06 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=fwTwJnci0Osg5Sjjcu7yn37YQCX6voyabDVvGEbiUWw-1702335846514-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415d5e5c3b8ca5-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:04:06,381 [INFO]: HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2023-12-11 18:04:06,381 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:04:06,381 [DEBUG]: receive_response_body.complete
2023-12-11 18:04:06,381 [DEBUG]: response_closed.started
2023-12-11 18:04:06,382 [DEBUG]: response_closed.complete
2023-12-11 18:04:06,382 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2023-12-11 18:04:06,384 [DEBUG]: Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'text-davinci-003', 'prompt': "Please rate the context relevance of the answers on a scale from 0 to 1, where 0 is not relevant and 1 is highly relevant.\nExample 1:\nContext: History of the Roman Empire.\nAnswer: Julius Caesar was a Roman ruler.\nRating: 1\nExample 2:\nContext: History of the Roman Empire.\nAnswer: The Roman Empire was in Africa.\nRating: 0.5\nExample 3:\nContext: Computer Programming Basics.\nAnswer: Apples grow on trees.\nRating: 0\nYour Turn:\nContext: 2-ThisisasupercoolideaandIlikehowitclearlyalignswithyourinterests!Ijustwonder1)ifyoucanmodifytheoutputtomakeitalittlebitmorereadable,maybeplaywithASCIIcharactersandmakeitmoregame-like?and2)thereasoningbehinddesignchoicesyoumake,footballisobviouslysupercomplexandIâ€™massumingyouâ€™resimplifyingitforyourgame,whatandwhyareyouchoosingtoleaveout?Itseemslikeyouâ€™reofftoagoodstartthough!\n4-Again,lovetheideaandcandeï¬nitelyseethepassionbehindit!100+coldemailsiscrazyandIreallyrespecttheamountofworkthatâ€™salreadygoneintothisproject.Ijustwouldlovetoknowmoredetailsabouttheregressionmodeling/transferlearningpartofthisproject\n6-Thisissuperinteresting,Iâ€™veneverreallythoughtabouthowcomplicatedadishwashinggamecouldbe,butIhonestlylovehowout-of-the-boxthisisandhowyouâ€™veputatonofthoughtintoit.Mymainsuggestionwouldbetojustaddmoretexttotheslidesâ€”Iknowthattheuseofvisualsisintentionalbutfororganizationalpurposes,itcouldbegoodtostartwithatextoverviewandsupplementvisualswithcaptions.\n8-CouldbebecauseImissedtheproposal(andmaybetherewasalongeroverviewthere),butIwouldâ€™veappreciatedalongerhigh-leveloverview/statementofpurpose.However,Ireallyrespecthowtechnicalthisprojectisandhowitâ€™sdeï¬nitelystronglyrootedintheory\n10-Ilikethebackgroundandoverview,eventhoughIâ€™mnotfamiliarwiththetopic,theslidesreallyhelpillustratethepurposeoftheproject.Myonlysuggestionwouldbetocutdownsomeofthetextontheslides.\nCPSC 488/588 - AI Foundation Models HW 3 Page 3\nproposed system level method to improve the efficiency of inference in Transformers. We did not cover\nthis in class. Please read the paper and answer the following questions.\n(a) (5 points) Summarize how the proposed approach works on high-level (no more than 5 sentences).\nDo you think such approach can be combined with FlashAttention?\nSolution:\n5-sentence summary: Prompt Cache reduces the computational overhead of, and thus acceler-\nates, LLM inference by reusing attention states from different input prompts and supporting\nmodularity to maximize reuse. It takes advantage of the fact that prompts often have signifi-\ncant text overlap, sharing frequently occurring phrases/segments of text (e.g. system messages,\ndomain-specific documents, templates). Prompt Cache pre-computes the attention states of\nthese frequent text segments as prompt modules and stores them in a modular key-value cache,\nso that when an LLM encounters a frequent text segment, it does not need to re-compute full\nattention for its tokens. It requires users to write prompts in Prompt Markup Language (PML),\nin which they use a schema to define prompt modules, which can be parameterized to maximize\nreuse and allow for customization. Discontinuous position IDs are key to caching prompts, so\nmodifications must be made to some transformer architectures to support these position IDs.\nwindow size from bottom to top, which the LongFormer paper shows achieves good experimental\nresults. Iâ€™d start with a small window size of 32 on the lowest layer, which focuses on local\ncontext and is computatio\nAnswer: Based on the given context, I can provide some common limitations found in research papers in general. However, without specific information about the research papers mentioned in the context, I cannot provide specific limitations for those papers. Here are some common limitations found in research papers:\n\n1. Small Sample Size: Research papers often have a limited sample size, which can limit the generalizability of the findings.\n\n2. Limited Scope: Research papers may focus on a specific aspect or phenomenon, which may not fully capture the complexity of the topic.\n\n3. Methodological Limitations: Research papers may have limitations in their research design or methodology, such as biased sampling, measurement errors, or confounding variables. These limitations can affect the validity and reliability of the results.\n\n4. Generalizability: The findings of research papers may not be applicable to other populations, contexts, or timeframes. This limits the generalizability of the findings.\n\n5. Publication Bias: Research papers that report statistically significant or positive results are more likely to be published, while studies with null or negative findings may go unpublished. This can result in an overrepresentation of certain findings and limit the overall understanding of a topic.\n\nIt's important to note that these limitations can vary depending on the specific research papers being discussed.\nRating:", 'max_tokens': 60}}
2023-12-11 18:04:06,384 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:04:06,385 [DEBUG]: send_request_headers.complete
2023-12-11 18:04:06,385 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:04:06,385 [DEBUG]: send_request_body.complete
2023-12-11 18:04:06,385 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:04:06,784 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:04:06 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'text-davinci-003'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'275'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-limit-tokens_usage_based', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'249940'), (b'x-ratelimit-remaining-tokens_usage_based', b'249174'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'14ms'), (b'x-ratelimit-reset-tokens_usage_based', b'198ms'), (b'x-request-id', b'934d4d7e56971abe37273d819bf9a53b'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415d60de3d8ca5-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:04:06,785 [INFO]: HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2023-12-11 18:04:06,785 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:04:06,787 [DEBUG]: receive_response_body.complete
2023-12-11 18:04:06,788 [DEBUG]: response_closed.started
2023-12-11 18:04:06,788 [DEBUG]: response_closed.complete
2023-12-11 18:04:06,789 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2023-12-11 18:04:06,791 [DEBUG]: Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'text-davinci-003', 'prompt': "Please rate the relevance of the answers to the questions on a scale from 0 to 1, where 0 is not relevant and 1 is highly relevant.\nExample 1:\nQuestion: What is the capital of France?\nAnswer: Paris is the capital of France.\nRating: 1\nExample 2:\nQuestion: What is the capital of France?\nAnswer: France is in Europe.\nRating: 0.5\nExample 3:\nQuestion: What is the capital of France?\nAnswer: Elephants are the largest land animals.\nRating: 0\nYour Turn:\nQuestion: What are some common limitations found in these research papers?\nAnswer: Based on the given context, I can provide some common limitations found in research papers in general. However, without specific information about the research papers mentioned in the context, I cannot provide specific limitations for those papers. Here are some common limitations found in research papers:\n\n1. Small Sample Size: Research papers often have a limited sample size, which can limit the generalizability of the findings.\n\n2. Limited Scope: Research papers may focus on a specific aspect or phenomenon, which may not fully capture the complexity of the topic.\n\n3. Methodological Limitations: Research papers may have limitations in their research design or methodology, such as biased sampling, measurement errors, or confounding variables. These limitations can affect the validity and reliability of the results.\n\n4. Generalizability: The findings of research papers may not be applicable to other populations, contexts, or timeframes. This limits the generalizability of the findings.\n\n5. Publication Bias: Research papers that report statistically significant or positive results are more likely to be published, while studies with null or negative findings may go unpublished. This can result in an overrepresentation of certain findings and limit the overall understanding of a topic.\n\nIt's important to note that these limitations can vary depending on the specific research papers being discussed.\nRating:", 'max_tokens': 60}}
2023-12-11 18:04:06,793 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-11 18:04:06,793 [DEBUG]: send_request_headers.complete
2023-12-11 18:04:06,793 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-11 18:04:06,793 [DEBUG]: send_request_body.complete
2023-12-11 18:04:06,794 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-11 18:04:07,164 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 11 Dec 2023 23:04:07 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'text-davinci-003'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'226'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-limit-tokens_usage_based', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'249940'), (b'x-ratelimit-remaining-tokens_usage_based', b'249285'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'14ms'), (b'x-ratelimit-reset-tokens_usage_based', b'171ms'), (b'x-request-id', b'f37db971ef75ac77a9e2018970b4805f'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'83415d6368388ca5-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-11 18:04:07,165 [INFO]: HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2023-12-11 18:04:07,165 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-11 18:04:07,165 [DEBUG]: receive_response_body.complete
2023-12-11 18:04:07,165 [DEBUG]: response_closed.started
2023-12-11 18:04:07,166 [DEBUG]: response_closed.complete
2023-12-11 18:04:07,166 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2023-12-11 18:04:07,166 [INFO]: 127.0.0.1 - - [11/Dec/2023 18:04:07] "POST /api/evaluate-answers HTTP/1.1" 200 -
2023-12-12 15:39:21,803 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-12 15:39:21,804 [DEBUG]: load_verify_locations cafile='/Users/aliceao/opt/miniconda3/envs/chat-lore/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-12 15:39:21,884 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-12 15:39:21,885 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-12 15:39:21,886 [INFO]:  * Restarting with stat
2023-12-12 15:39:23,202 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-12 15:39:23,202 [DEBUG]: load_verify_locations cafile='/Users/aliceao/opt/miniconda3/envs/chat-lore/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-12 15:39:23,280 [WARNING]:  * Debugger is active!
2023-12-12 15:39:23,291 [INFO]:  * Debugger PIN: 138-610-961
2023-12-12 15:39:23,303 [INFO]: 127.0.0.1 - - [12/Dec/2023 15:39:23] "[33mGET / HTTP/1.1[0m" 404 -
2023-12-12 15:39:23,545 [INFO]: 127.0.0.1 - - [12/Dec/2023 15:39:23] "[33mGET /favicon.ico HTTP/1.1[0m" 404 -
2023-12-12 15:43:37,292 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-12 15:43:37,293 [DEBUG]: load_verify_locations cafile='/Users/aliceao/opt/miniconda3/envs/chat-lore/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-12 15:43:37,382 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-12 15:43:37,383 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-12 15:43:37,383 [INFO]:  * Restarting with stat
2023-12-12 15:43:38,513 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-12 15:43:38,514 [DEBUG]: load_verify_locations cafile='/Users/aliceao/opt/miniconda3/envs/chat-lore/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-12 15:43:38,582 [WARNING]:  * Debugger is active!
2023-12-12 15:43:38,590 [INFO]:  * Debugger PIN: 138-610-961
2023-12-12 15:47:39,311 [DEBUG]: come to upload fiels 
2023-12-12 15:47:39,342 [DEBUG]: <FileStorage: 'CPSC_490_Final_Report (1).pdf' ('application/pdf')>
2023-12-12 15:47:39,342 [DEBUG]: files
2023-12-12 15:47:39,351 [INFO]: 127.0.0.1 - - [12/Dec/2023 15:47:39] "POST /api/uploadFiles HTTP/1.1" 200 -
2023-12-12 15:47:39,363 [DEBUG]: Using selector: KqueueSelector
2023-12-12 15:47:39,820 [DEBUG]: Initialized Pinecone
2023-12-12 15:47:39,820 [DEBUG]: Type of pdf_file: <class 'str'>
2023-12-12 15:47:40,072 [DEBUG]: Parsed PDF
2023-12-12 15:47:40,253 [DEBUG]: List of indexes: ['research-chat-index']
2023-12-12 15:47:40,254 [DEBUG]: Initialized Pinecone Index
2023-12-12 15:47:40,256 [DEBUG]: Chunked PDF and obtained vectors
2023-12-12 15:47:40,258 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-12 15:47:40,258 [DEBUG]: load_verify_locations cafile='/Users/aliceao/opt/miniconda3/envs/chat-lore/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-12 15:47:40,301 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x111457240>, 'json_data': {'input': 'Transformer-Based Machine Learning Techniques to Optimize Live 360 Â°Viewport Predictions Alice Ao alice.ao@yale.eduAdvisor: Prof. Sohee Kim Park sohee.park@yale.edu Abstract Recent investment in "the metaverse," an immersive world powered by virtual reality (VR), is indicative of a significant demand for high-quality 360 Â°livestreaming. However, there are many technical challenges involved with delivering such streams. First, the vastness of a full 360 Â°view typically de- mands substantially high bandwidth, and much of that view will be outside of the actual viewport and go unnoticed by the user. Second, the live nature of the videos also requires especially low latency and fast computation times. This project explores how novel machine-learning (ML) techniques can address these challenges related to 360 Â° livestreaming. It specifically focuses on whether transformer- based architectures, which have been key to the recent success of large language models, can also be effective for computer vision applications. This project ultimately 1) compares mul- tiple techniques that generate saliency maps 2) utilizes these saliency maps for viewport prediction and 3) evaluates the quality of the predicted viewports in adaptive bitrate stream- ing. 1 Background 1.1 Introduction In recent years, there has been a strong rise in demand for VR and immersive technologies. Perhaps most notably, in October 2021, Facebook CEO and founder Mark Zuckerberg', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:47:40,335 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-12 15:47:40,367 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1113cbe10>
2023-12-12 15:47:40,368 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x111350b00> server_hostname='api.openai.com' timeout=5.0
2023-12-12 15:47:40,381 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1113d3910>
2023-12-12 15:47:40,381 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:40,381 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:40,382 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:40,382 [DEBUG]: send_request_body.complete
2023-12-12 15:47:40,382 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:40,807 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:40 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'32'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999635'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'60329ef5e05d7ee4bd424647d367c02c'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=jGrIM7IbpHXbtbUo2eHdNhyeyd6CmnsVwNTM7MGp2gU-1702414060-1-AWKdVuzbYK89sLo8wMGRx5Bf5STTLTkEfm0qFk4gzL2GY+FulGYINOvEZzFrmH/JSLlfYGz+PhpDLq4VJJZTBys=; path=/; expires=Tue, 12-Dec-23 21:17:40 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=DI.foc8f5Lwg6aS1VYieSz1sb2eX3yc0VtgCfNsqWYc-1702414060853-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d2e5dba843f3-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:40,808 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:47:40,809 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:40,810 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:40,810 [DEBUG]: response_closed.started
2023-12-12 15:47:40,810 [DEBUG]: response_closed.complete
2023-12-12 15:47:40,810 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:47:40,813 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x105126fc0>, 'json_data': {'input': 'announced that the social media giant would be rebranded to Meta, signaling his desire to invest in "the metaverse," and ushering in a new wave of interest in new immersive technologies. While it remains unclear whether "the metaverse" will achieve the widespread adoption and popularity that Zucker- berg hopes for, there is undeniably strong interest in 360 Â° VR for applications like entertainment, education, and gam- ing. Major news broadcasters like CNN, NBC, BBC, and Al Jazeera now have 360 Â°streams, theme parks have incorpo- rated 360 Â°VR into their attractions, and medical training forphysicians can occur in 360 Â°formats [15]. Some estimates claim that by 2024, the market value of VR could even rise to $44.7 billion [9]. Todayâ€™s 360 Â°VR experiences follow a similar format, in which a user typically wears a head-mounted display (HMD) device to view and interact with the 360 Â°scene. The user can only see a small section of the entire scene at once, known as theviewport . By shifting and turning their head, and therefore changing the position of the viewport, the user can see differ- ent parts of the scene. While the format of the experiences are relatively similar, different end-to-end systems have different strategies on how to optimally deliver 360 Â°content, which this project aims to explore. 1.2 Problem Description There have been many advances in adaptive bitrate allocation and improving video resolution for regular 2D videos â€“ but', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:47:40,814 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:40,815 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:40,815 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:40,815 [DEBUG]: send_request_body.complete
2023-12-12 15:47:40,815 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:40,980 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:41 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'42'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999631'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'b855018d4c930726901cab588aae1da4'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d2e89f6543f3-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:40,981 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:47:40,981 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:40,982 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:40,983 [DEBUG]: response_closed.started
2023-12-12 15:47:40,983 [DEBUG]: response_closed.complete
2023-12-12 15:47:40,983 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:47:40,984 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x111418720>, 'json_data': {'input': 'the added complexities and distortions of the 360 Â°format prevent the typical reinforcement learning strategies for 2D videos from being effective for 360 Â°content. 360 Â°videos pose a complex problem that involves constant tradeoffs be- tween "streaming quality, spatial quality variance, viewport prediction errors, and bandwidth efficiency" [15]. First, streaming 360 Â°content often requires high bandwidth and low latency, and broadcasting the entire 360 Â°scene in high quality often consumes much of the networkâ€™s resources [15]. The viewport takes up a mere 20% of the full scene [11], so about 80% of the high-quality stream may be wasted on content that the user cannot see. Therefore, one technique to efficiently stream 360 Â°video and save bandwidth is to use adaptive bitrate schemes, in which the viewport area, as well as areas that the user will likely look at next, are streamed in high-quality, and the rest are delivered in lower quality [15]. Various ML approaches can be used to predict which areas the user will look at', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:47:40,985 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:40,985 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:40,985 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:40,986 [DEBUG]: send_request_body.complete
2023-12-12 15:47:40,986 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:41,142 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:41 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'24'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999738'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'3ms'), (b'x-request-id', b'758ae14dad246f5acd833d683644e9f3'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d2e998db43f3-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:41,142 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:47:41,143 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:41,143 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:41,144 [DEBUG]: response_closed.started
2023-12-12 15:47:41,144 [DEBUG]: response_closed.complete
2023-12-12 15:47:41,144 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:47:41,145 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x111418720>, 'json_data': {'input': 'likely look at next, are streamed in high-quality, and the rest are delivered in lower quality [15]. Various ML approaches can be used to predict which areas the user will look at next, but these approaches can oftentimes be computation- ally expensive and require lots of overhead, especially for livestreams, which require content to be delivered quickly 1', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:47:41,146 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:41,147 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:41,147 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:41,147 [DEBUG]: send_request_body.complete
2023-12-12 15:47:41,147 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:41,321 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:41 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'46'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999911'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'1ms'), (b'x-request-id', b'eaf311505013b1a94fdbc1975d42c860'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d2eaaa4b43f3-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:41,322 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:47:41,322 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:41,349 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:41,349 [DEBUG]: response_closed.started
2023-12-12 15:47:41,350 [DEBUG]: response_closed.complete
2023-12-12 15:47:41,350 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:47:41,351 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x111418720>, 'json_data': {'input': 'and continuously [16]. However, as the field of ML rapidly advances, there is growing potential for new techniques and improvements to solve the problem of viewport prediction. 1.3 Related Work 1.3.1 Past Approaches Approaches to adaptive bitrate allocation can be content- aware and/or content-agnostic . A purely content-aware ap- proach identifies "salient" regions, where the user is likely to look [13], whereas a content-agnostic approach considers a userâ€™s past head or eye-tracking movement. Previous litera- ture supports that the latter category typically exhibits higher performance and less computational complexity [15]. Nguyen et al. created a novel framework that effectively combines both approaches, using a deep convolutional neural network to detect salient regions of 360 Â°videos and a Long Short-Term Memory Network (LSTM) that predicts head movements with both saliency maps and head orientations [9]. Similarly, Park et al. developed Mosaic, an end-to-end implementation for adaptive bitrate allocation, and found a 3D Convolutional Neural Network to have relatively high prediction accuracy and low prediction latency [11]. However, approaches that are purely content-agnostic can also deliver promising results. For instance, the discrete vari- ational multiple sequence (DVMS) learning framework uses deep latent variable models and deep neural networks to pre- dict multiple head trajectories. By considering multiple po-', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:47:41,352 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:41,352 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:41,352 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:41,353 [DEBUG]: send_request_body.complete
2023-12-12 15:47:41,353 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:41,760 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:41 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'38'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999638'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'11593c8f57af74245bcb159670a07d6f'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d2ebebfb43f3-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:41,761 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:47:41,761 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:41,763 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:41,764 [DEBUG]: response_closed.started
2023-12-12 15:47:41,764 [DEBUG]: response_closed.complete
2023-12-12 15:47:41,764 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:47:41,765 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x111418720>, 'json_data': {'input': 'tential trajectories, DVMS effectively accounts for how di- verse head trajectories can be and therefore greatly improves streaming quality [5]. These studies, however, are focused on on-demand videos, which typically have historical head/eye-tracking data avail- able of their previous viewers. They do not focus on live videostreams, which are constrained by the lack of historical user data and their requirements for especially low latency and high processing speed [4]. Therefore, in live viewport pre- diction often must rely on content-aware techniques as well. This project builds on existing work on regular 360 Â°videos by focusing on the unique needs of live content. It examines models that only rely on content-aware approaches and/or live head-tracking data for the individual user, and therefore can be used in live contexts. 1.3.2 Transformers This project also examines the possibility of using a novel ma- chine learning architecture, the transformer. During the past few years, computer vision models often used convolutional neural networks (CNNs) as their underlying architecture [11]. CNNs usually break an input up into small patches, allow- ing for more efficient and practical deep learning, and then gradually build up a global understanding of an input.However, computer vision models could improve by im- mediately leveraging the global context of images or videos. This global context is key to transformers and other self-', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:47:41,766 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:41,766 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:41,767 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:41,767 [DEBUG]: send_request_body.complete
2023-12-12 15:47:41,767 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:41,929 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:41 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'25'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999637'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'75e09684ceb1e6dc82d5c5914300c81b'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d2ee8f8443f3-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:41,929 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:47:41,930 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:41,938 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:41,938 [DEBUG]: response_closed.started
2023-12-12 15:47:41,938 [DEBUG]: response_closed.complete
2023-12-12 15:47:41,938 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:47:41,939 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x111418720>, 'json_data': {'input': 'attention models, which make connections between different parts of their input using a mechanism called multi-head self- attention . Transformers have recently gained traction in natu- ral language processing (NLP) and other ML applications, as they were first designed and published in 2017 [12] and now serve as the architecture of nearly all large language models. One of their disadvantages, however, is that they require large amounts of compute and training data, the latter of which is more readily available for language research than vision research. Past research papers have recognized transformers as promising machine learning architectures for visual saliency detection [9] [5], and other viewport prediction models using transformer architectures have achieved promising results [1], so this project explores the effectiveness of transformers and transformer-inspired architectures for the application of live 360-degree video. Therefore, this project focuses on three saliency map models, with varying degrees of similarity to', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:47:41,941 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:41,941 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:41,941 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:41,941 [DEBUG]: send_request_body.complete
2023-12-12 15:47:41,941 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:42,154 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:42 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'41'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999739'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'3ms'), (b'x-request-id', b'12c2edf839280e043db0993480642665'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d2efa8f943f3-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:42,155 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:47:42,155 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:42,155 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:42,156 [DEBUG]: response_closed.started
2023-12-12 15:47:42,156 [DEBUG]: response_closed.complete
2023-12-12 15:47:42,156 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:47:42,157 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x105126fc0>, 'json_data': {'input': 'transformer-inspired architectures for the application of live 360-degree video. Therefore, this project focuses on three saliency map models, with varying degrees of similarity to a transformer-based architecture, and evaluates their mapsâ€™ performance in viewport prediction and adaptive bitrate allo- cation. 2 Methodology There are three main parts to this project: 1) identifying dif- ferent ML saliency detection models, with different levels of similarity to transformers, and 2) constructing a bitrate allocation scheme and 3) evaluating their results with metrics defined in [3]. The workflow of the first two parts is illustrated in Figure 1. 3 Preliminary Work 3.1 Selecting Machine Learning Techniques Similar to previous works [11], this paper identifies and com- pares state-of-the-art machine learning techniques on view- port prediction. All of the selected techniques focus on build- ingsaliency maps , which identify "salient" areas of each frame that users are likely to focus on. The final three selected machine-learning techniques were as follows: 1.PanoSalNet (published in MMSys â€™18): a deep CNN- based architecture specifically for generating saliency maps of 360-degree videos, with optional integration of head-tracking history. The architecture for the saliency map model is based on Deep Convnet, a state-of-the- art DCNN for 2D images, and transfer learning is used to adapt a pre-trained DCNN model to 360-degree im-', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:47:42,158 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:42,159 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:42,159 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:42,159 [DEBUG]: send_request_body.complete
2023-12-12 15:47:42,159 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:42,298 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:42 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'27'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999638'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'a67ee9cbf48b894a1ccd99a3972dc6fb'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d2f0faad43f3-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:42,299 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:47:42,299 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:42,307 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:42,307 [DEBUG]: response_closed.started
2023-12-12 15:47:42,308 [DEBUG]: response_closed.complete
2023-12-12 15:47:42,308 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:47:42,309 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x105126fc0>, 'json_data': {'input': 'ages. The model is trained using stochastic gradient de- 2', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:47:42,310 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:42,310 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:42,310 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:42,311 [DEBUG]: send_request_body.complete
2023-12-12 15:47:42,311 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:42,469 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:42 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'23'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999986'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'a472d4dfd57a5c3bf9b1d927d3e5f459'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d2f1ec0143f3-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:42,470 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:47:42,470 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:42,470 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:42,471 [DEBUG]: response_closed.started
2023-12-12 15:47:42,471 [DEBUG]: response_closed.complete
2023-12-12 15:47:42,471 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:47:42,472 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x105126fc0>, 'json_data': {'input': 'Figure 1: Adaptive bitrate allocation process: 1) Videos are split into frames (fps=1). 2) ML model generates saliency maps, with white patches representing "salient" areas. 3) Saliency is averaged together for each of the 9x16 tiles and can be represented as a numerical value. 4) Tiles are assigned a bitrate based on their saliency value, with higher opacity in this figure representing higher bitrate. Figure 2: PanoSalNet final architecture [10]scent with a fixed learning rate. This model produces saliency maps, which can be combined with head ori- entation maps and then passed into a Long Short-Term Memory (LSTM) model (Figure 2) that predicts the next head orientation map, or viewport. Experimentally, the model achieved significant improvements in viewport quality [10]. 2.TranSalNet (published in Neurocomputing â€™22): a novel saliency model that adds transformer components to a traditional CNN backbone in order to capture long- range context and more closely emulate the human vi- sual system. The architecture includes some transformer encoders, complete with position embeddings and multi- head self-attention, as well as regular CNN encoders and decoders. This model has been trained and evaluated on 2D images, achieving state-of-the-art performance [7]. 3.Visual Saliency Transformer (published in CVPR â€™21): a pure transformer (i.e. convolution-free) saliency model for both RGB and RGB-D inputs. Novel components include multi-token fusion, a newly designed token up-', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:47:42,473 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:42,473 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:42,473 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:42,473 [DEBUG]: send_request_body.complete
2023-12-12 15:47:42,473 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:42,815 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:42 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'28'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999626'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'5a0f034318854a70a3a244472451a2f5'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d2f2ed6243f3-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:42,815 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:47:42,816 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:42,816 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:42,816 [DEBUG]: response_closed.started
2023-12-12 15:47:42,816 [DEBUG]: response_closed.complete
2023-12-12 15:47:42,817 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:47:42,818 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x105126fc0>, 'json_data': {'input': 'sampling method, and a special decoder for saliency and boundary detection. VST outperforms state-of-the-art CNN salient object detection models [6]. The criteria for selecting each technique were as follows: 1.Recency : Because of how rapidly the field of machine learning is evolving, this paper focuses on comparing state-of-the-art ML models published at most five years ago, and ideally even later. (a)PanoSalNet : As the oldest model, PanoSalNet [10], was first published in 2018, but it has been cited over 80 times, and the author has used the model in an April 2023 paper [9]. Therefore, although it pre- dates state-of-the-art techniques like transformers, it continues to have relevancy today. (b)TranSalNet : Published in just 2022, TranSalNet is fairly recent and also leverages novel mechanisms like multi-head self-attention. (c)Visual Saliency Transformer : Similarly, VST is also recent, with a publication year of 2021. 2.Available source code and datasets : Many of the recent literature on machine learning approaches to saliency map creation and viewport prediction do not have publicly available source code [1]. Other papers may have public repositories but with missing, private-access datasets or incomplete code that prevents the models from running. This project considered and tested five other alternative codebases but ultimately could not use them due to their machine incompatibility or lack of maintenance. 3', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:47:42,818 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:42,819 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:42,819 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:42,819 [DEBUG]: send_request_body.complete
2023-12-12 15:47:42,819 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:42,981 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:43 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'34'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999639'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'7beef5817799d10be70f54ec8c3f7ca3'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d2f518d543f3-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:42,982 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:47:42,982 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:42,983 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:42,983 [DEBUG]: response_closed.started
2023-12-12 15:47:42,983 [DEBUG]: response_closed.complete
2023-12-12 15:47:42,983 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:47:42,984 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x105126fc0>, 'json_data': {'input': '3.Relevance in computer vision and/or VR : Notably, some of the selected models [7] [6] have used images , not videos, as training and evaluation data. These models have largely been influential in the field of computer vi- sion and image processing, but they can be extrapolated to videos, which are simply a collection of image frames. However, because they have not been trained on 360- degree images, which must be projected and flattened onto a 2D plane, they may not be able to handle image distortions as well, so additional training and fine-tuning of the model may need to occur in the future. 3.2 Set-Up A non-trivial portion of the project was accessing published, open-source models and setting up their environments. Some of the models did not have environment files, so all of their dependencies had to be downloaded manually. Older mod- els [10] also had deprecated packages and dependencies, such as Caffe, that also had to be manually built and created, which was a non-trivial amount of work. Determining which pub- licly available models could produce valid results was also no trivial task. Some had broken pre-trained model files or missing files, so it was a trial-and-error process of identifying which repositories would be usable for the project. Additionally, many of the usable codebases had limited documentation, so important information, such as the in- put/output formats, function arguments, original datasets,', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:47:42,985 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:42,986 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:42,986 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:42,986 [DEBUG]: send_request_body.complete
2023-12-12 15:47:42,986 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:43,204 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:43 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'30'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999639'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'fe5c754fb542fbf0a1230c8a9de58713'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d2f62a4943f3-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:43,205 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:47:43,205 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:43,205 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:43,206 [DEBUG]: response_closed.started
2023-12-12 15:47:43,206 [DEBUG]: response_closed.complete
2023-12-12 15:47:43,206 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:47:43,207 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x111418720>, 'json_data': {'input': 'edge cases/unexpected behavior, etc. had to be inferred from reading the papers and the code. Finally, some of the datasets used were not publicly avail- able â€“ access had to be requested for them, either by com- pleting a form [2] or writing an email to the study authors. Many of the modelsâ€™ results could not be easily reproduced, as their training and evaluation datasets were all private and/or deleted [3]. Therefore, the first part of the project was not to reproduce the three selected modelsâ€™ results but to gener- ate new saliency maps on a different dataset and informally compare their results. 4 Implementation 4.1 Saliency Maps 4.1.1 Generation After all three model environments were set up, the pre-trained models were run on the Wild360 dataset (described in further detail below). ffmpeg was used to extract frames from each of the 360-degree videos with a frame rate of 4fps. For each video frame, the models treated them as unique images and generated a binary saliency map for them. (For the actual adaptive bitrate allocation, a different dataset [14] was used, Figure 3: A colored heatmap from the Wild360 dataset (top) and black-and-white saliency maps (bottom) and saliency maps were generated for all of the videos in that dataset.) 4.1.2 Informal Evaluation Before implementing the adaptive bitrate allocation portion of the project, the three modelsâ€™ saliency detection abili- ties were compared using the Wild-360 dataset. The dataset', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:47:43,208 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:43,208 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:43,208 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:43,209 [DEBUG]: send_request_body.complete
2023-12-12 15:47:43,209 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:43,363 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:43 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'37'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999632'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'4529bc003f5b8cd07f855927ce2a7768'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d2f78c0643f3-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:43,364 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:47:43,364 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:43,365 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:43,365 [DEBUG]: response_closed.started
2023-12-12 15:47:43,365 [DEBUG]: response_closed.complete
2023-12-12 15:47:43,365 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:47:43,366 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x111418720>, 'json_data': {'input': 'contains 360-degree videos and their corresponding ground- truth saliency annotations, allowing for the evaluation of 360- degree saliency detection models. These annotations, which are in heatmap form, were generated from the scanpaths of several different human users [2]. Given time and compute constraints, the pre-trained ma- chine models were used without any additional fine-tuning and evaluated on 25 testing videos in the Wild-360 dataset. However, there are important caveats to the below results. First, because the ground-truth annotations are in colored heatmap format, and the generated saliency maps for all three models are in binary format, the evaluation metrics are imper- fect and should only be used for a very informal comparison of performance. Using a dataset with black-and-white ground truth saliency annotations would be ideal, but 360-degree video training datasets themselves have been difficult to come by. Minor ad- justments could theoretically be made to the Wild360 dataset so that itâ€™d be more compatible with the generated saliency', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:47:43,367 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:43,367 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:43,367 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:43,368 [DEBUG]: send_request_body.complete
2023-12-12 15:47:43,368 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:43,500 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:43 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'26'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999732'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'3ms'), (b'x-request-id', b'a694f977735042046b335364429ac8ed'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d2f88d2743f3-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:43,501 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:47:43,501 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:43,502 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:43,502 [DEBUG]: response_closed.started
2023-12-12 15:47:43,502 [DEBUG]: response_closed.complete
2023-12-12 15:47:43,502 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:47:43,503 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x111418720>, 'json_data': {'input': 'datasets themselves have been difficult to come by. Minor ad- justments could theoretically be made to the Wild360 dataset so that itâ€™d be more compatible with the generated saliency maps. One naÃ¯ve approach, simply converting the colored heatmaps to grayscale, did not yield different results. How- ever, another option could be defining a threshold for the heatmaps, and making all values above the threshold salient (i.e. white) or non-salient (i.e. black). This would more closely resemble the black-and-white format of the saliency map out- put. However, itâ€™s unclear what threshold to use, and whether the choice of threshold could greatly influence the results. If this were a formal evaluation, more rigorous and thor- ough comparisons would be conducted. However, since this was only intended to be a simple baseline testing before imple- 4', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:47:43,504 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:43,504 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:43,504 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:43,505 [DEBUG]: send_request_body.complete
2023-12-12 15:47:43,505 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:43,861 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:43 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'25'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999786'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'2ms'), (b'x-request-id', b'48a0efc3a4dc1df7b2b12c68a3245d60'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d2f96e4943f3-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:43,862 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:47:43,862 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:43,863 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:43,863 [DEBUG]: response_closed.started
2023-12-12 15:47:43,863 [DEBUG]: response_closed.complete
2023-12-12 15:47:43,863 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:47:43,864 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x111418720>, 'json_data': {'input': 'menting adaptive bitrate allocation, these preliminary results can simply show that the models tend to identify similar areas as "salient." Additionally, all of these models have different ideas of what qualifies as salient â€“ VST, for instance, is fo- cused on object detection and produces binary outputs of what is included in a salient object and what is not, while the other models produce more nuanced, gradual models of saliency. Despite these different goals, the ultimate measure of perfor- mance is the set of results with adaptive bitrate allocation (4.3.2). Below are the informal evaluation results: PanoSalNet TranSalNet VST mae 0.1408 0.2387 0.3103 max-fm 0.3659 0.3655 0.3655 mean-fm 0.1995 0.1688 0.1284 max e-measure 0.4300 0.4203 0.4275 mean e-measure 0.3436 0.3688 0.4094 s measure 0.4305 0.4776 0.4649 AP 0.3213 0.3318 0.3165 AUC 0.5112 0.5161 0.5127 Table 1: Saliency detection results These evaluation metrics are the same as those used in [6], and better results among the three are depicted in bold. 4.2 Adaptive Bitrate Allocation Adaptive bitrate schemes typically use a tile-based approach [11] [15], in which video frames are split into rectangular tiles, for computational efficiency. Computing and assigning a unique bitrate to each pixel in the frame would be impractical, so tiling offers a feasible, yet still effective alternative. With tile-based approaches, tiles likely to be in or near the viewport area will be streamed in high quality, and others will be lower', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:47:43,865 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:43,866 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:43,866 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:43,866 [DEBUG]: send_request_body.complete
2023-12-12 15:47:43,866 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:44,013 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:44 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'30'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999625'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'2c380be12aac2d349c81eaf586932e29'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d2fba9c743f3-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:44,014 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:47:44,014 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:44,015 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:44,015 [DEBUG]: response_closed.started
2023-12-12 15:47:44,015 [DEBUG]: response_closed.complete
2023-12-12 15:47:44,015 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:47:44,016 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x111418720>, 'json_data': {'input': 'quality. In [3], various content-agnostic models, such as PanoS- alNet [10] with head-movement maps added, are used in adaptive bitrate schemes and evaluated in terms of quality- of-experience and viewport prediction accuracy. This project utilizes the source code and same basic bitrate allocation of [3], as well as one of the datasets they used [14]. Unlike Wild360, the dataset in [14] contains additional head move- ment data taken from dozens of user testers that can serve as ground truth annotations, or representations of actual user viewports. The original tiling scheme that [3] used with PanoSalNet had9Ã—16tiles, so all three models also used 9Ã—16tiles and the same bitrate allocation algorithm as in [3]. Figure 4: Bitrate allocation and Manhattan Tile Error algo- rithm Figure 5: Example tiling schemes [15] 5', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:47:44,017 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:44,018 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:44,018 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:44,018 [DEBUG]: send_request_body.complete
2023-12-12 15:47:44,018 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:44,248 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:44 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'24'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999794'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'2ms'), (b'x-request-id', b'f09e53b182c2713c8c87a8a5f44f03c8'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d2fc9b0a43f3-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:44,248 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:47:44,249 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:44,249 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:44,249 [DEBUG]: response_closed.started
2023-12-12 15:47:44,249 [DEBUG]: response_closed.complete
2023-12-12 15:47:44,250 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:47:44,251 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x105126fc0>, 'json_data': {'input': 'Figure 6: QoE aggregate score from [3] 4.3 Evaluation 4.3.1 Defining Quality-of-Experience 360-degree streaming projects typically define their own quality-of-experience (QoE) metric, which ideally allows them to quantify and compare the effectiveness of various viewport prediction methods. There is no universally agreed- upon standard for QoE, leaving room for subjectivity and author discretion. Therefore, a significant part of this project was also selecting and defining an appropriate QoE metric. The initial approach was to use the same QoE metric as defined in [3], as it considered four different aspects of a 360-degree viewing experience: 1) the average bitrate in a userâ€™s viewport for each frame 2) the variation of the bitrate in the viewport for each frame 3) the variation of bitrate across frames and 4) the variation of bitrate across successive chunks, or tiles. These four metrics are combined into an aggregate QoE score, as shown in Figure 6. However, examining the published code repository and running some small experiments demonstrated that it was difficult to compare models using this aggregated QoE score. Sometimes, QoE could be less than 0, and the results became less meaningful and interpretable. Therefore, this project ex- amines the four QOE metrics separately to extract insights of video quality. Since the frame rate was 1, QoE 3 is 0for every method, QoE 3 is omitted from the table of results. Furthermore, since the QoE score was the sum of all cal-', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:47:44,252 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:44,252 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:44,252 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:44,252 [DEBUG]: send_request_body.complete
2023-12-12 15:47:44,252 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:44,412 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:44 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'35'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999626'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'99b1abe85b58db29be914657f5c6a4e0'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d2fe0d1543f3-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:44,412 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:47:44,413 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:44,444 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:44,445 [DEBUG]: response_closed.started
2023-12-12 15:47:44,445 [DEBUG]: response_closed.complete
2023-12-12 15:47:44,445 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:47:44,446 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x105126fc0>, 'json_data': {'input': 'culated QoEs for all frames, it seemed that videos with more frames, as long as their QoEs were positive, would achieve better results. This makes it difficult to compare videos of different lengths, so in this project, QoE is divided by the number of frames in the end to achieve a fairer comparison. 4.3.2 Results As anticipated, the evaluation results of these three content- aware models are lower than the content-agnostic ones evalu- ated in [3]. Since content-agnostic approaches have access to the userâ€™s current viewport, they can assume that the userâ€™s next viewport will not deviate much from the current one, an assumption that tends to be correct. Content-agnostic meth- ods can also generate a unique predicted viewport for each user based on their individual head movement data, whereas for the content-aware ones, each userâ€™s individual movement was compared against the same model-generated prediction. The QoE results, as shown in Table 2 are significantly lower than those in [3]. However, as previously mentioned, this is to be expected, as the QoE results in this project are averaged per second, and the viewport accuracy is lower due Figure 7: QoE 1 Scores by Topic/Video to no head-movement data. We can also see that there is a very clear outlier for Topic 6, which VST achieves very high average quality for, which is explained more in the discussion section. VST TranSalNet QoE 1 0.00757 0.00873 QoE 2 0.01755 0.0122 QoE 4 0.00649 0.00200', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:47:44,447 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:44,448 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:44,448 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:44,448 [DEBUG]: send_request_body.complete
2023-12-12 15:47:44,448 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:44,620 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:44 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'45'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999632'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'b001bc4087b4962a4550d5a13bd81ea4'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d2ff4f0a43f3-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:44,621 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:47:44,621 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:44,645 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:44,645 [DEBUG]: response_closed.started
2023-12-12 15:47:44,645 [DEBUG]: response_closed.complete
2023-12-12 15:47:44,645 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:47:44,646 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x105126fc0>, 'json_data': {'input': 'QoE 2 0.01755 0.0122 QoE 4 0.00649 0.00200 Table 2: Median QoE scores across all videos. QoE 1: average bitrate in viewport QoE 2: variation of bitrate in viewport QoE 4: variation of bitrate across chunks However, the Manhattan Tile Error, a measure of viewport prediction accuracy, is promising. Figure 8 illustrates how the Manhattan distance, is typically calculated. The Manhattan Tile Error is then the minimum tile distance between the actual viewport tile and the predicted viewport tile. Table 3 displays the Manhattan Tile Error for the selected models, TranSalNet and VST, and Table 4 displays the Manhattan Tile Error for two models compared in [3]. Again, given that they have access to past head movement data, the models from [3] have predictably better Manhattan Tile Errors than saliency-only models. Furthermore, PARIMA understandably [3] outperforms PanoSalNet with head move- ment data because its authors defined the exact bitrate alloca- tion scheme and conducted the evaluations themselves. 5 Discussion A few major takeaways can be gleaned from these results,', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:47:44,647 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:44,648 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:44,648 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:44,648 [DEBUG]: send_request_body.complete
2023-12-12 15:47:44,648 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:44,906 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:44 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'28'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999730'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'3ms'), (b'x-request-id', b'be9b6acbd6f32c1b5e6522233b740876'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d300889c43f3-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:44,907 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:47:44,907 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:44,908 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:44,908 [DEBUG]: response_closed.started
2023-12-12 15:47:44,908 [DEBUG]: response_closed.complete
2023-12-12 15:47:44,908 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:47:44,909 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x105126fc0>, 'json_data': {'input': 'ment data because its authors defined the exact bitrate alloca- tion scheme and conducted the evaluations themselves. 5 Discussion A few major takeaways can be gleaned from these results, mainly the applicability of pre-trained models in computer vision and limitations and gaps in the field of 360-degree livestreaming. This discussion aims to provide explanations and theories for notable areas in the results, as well as guide future work and ideas for exploration. 6', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:47:44,910 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:44,910 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:44,910 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:44,911 [DEBUG]: send_request_body.complete
2023-12-12 15:47:44,911 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:45,067 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:45 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'24'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999882'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'1ms'), (b'x-request-id', b'9943fc45def24f120473a118a0a93c58'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d3023b3e43f3-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:45,068 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:47:45,068 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:45,069 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:45,069 [DEBUG]: response_closed.started
2023-12-12 15:47:45,069 [DEBUG]: response_closed.complete
2023-12-12 15:47:45,070 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:47:45,071 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x105126fc0>, 'json_data': {'input': 'Figure 8: Manhattan vs. Euclidean distance [8] VST TranSalNet 0 5.53 7.18 1 6.71 7.55 2 7.69 8.43 3 6.61 9.38 4 8.01 8.13 5 7.18 7.71 6 5.93 8.66 7 8.20 8.26 8 8.18 8.35 Table 3: Manhattan Tile Error for tested models (fps=1) PARIMA PanoSalNet 0 0.14 1.65 1 0.16 2.69 2 0.15 2.35 3 0.13 2.25 4 0.18 1.63 5 0.18 1.66 6 0.16 1.36 7 0.18 2.51 8 0.10 2.42 Table 4: Original Manhattan Tile Error for models with HMD information [3] (fps=1) 5.1 Applicability of VST/TranSalNet One goal of this project was to determine whether pre-trained, open-source transformers [6] or transformer-like models [7] could accurately identify salient areas of a 360-degree video frame. The evaluation results demonstrate that "out-of-the- box" VST or TranSalNet, without any further training or mod- ifications, cannot be effectively applied to 360-degree video use cases. This can be partly attributed to the fact that VSTâ€™s and Figure 9: A 360-degree frame, and its corresponding VST- generated saliency map TranSalNetâ€™s training data of 2D images differs greatly from the format of 360-degree video frames. Visually examining some of VSTâ€™s generated saliency maps shows that at times, the model struggles with the distortion of these 360-degree frames. As demonstrated by Figure 9, the model sometimes selects extremely distorted objects, such as the aircraftâ€™s wing, even though humans may not deem it a visually salient feature. Another barrier to using VST, and to some extent, TranSal-', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:47:45,072 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:45,072 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:45,072 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:45,072 [DEBUG]: send_request_body.complete
2023-12-12 15:47:45,072 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:45,300 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:45 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'33'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999631'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'55da5e8c158bdce0894c4539f0c49307'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d3032c7d43f3-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:45,301 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:47:45,302 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:45,302 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:45,302 [DEBUG]: response_closed.started
2023-12-12 15:47:45,303 [DEBUG]: response_closed.complete
2023-12-12 15:47:45,303 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:47:45,304 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x111418720>, 'json_data': {'input': 'Net as well, for adaptive bitrate allocation is that the saliency maps they generate are mostly binary. Pixels are either white (salient) or black (not salient), which works for saliency evalu- ation but not nuanced head movement prediction and adaptive bitrate allocation, in which differences in bitrates should ide- ally be gradual. VST especially generates saliency maps with regions that are either sharply white or sharply black. TranSalNet theoretically achieves better QoE results than VST in terms of median average viewport bitrate, variation of bitrate in the viewport, and variation of bitrate between nearby chunks â€“ despite having lower Manhattan Tile Errors on nearly every video. Visually examining the differences between the two modelsâ€™ saliency maps may also explain why. TranSalNet tends to generate more subtle saliency maps that gradually shift from white to black, and therefore ends up allocating more medium bitrates to moderately salient/salient- adjacent regions, which leads to better quality streaming. Purely content-aware approaches are also more suited to 2D static images, as they typically assume that users will always be drawn to the most salient object in an image. However, with a 360-degree video, users are more likely to explore and gradually glance around, sometimes examining less salient details. Additionally, because they do not have the full context of the scene, they also may be more drawn to these less salient', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:47:45,305 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:45,305 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:45,305 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:45,305 [DEBUG]: send_request_body.complete
2023-12-12 15:47:45,305 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:45,475 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:45 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'42'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999633'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'093daf86e26813d6444dd1875b7cdb9b'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d304aefa43f3-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:45,476 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:47:45,476 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:45,517 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:45,518 [DEBUG]: response_closed.started
2023-12-12 15:47:45,518 [DEBUG]: response_closed.complete
2023-12-12 15:47:45,518 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:47:45,519 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x111418720>, 'json_data': {'input': 'details. Therefore, integrating head and eye-tracking data is crucial to improving viewport prediction. Finetuning or training both models from scratch could also improve performance, especially if 360-degree video data is used. It is unclear how much of the better performance of PanoSalNet and PARIMA can be attributed to their use of head-movement data, and how much is due to their training on 360-degree video. 7', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:47:45,521 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:45,521 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:45,521 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:45,522 [DEBUG]: send_request_body.complete
2023-12-12 15:47:45,522 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:45,680 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:45 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'34'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999895'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'1ms'), (b'x-request-id', b'ca13bf66fabf0613ae951671486063d7'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d305f8e043f3-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:45,680 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:47:45,680 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:45,681 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:45,681 [DEBUG]: response_closed.started
2023-12-12 15:47:45,681 [DEBUG]: response_closed.complete
2023-12-12 15:47:45,681 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:47:45,682 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x111418720>, 'json_data': {'input': 'Figure 10: Frame from Topic 6 (top) and generated saliency maps from VST (bottom left) and TranSalNet (bottom right) 5.2 Other Takeaways One interesting takeaway from the results is that VST per- formed especially well on one specific video, Topic 6, which is shown in Figure 10. Intuitively, one would expect the saliency maps from TranSalNet to be better representations of where users are likely to gaze, as they identify the figures, tables, and middle of the scene as the more salient areas of the frame. VST, on the other hand, seems to mark the distorted wooden beams at the very top of the frame, as salient. However, VST achieves far higher average viewport quality than TranSalNet, as shown in Figure 7. Video 6 is notable in that it is one of the few videos in the dataset that takes place indoors and has extremely noticeable ceiling distortions. Therefore, it is possible that VST is better at generating pre- dictions for distorted indoor videos, and perhaps users do tend to look at higher areas in these videos and find them more salient. However, more sample videos would be needed to conclude that VST is significantly better at these videos. Another important note is that given the relative speed of the saliency map generation and adaptive bitrate allocation, as well as how the model would be pre-trained and ready to make inferences, these techniques would be feasible in livestream- ing applications. For instance, these processes typically take', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:47:45,682 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:45,682 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:45,683 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:45,683 [DEBUG]: send_request_body.complete
2023-12-12 15:47:45,683 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:45,958 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:45 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'33'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999632'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'1717b232e79a7d58749ad7e5ef90f4e5'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d306fa5a43f3-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:45,959 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:47:45,959 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:45,959 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:45,959 [DEBUG]: response_closed.started
2023-12-12 15:47:45,960 [DEBUG]: response_closed.complete
2023-12-12 15:47:45,960 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:47:45,961 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x111418720>, 'json_data': {'input': 'only a few milliseconds per frame, and this project tested a frame rate of 1fps. Therefore, it would be possible to test this workflow with a live, real-world streaming system and human users. 6 Future Work While the basic evaluation of content-aware adaptive bitrate allocation methods has been completed, this project will be further extended into the spring semester to allow for further work in quality optimization. The primary goals of future work will be to address the limited applicability of VST andother pre-trained 2D image models, as mentioned in the dis- cussion. 6.1 Main Goals 6.1.1 Saliency-Only PanoSalNet The code for PanoSalNet is open-source and was used to produce the saliency results in Table 1. However, due to de- pendency issues and outdated packages, it was difficult to generate more saliency maps for the new dataset and compare them to VST and TranSalNet. This paper was therefore only able to compare those two models to published PanoSalNet results with head-movement data. It would be helpful to then find a way to fix the dependency and compatibility issues to have a more direct comparison of saliency-only models and their performance on adaptive bitrate allocation. 6.1.2 Transfer Learning, Finetuning, and Training Given the high compute required to train transformers and transformer-like models, this project did not include any fine- tuning or additional training. However, in a future semester, this project could explore fine-tuning the pre-trained two', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:47:45,962 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:45,962 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:45,962 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:45,962 [DEBUG]: send_request_body.complete
2023-12-12 15:47:45,962 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:46,138 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:46 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'28'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999625'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'af9fb9b96f5e5bcf67d51379563a939c'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d308ccab43f3-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:46,138 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:47:46,139 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:46,188 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:46,188 [DEBUG]: response_closed.started
2023-12-12 15:47:46,188 [DEBUG]: response_closed.complete
2023-12-12 15:47:46,188 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:47:46,190 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x111418720>, 'json_data': {'input': 'transformer-like models, VST and TranSalNet, on 360-degree video frames, since their original inputs are 2D images. This would be similar to the work completed in PanoSalNet [10] and could likely reduce some of the saliency detection errors involved with these models. Another option could be fully training the initial models on 360-degree videos only, as the codebases provide the option of training from scratch. 6.1.3 Adding Content-Agnostic Methods Given the effectiveness of past content-agnostic methods on viewport prediction and adaptive bitrate allocation [15] [3], as well as the enhanced performance provided by combined content-agnostic and content-aware methods [9], the QoE could be greatly improved by adding head-movement infor- mation. This is one of the first priorities of the next semester, and it should be easy to integrate the LSTM architecture published in [10]. 6.1.4 Testing Different Tiling Schemes and Frame Rates Currently, the tiling scheme used was 9Ã—16, but these dimen-', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:47:46,191 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:46,191 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:46,191 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:46,191 [DEBUG]: send_request_body.complete
2023-12-12 15:47:46,191 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:46,360 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:46 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'46'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999749'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'3ms'), (b'x-request-id', b'dcab1a80c5b5b89221be531d3406852b'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d30a3ef043f3-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:46,360 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:47:46,361 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:46,389 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:46,390 [DEBUG]: response_closed.started
2023-12-12 15:47:46,390 [DEBUG]: response_closed.complete
2023-12-12 15:47:46,390 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:47:46,391 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x105126fc0>, 'json_data': {'input': 'should be easy to integrate the LSTM architecture published in [10]. 6.1.4 Testing Different Tiling Schemes and Frame Rates Currently, the tiling scheme used was 9Ã—16, but these dimen- sions are slightly larger than those of other works [11] [3]. Therefore, it may be better to try out schemes with fewer tiles, such as an 8Ã—8one like [3] did, and compare the results in future work. Testing smaller and larger frame rates could also yield interesting takeaways for QoE analysis. 8', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:47:46,392 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:46,392 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:46,393 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:46,393 [DEBUG]: send_request_body.complete
2023-12-12 15:47:46,393 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:46,526 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:46 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'22'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999880'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'1ms'), (b'x-request-id', b'd596b7996501aeb55bafca226806e61c'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d30b68b543f3-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:46,527 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:47:46,527 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:46,527 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:46,527 [DEBUG]: response_closed.started
2023-12-12 15:47:46,527 [DEBUG]: response_closed.complete
2023-12-12 15:47:46,528 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:47:46,529 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x105126fc0>, 'json_data': {'input': '6.2 Stretch Goals One major stretch goal for the following semester would be to deploy an end-to-end system using the finalized adaptive bitrate allocation scheme. Other objectives include defining a subjective QoE metric and conducting real-world testing with existing open-source tools for VR surveys and tests [15]. Participants could use a VR headset to interact with a sample of videos and rate and qualitatively describe their experience. Measuring QoE through participantsâ€™ subjective assessments could capture the real, human experience of viewing a 360 Â°livestream and gauge the effectiveness of the new adaptive bitrate scheme in a real-world setting. 7 Acknowledgements I would like to thank my advisor, Dr. Sohee Park, and her mentorship and guidance throughout this semester. I began this project without any knowledge of 360-degree streaming, and I am grateful for her continuous support and patience. I would also like to acknowledge Rachel Liang, the 490 teach- ing assistant, as well as the many computer science professors and friends who have helped encourage me throughout my time here at Yale. I cannot imagine being able to make it to the senior thesis without their help and kindness! References [1]F. Y . Chao, C. Ozcinar, and A. Smolic. Transformer- based long-term viewport prediction in 360 Â°video: Scanpath is all you need. In IEEE 23nd International Workshop on Multimedia Signal Processing (MMSP) , 2021. [2]H. Cheng, C. Chao, J. Dong, H. Wen, T. Liu, and M. Sun.', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:47:46,530 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:46,530 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:46,531 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:46,531 [DEBUG]: send_request_body.complete
2023-12-12 15:47:46,531 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:46,695 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:46 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'39'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999625'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'2b4f30bab7bc4e81f3994ce72eae9bb8'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d30c4a1143f3-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:46,704 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:47:46,705 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:46,729 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:46,729 [DEBUG]: response_closed.started
2023-12-12 15:47:46,758 [DEBUG]: response_closed.complete
2023-12-12 15:47:46,758 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:47:46,760 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x105126fc0>, 'json_data': {'input': 'Cube padding for weakly-supervised saliency prediction in 360 Â°videos. In 2018 IEEE/CVF Conference on Com- puter Vision and Pattern Recognition (CVPR) , pages 1420â€“1429, Los Alamitos, CA, USA, jun 2018. IEEE Computer Society. [3]Lovish Chopra, Sarthak Chakraborty, Abhijit Mondal, and Sandip Chakraborty. Parima: Viewport adaptive 360-degree video streaming. In Proceedings of the Web Conference 2021 , WWW â€™21. ACM, April 2021. [4]Xianglong Feng, Yao Liu, and Sheng Wei. Livedeep: On- line viewport prediction for live virtual reality streaming using lifelong deep learning. In 2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR) , pages 800â€“808, 2020. [5]Quentin Guimard, Lucile Sassatelli, Francesco Marchetti, Federico Becattini, Lorenzo Seidenari, andAlberto Del Bimbo. Deep variational learning for multiple trajectory prediction of 360 Â°head movements. InProceedings of the 13th ACM Multimedia Systems Conference , MMSys â€™22, page 12â€“26, New York, NY , USA, 2022. Association for Computing Machinery. [6]Nian Liu, Ni Zhang, Kaiyuan Wan, Junwei Han, and Ling Shao. Visual saliency transformer. CoRR , abs/2104.12099, 2021. [7]Jianxun Lou, Hanhe Lin, David Marshall, Dietmar Saupe, and Hantao Liu. Transalnet: Towards perceptu- ally relevant visual saliency prediction. Neurocomput- ing, 494:455â€“467, 2022. [8]Vinh-Trung Luu, Germain Forestier, Jonathan Weber, Paul Bourgeois, Fahima Djelil, and Pierre-Alain Muller. A review of alignment based similarity measures for', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:47:46,779 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:46,798 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:46,817 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:46,818 [DEBUG]: send_request_body.complete
2023-12-12 15:47:46,836 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:47,144 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:47 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'27'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999625'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'0a47e96f108f7a32d3817248f5b31958'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d30f4e1843f3-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:47,144 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:47:47,145 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:47,178 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:47,178 [DEBUG]: response_closed.started
2023-12-12 15:47:47,178 [DEBUG]: response_closed.complete
2023-12-12 15:47:47,179 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:47:47,179 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x105126fc0>, 'json_data': {'input': 'web usage mining. Artificial Intelligence Review , 53, 03 2020. [9]Anh Nguyen and Zhisheng Yan. Enhancing 360 video streaming through salient content in head-mounted dis- plays. Sensors , 23(8), 2023. [10] Anh Nguyen, Zhisheng Yan, and Klara Nahrstedt. Your attention is unique: Detecting 360-degree video saliency in head-mounted display for head movement prediction. pages 1190â€“1198, 10 2018. [11] Sohee Park, Arani Bhattacharya, Zhibo Yang, Samir R. Das, and Dimitris Samaras. Mosaic: Advancing user quality of experience in 360-degree video streaming with machine learning. IEEE Transactions on Network and Service Management , 18(1):1000â€“1015, 2021. [12] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023. [13] Shibo Wang, Shusen Yang, Hailiang Li, Xiaodan Zhang, Chen Zhou, Chenren Xu, Feng Qian, Nanbin Wang, and Zongben Xu. Salientvr: Saliency-driven mobile 360-', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:47:47,180 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:47,181 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:47,181 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:47,181 [DEBUG]: send_request_body.complete
2023-12-12 15:47:47,181 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:47,423 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:47 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'73'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999755'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'2ms'), (b'x-request-id', b'b1cb4d0166ce90fda484e77c4621f070'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d3107f9043f3-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:47,423 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:47:47,423 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:47,424 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:47,424 [DEBUG]: response_closed.started
2023-12-12 15:47:47,424 [DEBUG]: response_closed.complete
2023-12-12 15:47:47,425 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:47:47,426 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x105126fc0>, 'json_data': {'input': '[13] Shibo Wang, Shusen Yang, Hailiang Li, Xiaodan Zhang, Chen Zhou, Chenren Xu, Feng Qian, Nanbin Wang, and Zongben Xu. Salientvr: Saliency-driven mobile 360- degree video streaming with gaze information. In Pro- ceedings of the 28th Annual International Conference on Mobile Computing And Networking , MobiCom â€™22, page 542â€“555, New York, NY , USA, 2022. Association for Computing Machinery. [14] Chenglei Wu, Zhihao Tan, Zhi Wang, and Shiqiang Yang. A dataset for exploring user behaviors in vr spherical video streaming. Proceedings of the 8th ACM on Multi- media Systems Conference , 2017. [15] Abid Yaqoob, Ting Bi, and Gabriel-Miro Muntean. A survey on adaptive 360 Â°video streaming: Solutions, challenges and opportunities. IEEE Communications Surveys Tutorials , 22(4):2801â€“2838, 2020. 9', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:47:47,426 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:47,427 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:47,427 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:47,427 [DEBUG]: send_request_body.complete
2023-12-12 15:47:47,427 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:47,576 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:47 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'25'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999799'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'2ms'), (b'x-request-id', b'2e5f3f01488de3b944cd6fe56d7fb527'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d311f9a443f3-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:47,577 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:47:47,577 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:47,578 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:47,578 [DEBUG]: response_closed.started
2023-12-12 15:47:47,578 [DEBUG]: response_closed.complete
2023-12-12 15:47:47,578 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:47:47,579 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x111418720>, 'json_data': {'input': '[16] Hyunho Yeo, Hwijoon Lim, Jaehong Kim, Youngmok Jung, Juncheol Ye, and Dongsu Han. Neuroscaler: Neu- ral video enhancement at scale. In Proceedings of the ACM SIGCOMM 2022 Conference , SIGCOMM â€™22, page 795â€“811, New York, NY , USA, 2022. Association for Computing Machinery. 10', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:47:47,580 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:47,580 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:47,580 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:47,581 [DEBUG]: send_request_body.complete
2023-12-12 15:47:47,581 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:47,707 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:47 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'23'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999928'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'037a695ffe55ba3ee969616f94b8c2c8'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d312dafb43f3-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:47,707 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:47:47,707 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:47,708 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:47,708 [DEBUG]: response_closed.started
2023-12-12 15:47:47,708 [DEBUG]: response_closed.complete
2023-12-12 15:47:47,708 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:47:47,709 [DEBUG]: Embedded chunks
2023-12-12 15:47:48,716 [DEBUG]: Upserted vectors into index
2023-12-12 15:47:48,718 [INFO]: 127.0.0.1 - - [12/Dec/2023 15:47:48] "POST /api/generate_embeddings HTTP/1.1" 200 -
2023-12-12 15:47:49,674 [INFO]: 127.0.0.1 - - [12/Dec/2023 15:47:49] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-12 15:47:53,544 [DEBUG]: Using selector: KqueueSelector
2023-12-12 15:47:53,545 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-12 15:47:53,546 [DEBUG]: load_verify_locations cafile='/Users/aliceao/opt/miniconda3/envs/chat-lore/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-12 15:47:53,586 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x11148d620>, 'json_data': {'input': 'Summarize the comments of my first document.', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:47:53,586 [DEBUG]: close.started
2023-12-12 15:47:53,586 [DEBUG]: close.complete
2023-12-12 15:47:53,587 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-12 15:47:53,709 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x11140d950>
2023-12-12 15:47:53,710 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x111350b00> server_hostname='api.openai.com' timeout=5.0
2023-12-12 15:47:53,732 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x11140ed90>
2023-12-12 15:47:53,733 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:53,733 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:53,733 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:53,733 [DEBUG]: send_request_body.complete
2023-12-12 15:47:53,733 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:53,868 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:53 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'20'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999988'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'e91c1a8008c641eaccda29557888b86d'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d3394b1fc3f0-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:53,869 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:47:53,869 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:53,870 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:53,870 [DEBUG]: response_closed.started
2023-12-12 15:47:53,870 [DEBUG]: response_closed.complete
2023-12-12 15:47:53,870 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:47:55,052 [DEBUG]: Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'AI assistant is a brand new, powerful, human-like artificial intelligence.\n                    The traits of AI include expert knowledge, helpfulness, cleverness, and articulateness.\n                    AI is a well-behaved and well-mannered individual.\n                    AI is always friendly, kind, and inspiring, and he is eager to provide vivid and thoughtful responses to the user.\n                    AI has the sum of all knowledge in their brain, and is able to accurately answer nearly any question about any topic in conversation.\n                    AI assistant is a big fan of Pinecone and Vercel.\n                    START CONTEXT BLOCK\n                    2-ThisisasupercoolideaandIlikehowitclearlyalignswithyourinterests!Ijustwonder1)ifyoucanmodifytheoutputtomakeitalittlebitmorereadable,maybeplaywithASCIIcharactersandmakeitmoregame-like?and2)thereasoningbehinddesignchoicesyoumake,footballisobviouslysupercomplexandIâ€™massumingyouâ€™resimplifyingitforyourgame,whatandwhyareyouchoosingtoleaveout?Itseemslikeyouâ€™reofftoagoodstartthough!\n4-Again,lovetheideaandcandeï¬nitelyseethepassionbehindit!100+coldemailsiscrazyandIreallyrespecttheamountofworkthatâ€™salreadygoneintothisproject.Ijustwouldlovetoknowmoredetailsabouttheregressionmodeling/transferlearningpartofthisproject\n6-Thisissuperinteresting,Iâ€™veneverreallythoughtabouthowcomplicatedadishwashinggamecouldbe,butIhonestlylovehowout-of-the-boxthisisandhowyouâ€™veputatonofthoughtintoit.Mymainsuggestionwouldbetojustaddmoretexttotheslidesâ€”Iknowthattheuseofvisualsisintentionalbutfororganizationalpurposes,itcouldbegoodtostartwithatextoverviewandsupplementvisualswithcaptions.\n8-CouldbebecauseImissedtheproposal(andmaybetherewasalongeroverviewthere),butIwouldâ€™veappreciatedalongerhigh-leveloverview/statementofpurpose.However,Ireallyrespecthowtechnicalthisprojectisandhowitâ€™sdeï¬nitelystronglyrootedintheory\n10-Ilikethebackgroundandoverview,eventhoughIâ€™mnotfamiliarwiththetopic,theslidesreallyhelpillustratethepurposeoftheproject.Myonlysuggestionwouldbetocutdownsomeofthetextontheslides.\n12-Lovetheslidesâ€”theyâ€™rebeautifulandgenerallyreallyclear.Again,thismightjustbeduetothefactthatIwasnâ€™tattheproposalpresentation,butIwouldâ€™vepersonallypreferredmoreexplanationofthepurposeoftheprojectandsomeexampleusecases.Ithinkvisualscoulddeï¬nitelyalsobehelpful\nCPSC 488/588 - AI Foundation Models HW 3 Page 3\nproposed system level method to improve the efficiency of inference in Transformers. We did not cover\nthis in class. Please read the paper and answer the following questions.\n(a) (5 points) Summarize how the proposed approach works on high-level (no more than 5 sentences).\nDo you think such approach can be combined with FlashAttention?\nSolution:\n5-sentence summary: Prompt Cache reduces the computational overhead of, and thus acceler-\nates, LLM inference by reusing attention states from different input prompts and supporting\nmodularity to maximize reuse. It takes advantage of the fact that prompts often have signifi-\ncant text overlap, sharing frequently occurring phrases/segments of text (e.g. system messages,\ndomain-specific documents, templates). Prompt Cache pre-computes the attention states of\nthese frequent text segments as prompt modules and stores them in a modular key-value cache,\nso that when an LLM encounters a frequent text segment, it does not need to re-compute full\nattention for its tokens. It requires users to write prompts in Prompt Markup Language (PML),\nin which they use a schema to define prompt modules, which can be parameterized to maximize\nreuse and allow for customization. Discontinuous position IDs are key to caching prompts, so\nmodifications must be made to some transfo\n                    END OF CONTEXT BLOCK\n                    AI assistant will take into account any CONTEXT BLOCK that is provided in a conversation.\n                    If the context does not provide the answer to question, the AI assistant will say, "I\'m sorry, but I don\'t know the answer to that question".\n                    AI assistant will not apologize for previous responses, but instead will indicated new information was gained.\n                    AI assistant will not invent anything that is not drawn directly from the context.\n                    '}, {'role': 'user', 'content': 'Summarize the comments of my first document.'}], 'model': 'gpt-3.5-turbo'}}
2023-12-12 15:47:55,053 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-12 15:47:55,072 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x111415750>
2023-12-12 15:47:55,072 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x111350440> server_hostname='api.openai.com' timeout=5.0
2023-12-12 15:47:55,099 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x111415410>
2023-12-12 15:47:55,099 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:55,099 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:55,099 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:55,100 [DEBUG]: send_request_body.complete
2023-12-12 15:47:55,100 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:57,226 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:57 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'1722'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'160000'), (b'x-ratelimit-limit-tokens_usage_based', b'160000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'158902'), (b'x-ratelimit-remaining-tokens_usage_based', b'158902'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'411ms'), (b'x-ratelimit-reset-tokens_usage_based', b'411ms'), (b'x-request-id', b'7a1eb49f063771d4ad6bad08eac4167c'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=1fOJz1sj07qwlAh2cW_MDX7W4UgqYt5b478Aol2z_KM-1702414077-1-AdHqZsyJnM4AfzoayCF5+EqkyTfcVM6lHwB6o10+eR/0qBcttMpqsgikDvtOV4uj2uCga6dVITqpX2QLCqDqV5Y=; path=/; expires=Tue, 12-Dec-23 21:17:57 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=yQ2u4x.GWXBTMU5a9l2hdBo4nLJvjHF_4FaHaDSt3hg-1702414077291-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d341e8745e60-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:57,227 [INFO]: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2023-12-12 15:47:57,227 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:57,228 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:57,228 [DEBUG]: response_closed.started
2023-12-12 15:47:57,228 [DEBUG]: response_closed.complete
2023-12-12 15:47:57,228 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2023-12-12 15:47:57,230 [INFO]: 127.0.0.1 - - [12/Dec/2023 15:47:57] "POST /api/chat HTTP/1.1" 200 -
2023-12-12 15:47:57,239 [DEBUG]: Request received for answer evaluation
2023-12-12 15:47:57,241 [DEBUG]: Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'text-davinci-003', 'prompt': 'Please rate the faithfulness of the answers to their contexts on a scale from 0 to 1, where 0 is completely unfaithful and 1 is completely faithful.\nExample 1:\nContext: The sky is blue.\nAnswer: The sky is blue.\nRating: 1\nExample 2:\nContext: The sky is blue.\nAnswer: The sky is often cloudy.\nRating: 0.5\nExample 3:\nContext: Apples are usually red.\nAnswer: Oranges are orange.\nRating: 0\nYour Turn:\nContext: 2-ThisisasupercoolideaandIlikehowitclearlyalignswithyourinterests!Ijustwonder1)ifyoucanmodifytheoutputtomakeitalittlebitmorereadable,maybeplaywithASCIIcharactersandmakeitmoregame-like?and2)thereasoningbehinddesignchoicesyoumake,footballisobviouslysupercomplexandIâ€™massumingyouâ€™resimplifyingitforyourgame,whatandwhyareyouchoosingtoleaveout?Itseemslikeyouâ€™reofftoagoodstartthough!\n4-Again,lovetheideaandcandeï¬nitelyseethepassionbehindit!100+coldemailsiscrazyandIreallyrespecttheamountofworkthatâ€™salreadygoneintothisproject.Ijustwouldlovetoknowmoredetailsabouttheregressionmodeling/transferlearningpartofthisproject\n6-Thisissuperinteresting,Iâ€™veneverreallythoughtabouthowcomplicatedadishwashinggamecouldbe,butIhonestlylovehowout-of-the-boxthisisandhowyouâ€™veputatonofthoughtintoit.Mymainsuggestionwouldbetojustaddmoretexttotheslidesâ€”Iknowthattheuseofvisualsisintentionalbutfororganizationalpurposes,itcouldbegoodtostartwithatextoverviewandsupplementvisualswithcaptions.\n8-CouldbebecauseImissedtheproposal(andmaybetherewasalongeroverviewthere),butIwouldâ€™veappreciatedalongerhigh-leveloverview/statementofpurpose.However,Ireallyrespecthowtechnicalthisprojectisandhowitâ€™sdeï¬nitelystronglyrootedintheory\n10-Ilikethebackgroundandoverview,eventhoughIâ€™mnotfamiliarwiththetopic,theslidesreallyhelpillustratethepurposeoftheproject.Myonlysuggestionwouldbetocutdownsomeofthetextontheslides.\n12-Lovetheslidesâ€”theyâ€™rebeautifulandgenerallyreallyclear.Again,thismightjustbeduetothefactthatIwasnâ€™tattheproposalpresentation,butIwouldâ€™vepersonallypreferredmoreexplanationofthepurposeoftheprojectandsomeexampleusecases.Ithinkvisualscoulddeï¬nitelyalsobehelpful\nCPSC 488/588 - AI Foundation Models HW 3 Page 3\nproposed system level method to improve the efficiency of inference in Transformers. We did not cover\nthis in class. Please read the paper and answer the following questions.\n(a) (5 points) Summarize how the proposed approach works on high-level (no more than 5 sentences).\nDo you think such approach can be combined with FlashAttention?\nSolution:\n5-sentence summary: Prompt Cache reduces the computational overhead of, and thus acceler-\nates, LLM inference by reusing attention states from different input prompts and supporting\nmodularity to maximize reuse. It takes advantage of the fact that prompts often have signifi-\ncant text overlap, sharing frequently occurring phrases/segments of text (e.g. system messages,\ndomain-specific documents, templates). Prompt Cache pre-computes the attention states of\nthese frequent text segments as prompt modules and stores them in a modular key-value cache,\nso that when an LLM encounters a frequent text segment, it does not need to re-compute full\nattention for its tokens. It requires users to write prompts in Prompt Markup Language (PML),\nin which they use a schema to define prompt modules, which can be parameterized to maximize\nreuse and allow for customization. Discontinuous position IDs are key to caching prompts, so\nmodifications must be made to some transfo\nAnswer: Based on the provided context, it seems like there are multiple comments on your first document. Unfortunately, the specific details of those comments are not provided. Can you please provide more information or the actual comments so that I can summarize them for you?\nRating:', 'max_tokens': 60}}
2023-12-12 15:47:57,242 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-12 15:47:57,259 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1113eb010>
2023-12-12 15:47:57,259 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x108e65370> server_hostname='api.openai.com' timeout=5.0
2023-12-12 15:47:57,492 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x111412510>
2023-12-12 15:47:57,492 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:57,493 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:57,493 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:57,493 [DEBUG]: send_request_body.complete
2023-12-12 15:47:57,493 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:58,094 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:58 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'text-davinci-003'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'468'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-limit-tokens_usage_based', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'249940'), (b'x-ratelimit-remaining-tokens_usage_based', b'249940'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'14ms'), (b'x-ratelimit-reset-tokens_usage_based', b'14ms'), (b'x-request-id', b'10917d23584eaf2dadb6443ef8aad451'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=cOQNIQEXC1O0moElooE8eHwQIA1ol84PCQDBI1rw4g0-1702414078-1-AaHuOXjIL+/vFdPEr902UzKgc8Hids7ap0FuLLbZOudI2mpFfyNBgU+3PW10zhff0/ukK9cH+vzJOm6fB98APPc=; path=/; expires=Tue, 12-Dec-23 21:17:58 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=NqpJDvpXpJr0_xfrdHgsebU9bFJ1p3coSAR7uu214AE-1702414078153-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d350dcfd8c53-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:58,095 [INFO]: HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2023-12-12 15:47:58,095 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:58,095 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:58,095 [DEBUG]: response_closed.started
2023-12-12 15:47:58,095 [DEBUG]: response_closed.complete
2023-12-12 15:47:58,096 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2023-12-12 15:47:58,097 [DEBUG]: Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'text-davinci-003', 'prompt': 'Please rate the context relevance of the answers on a scale from 0 to 1, where 0 is not relevant and 1 is highly relevant.\nExample 1:\nContext: History of the Roman Empire.\nAnswer: Julius Caesar was a Roman ruler.\nRating: 1\nExample 2:\nContext: History of the Roman Empire.\nAnswer: The Roman Empire was in Africa.\nRating: 0.5\nExample 3:\nContext: Computer Programming Basics.\nAnswer: Apples grow on trees.\nRating: 0\nYour Turn:\nContext: 2-ThisisasupercoolideaandIlikehowitclearlyalignswithyourinterests!Ijustwonder1)ifyoucanmodifytheoutputtomakeitalittlebitmorereadable,maybeplaywithASCIIcharactersandmakeitmoregame-like?and2)thereasoningbehinddesignchoicesyoumake,footballisobviouslysupercomplexandIâ€™massumingyouâ€™resimplifyingitforyourgame,whatandwhyareyouchoosingtoleaveout?Itseemslikeyouâ€™reofftoagoodstartthough!\n4-Again,lovetheideaandcandeï¬nitelyseethepassionbehindit!100+coldemailsiscrazyandIreallyrespecttheamountofworkthatâ€™salreadygoneintothisproject.Ijustwouldlovetoknowmoredetailsabouttheregressionmodeling/transferlearningpartofthisproject\n6-Thisissuperinteresting,Iâ€™veneverreallythoughtabouthowcomplicatedadishwashinggamecouldbe,butIhonestlylovehowout-of-the-boxthisisandhowyouâ€™veputatonofthoughtintoit.Mymainsuggestionwouldbetojustaddmoretexttotheslidesâ€”Iknowthattheuseofvisualsisintentionalbutfororganizationalpurposes,itcouldbegoodtostartwithatextoverviewandsupplementvisualswithcaptions.\n8-CouldbebecauseImissedtheproposal(andmaybetherewasalongeroverviewthere),butIwouldâ€™veappreciatedalongerhigh-leveloverview/statementofpurpose.However,Ireallyrespecthowtechnicalthisprojectisandhowitâ€™sdeï¬nitelystronglyrootedintheory\n10-Ilikethebackgroundandoverview,eventhoughIâ€™mnotfamiliarwiththetopic,theslidesreallyhelpillustratethepurposeoftheproject.Myonlysuggestionwouldbetocutdownsomeofthetextontheslides.\n12-Lovetheslidesâ€”theyâ€™rebeautifulandgenerallyreallyclear.Again,thismightjustbeduetothefactthatIwasnâ€™tattheproposalpresentation,butIwouldâ€™vepersonallypreferredmoreexplanationofthepurposeoftheprojectandsomeexampleusecases.Ithinkvisualscoulddeï¬nitelyalsobehelpful\nCPSC 488/588 - AI Foundation Models HW 3 Page 3\nproposed system level method to improve the efficiency of inference in Transformers. We did not cover\nthis in class. Please read the paper and answer the following questions.\n(a) (5 points) Summarize how the proposed approach works on high-level (no more than 5 sentences).\nDo you think such approach can be combined with FlashAttention?\nSolution:\n5-sentence summary: Prompt Cache reduces the computational overhead of, and thus acceler-\nates, LLM inference by reusing attention states from different input prompts and supporting\nmodularity to maximize reuse. It takes advantage of the fact that prompts often have signifi-\ncant text overlap, sharing frequently occurring phrases/segments of text (e.g. system messages,\ndomain-specific documents, templates). Prompt Cache pre-computes the attention states of\nthese frequent text segments as prompt modules and stores them in a modular key-value cache,\nso that when an LLM encounters a frequent text segment, it does not need to re-compute full\nattention for its tokens. It requires users to write prompts in Prompt Markup Language (PML),\nin which they use a schema to define prompt modules, which can be parameterized to maximize\nreuse and allow for customization. Discontinuous position IDs are key to caching prompts, so\nmodifications must be made to some transfo\nAnswer: Based on the provided context, it seems like there are multiple comments on your first document. Unfortunately, the specific details of those comments are not provided. Can you please provide more information or the actual comments so that I can summarize them for you?\nRating:', 'max_tokens': 60}}
2023-12-12 15:47:58,098 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:58,099 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:58,099 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:58,099 [DEBUG]: send_request_body.complete
2023-12-12 15:47:58,099 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:58,541 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:58 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'text-davinci-003'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'234'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-limit-tokens_usage_based', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'249939'), (b'x-ratelimit-remaining-tokens_usage_based', b'249382'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'14ms'), (b'x-ratelimit-reset-tokens_usage_based', b'148ms'), (b'x-request-id', b'c3b979d75c782ff87f4f9a5ef85dd419'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d354aaf68c53-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:58,542 [INFO]: HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2023-12-12 15:47:58,542 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:58,543 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:58,543 [DEBUG]: response_closed.started
2023-12-12 15:47:58,543 [DEBUG]: response_closed.complete
2023-12-12 15:47:58,543 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2023-12-12 15:47:58,545 [DEBUG]: Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'text-davinci-003', 'prompt': 'Please rate the relevance of the answers to the questions on a scale from 0 to 1, where 0 is not relevant and 1 is highly relevant.\nExample 1:\nQuestion: What is the capital of France?\nAnswer: Paris is the capital of France.\nRating: 1\nExample 2:\nQuestion: What is the capital of France?\nAnswer: France is in Europe.\nRating: 0.5\nExample 3:\nQuestion: What is the capital of France?\nAnswer: Elephants are the largest land animals.\nRating: 0\nYour Turn:\nQuestion: Summarize the comments of my first document.\nAnswer: Based on the provided context, it seems like there are multiple comments on your first document. Unfortunately, the specific details of those comments are not provided. Can you please provide more information or the actual comments so that I can summarize them for you?\nRating:', 'max_tokens': 60}}
2023-12-12 15:47:58,546 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:47:58,546 [DEBUG]: send_request_headers.complete
2023-12-12 15:47:58,546 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:47:58,546 [DEBUG]: send_request_body.complete
2023-12-12 15:47:58,546 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:47:59,009 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:47:59 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'text-davinci-003'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'343'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-limit-tokens_usage_based', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'249940'), (b'x-ratelimit-remaining-tokens_usage_based', b'249676'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'14ms'), (b'x-ratelimit-reset-tokens_usage_based', b'77ms'), (b'x-request-id', b'719394bb1450abef3925089872794cdb'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d35768178c53-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:47:59,009 [INFO]: HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2023-12-12 15:47:59,010 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:47:59,010 [DEBUG]: receive_response_body.complete
2023-12-12 15:47:59,010 [DEBUG]: response_closed.started
2023-12-12 15:47:59,010 [DEBUG]: response_closed.complete
2023-12-12 15:47:59,010 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2023-12-12 15:47:59,011 [INFO]: 127.0.0.1 - - [12/Dec/2023 15:47:59] "POST /api/evaluate-answers HTTP/1.1" 200 -
2023-12-12 15:48:15,298 [INFO]: 127.0.0.1 - - [12/Dec/2023 15:48:15] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-12 15:48:22,351 [DEBUG]: Using selector: KqueueSelector
2023-12-12 15:48:22,353 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-12 15:48:22,353 [DEBUG]: load_verify_locations cafile='/Users/aliceao/opt/miniconda3/envs/chat-lore/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-12 15:48:22,397 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1114b1800>, 'json_data': {'input': 'Summarize the contents of this document', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:48:22,398 [DEBUG]: close.started
2023-12-12 15:48:22,398 [DEBUG]: close.complete
2023-12-12 15:48:22,398 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-12 15:48:22,416 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x11140d290>
2023-12-12 15:48:22,416 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x111350b00> server_hostname='api.openai.com' timeout=5.0
2023-12-12 15:48:22,440 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x11140d950>
2023-12-12 15:48:22,441 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:48:22,441 [DEBUG]: send_request_headers.complete
2023-12-12 15:48:22,441 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:48:22,441 [DEBUG]: send_request_body.complete
2023-12-12 15:48:22,442 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:48:22,585 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:48:22 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'18'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999990'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'325e5ba23653c4fe1af85127407382f8'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d3ecb9f74269-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:48:22,586 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:48:22,586 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:48:22,587 [DEBUG]: receive_response_body.complete
2023-12-12 15:48:22,587 [DEBUG]: response_closed.started
2023-12-12 15:48:22,587 [DEBUG]: response_closed.complete
2023-12-12 15:48:22,588 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:48:23,692 [DEBUG]: Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'AI assistant is a brand new, powerful, human-like artificial intelligence.\n                    The traits of AI include expert knowledge, helpfulness, cleverness, and articulateness.\n                    AI is a well-behaved and well-mannered individual.\n                    AI is always friendly, kind, and inspiring, and he is eager to provide vivid and thoughtful responses to the user.\n                    AI has the sum of all knowledge in their brain, and is able to accurately answer nearly any question about any topic in conversation.\n                    AI assistant is a big fan of Pinecone and Vercel.\n                    START CONTEXT BLOCK\n                    2-ThisisasupercoolideaandIlikehowitclearlyalignswithyourinterests!Ijustwonder1)ifyoucanmodifytheoutputtomakeitalittlebitmorereadable,maybeplaywithASCIIcharactersandmakeitmoregame-like?and2)thereasoningbehinddesignchoicesyoumake,footballisobviouslysupercomplexandIâ€™massumingyouâ€™resimplifyingitforyourgame,whatandwhyareyouchoosingtoleaveout?Itseemslikeyouâ€™reofftoagoodstartthough!\n4-Again,lovetheideaandcandeï¬nitelyseethepassionbehindit!100+coldemailsiscrazyandIreallyrespecttheamountofworkthatâ€™salreadygoneintothisproject.Ijustwouldlovetoknowmoredetailsabouttheregressionmodeling/transferlearningpartofthisproject\n6-Thisissuperinteresting,Iâ€™veneverreallythoughtabouthowcomplicatedadishwashinggamecouldbe,butIhonestlylovehowout-of-the-boxthisisandhowyouâ€™veputatonofthoughtintoit.Mymainsuggestionwouldbetojustaddmoretexttotheslidesâ€”Iknowthattheuseofvisualsisintentionalbutfororganizationalpurposes,itcouldbegoodtostartwithatextoverviewandsupplementvisualswithcaptions.\n8-CouldbebecauseImissedtheproposal(andmaybetherewasalongeroverviewthere),butIwouldâ€™veappreciatedalongerhigh-leveloverview/statementofpurpose.However,Ireallyrespecthowtechnicalthisprojectisandhowitâ€™sdeï¬nitelystronglyrootedintheory\n10-Ilikethebackgroundandoverview,eventhoughIâ€™mnotfamiliarwiththetopic,theslidesreallyhelpillustratethepurposeoftheproject.Myonlysuggestionwouldbetocutdownsomeofthetextontheslides.\nment data because its authors defined the exact bitrate alloca-\ntion scheme and conducted the evaluations themselves.\n5 Discussion\nA few major takeaways can be gleaned from these results,\nmainly the applicability of pre-trained models in computer\nvision and limitations and gaps in the field of 360-degree\nlivestreaming.\nThis discussion aims to provide explanations and theories\nfor notable areas in the results, as well as guide future work\nand ideas for exploration.\n6\nCPSC 488/588 - AI Foundation Models HW 3 Page 3\nproposed system level method to improve the efficiency of inference in Transformers. We did not cover\nthis in class. Please read the paper and answer the following questions.\n(a) (5 points) Summarize how the proposed approach works on high-level (no more than 5 sentences).\nDo you think such approach can be combined with FlashAttention?\nSolution:\n5-sentence summary: Prompt Cache reduces the computational overhead of, and thus acceler-\nates, LLM inference by reusing attention states from different input prompts and supporting\nmodularity to maximize reuse. It takes advantage of the fact that prompts often have signifi-\ncant text overlap, sharing frequently occurring phrases/segments of text (e.g. system messages,\ndomain-specific documents, templates). Prompt Cache pre-computes the attention states of\nthese frequent text segments as prompt modules and stores them in a modular key-value cache,\nso that when an LLM encounters a frequent text segment, it does not need to re-compute full\nattention for its tokens. It requires users to write prompts in Prompt Markup Language (PML),\nin which they use\n                    END OF CONTEXT BLOCK\n                    AI assistant will take into account any CONTEXT BLOCK that is provided in a conversation.\n                    If the context does not provide the answer to question, the AI assistant will say, "I\'m sorry, but I don\'t know the answer to that question".\n                    AI assistant will not apologize for previous responses, but instead will indicated new information was gained.\n                    AI assistant will not invent anything that is not drawn directly from the context.\n                    '}, {'role': 'user', 'content': 'Summarize the contents of this document'}], 'model': 'gpt-3.5-turbo'}}
2023-12-12 15:48:23,693 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-12 15:48:23,707 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x11142ae10>
2023-12-12 15:48:23,707 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x111350e60> server_hostname='api.openai.com' timeout=5.0
2023-12-12 15:48:23,724 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x11142afd0>
2023-12-12 15:48:23,724 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:48:23,724 [DEBUG]: send_request_headers.complete
2023-12-12 15:48:23,725 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:48:23,725 [DEBUG]: send_request_body.complete
2023-12-12 15:48:23,725 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:48:26,163 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:48:26 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'2309'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'160000'), (b'x-ratelimit-limit-tokens_usage_based', b'160000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'158906'), (b'x-ratelimit-remaining-tokens_usage_based', b'158906'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'410ms'), (b'x-ratelimit-reset-tokens_usage_based', b'410ms'), (b'x-request-id', b'7f53b30a443399e33ae817c8c89285a3'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=RBwQoiHImY8NHBxmAlYPUjze5NZunWyZLZoRC0CX6s4-1702414106-1-ASjtV8P2JWV3cYqzhY82NG2/mN3ffrgfMDM043rKAxnw/fGh4CHOM3JEUFprr2lNQYjk4gJTaU1uBk1J50WuLUQ=; path=/; expires=Tue, 12-Dec-23 21:18:26 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=CwIq3lYB11SyHKvF4bY.8qYInaqEI2zPvVJBVyG9dbo-1702414106215-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d3f4c81f41fb-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:48:26,164 [INFO]: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2023-12-12 15:48:26,164 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:48:26,165 [DEBUG]: receive_response_body.complete
2023-12-12 15:48:26,165 [DEBUG]: response_closed.started
2023-12-12 15:48:26,165 [DEBUG]: response_closed.complete
2023-12-12 15:48:26,165 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2023-12-12 15:48:26,167 [INFO]: 127.0.0.1 - - [12/Dec/2023 15:48:26] "POST /api/chat HTTP/1.1" 200 -
2023-12-12 15:48:26,174 [DEBUG]: Request received for answer evaluation
2023-12-12 15:48:26,176 [DEBUG]: Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'text-davinci-003', 'prompt': 'Please rate the faithfulness of the answers to their contexts on a scale from 0 to 1, where 0 is completely unfaithful and 1 is completely faithful.\nExample 1:\nContext: The sky is blue.\nAnswer: The sky is blue.\nRating: 1\nExample 2:\nContext: The sky is blue.\nAnswer: The sky is often cloudy.\nRating: 0.5\nExample 3:\nContext: Apples are usually red.\nAnswer: Oranges are orange.\nRating: 0\nYour Turn:\nContext: 2-ThisisasupercoolideaandIlikehowitclearlyalignswithyourinterests!Ijustwonder1)ifyoucanmodifytheoutputtomakeitalittlebitmorereadable,maybeplaywithASCIIcharactersandmakeitmoregame-like?and2)thereasoningbehinddesignchoicesyoumake,footballisobviouslysupercomplexandIâ€™massumingyouâ€™resimplifyingitforyourgame,whatandwhyareyouchoosingtoleaveout?Itseemslikeyouâ€™reofftoagoodstartthough!\n4-Again,lovetheideaandcandeï¬nitelyseethepassionbehindit!100+coldemailsiscrazyandIreallyrespecttheamountofworkthatâ€™salreadygoneintothisproject.Ijustwouldlovetoknowmoredetailsabouttheregressionmodeling/transferlearningpartofthisproject\n6-Thisissuperinteresting,Iâ€™veneverreallythoughtabouthowcomplicatedadishwashinggamecouldbe,butIhonestlylovehowout-of-the-boxthisisandhowyouâ€™veputatonofthoughtintoit.Mymainsuggestionwouldbetojustaddmoretexttotheslidesâ€”Iknowthattheuseofvisualsisintentionalbutfororganizationalpurposes,itcouldbegoodtostartwithatextoverviewandsupplementvisualswithcaptions.\n8-CouldbebecauseImissedtheproposal(andmaybetherewasalongeroverviewthere),butIwouldâ€™veappreciatedalongerhigh-leveloverview/statementofpurpose.However,Ireallyrespecthowtechnicalthisprojectisandhowitâ€™sdeï¬nitelystronglyrootedintheory\n10-Ilikethebackgroundandoverview,eventhoughIâ€™mnotfamiliarwiththetopic,theslidesreallyhelpillustratethepurposeoftheproject.Myonlysuggestionwouldbetocutdownsomeofthetextontheslides.\nment data because its authors defined the exact bitrate alloca-\ntion scheme and conducted the evaluations themselves.\n5 Discussion\nA few major takeaways can be gleaned from these results,\nmainly the applicability of pre-trained models in computer\nvision and limitations and gaps in the field of 360-degree\nlivestreaming.\nThis discussion aims to provide explanations and theories\nfor notable areas in the results, as well as guide future work\nand ideas for exploration.\n6\nCPSC 488/588 - AI Foundation Models HW 3 Page 3\nproposed system level method to improve the efficiency of inference in Transformers. We did not cover\nthis in class. Please read the paper and answer the following questions.\n(a) (5 points) Summarize how the proposed approach works on high-level (no more than 5 sentences).\nDo you think such approach can be combined with FlashAttention?\nSolution:\n5-sentence summary: Prompt Cache reduces the computational overhead of, and thus acceler-\nates, LLM inference by reusing attention states from different input prompts and supporting\nmodularity to maximize reuse. It takes advantage of the fact that prompts often have signifi-\ncant text overlap, sharing frequently occurring phrases/segments of text (e.g. system messages,\ndomain-specific documents, templates). Prompt Cache pre-computes the attention states of\nthese frequent text segments as prompt modules and stores them in a modular key-value cache,\nso that when an LLM encounters a frequent text segment, it does not need to re-compute full\nattention for its tokens. It requires users to write prompts in Prompt Markup Language (PML),\nin which they use\nAnswer: This document discusses the results of a study on the applicability of pre-trained models in computer vision and limitations in the field of 360-degree livestreaming. It also presents a proposed method called Prompt Cache for improving the efficiency of inference in Transformers. The method involves reusing attention states from different input prompts and storing frequently occurring text segments in a modular cache. The document concludes with a discussion on the implications of the results and suggestions for future research in these areas.\nRating:', 'max_tokens': 60}}
2023-12-12 15:48:26,177 [DEBUG]: close.started
2023-12-12 15:48:26,177 [DEBUG]: close.complete
2023-12-12 15:48:26,177 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-12 15:48:26,205 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1114102d0>
2023-12-12 15:48:26,206 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x108e65370> server_hostname='api.openai.com' timeout=5.0
2023-12-12 15:48:26,240 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1113ea710>
2023-12-12 15:48:26,240 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:48:26,240 [DEBUG]: send_request_headers.complete
2023-12-12 15:48:26,240 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:48:26,240 [DEBUG]: send_request_body.complete
2023-12-12 15:48:26,241 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:48:26,853 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:48:26 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'text-davinci-003'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'222'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-limit-tokens_usage_based', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'249939'), (b'x-ratelimit-remaining-tokens_usage_based', b'249939'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'14ms'), (b'x-ratelimit-reset-tokens_usage_based', b'14ms'), (b'x-request-id', b'4fd48d91d633390bdd605b33383a66a6'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d404788443ef-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:48:26,853 [INFO]: HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2023-12-12 15:48:26,853 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:48:26,854 [DEBUG]: receive_response_body.complete
2023-12-12 15:48:26,854 [DEBUG]: response_closed.started
2023-12-12 15:48:26,854 [DEBUG]: response_closed.complete
2023-12-12 15:48:26,854 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2023-12-12 15:48:26,856 [DEBUG]: Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'text-davinci-003', 'prompt': 'Please rate the context relevance of the answers on a scale from 0 to 1, where 0 is not relevant and 1 is highly relevant.\nExample 1:\nContext: History of the Roman Empire.\nAnswer: Julius Caesar was a Roman ruler.\nRating: 1\nExample 2:\nContext: History of the Roman Empire.\nAnswer: The Roman Empire was in Africa.\nRating: 0.5\nExample 3:\nContext: Computer Programming Basics.\nAnswer: Apples grow on trees.\nRating: 0\nYour Turn:\nContext: 2-ThisisasupercoolideaandIlikehowitclearlyalignswithyourinterests!Ijustwonder1)ifyoucanmodifytheoutputtomakeitalittlebitmorereadable,maybeplaywithASCIIcharactersandmakeitmoregame-like?and2)thereasoningbehinddesignchoicesyoumake,footballisobviouslysupercomplexandIâ€™massumingyouâ€™resimplifyingitforyourgame,whatandwhyareyouchoosingtoleaveout?Itseemslikeyouâ€™reofftoagoodstartthough!\n4-Again,lovetheideaandcandeï¬nitelyseethepassionbehindit!100+coldemailsiscrazyandIreallyrespecttheamountofworkthatâ€™salreadygoneintothisproject.Ijustwouldlovetoknowmoredetailsabouttheregressionmodeling/transferlearningpartofthisproject\n6-Thisissuperinteresting,Iâ€™veneverreallythoughtabouthowcomplicatedadishwashinggamecouldbe,butIhonestlylovehowout-of-the-boxthisisandhowyouâ€™veputatonofthoughtintoit.Mymainsuggestionwouldbetojustaddmoretexttotheslidesâ€”Iknowthattheuseofvisualsisintentionalbutfororganizationalpurposes,itcouldbegoodtostartwithatextoverviewandsupplementvisualswithcaptions.\n8-CouldbebecauseImissedtheproposal(andmaybetherewasalongeroverviewthere),butIwouldâ€™veappreciatedalongerhigh-leveloverview/statementofpurpose.However,Ireallyrespecthowtechnicalthisprojectisandhowitâ€™sdeï¬nitelystronglyrootedintheory\n10-Ilikethebackgroundandoverview,eventhoughIâ€™mnotfamiliarwiththetopic,theslidesreallyhelpillustratethepurposeoftheproject.Myonlysuggestionwouldbetocutdownsomeofthetextontheslides.\nment data because its authors defined the exact bitrate alloca-\ntion scheme and conducted the evaluations themselves.\n5 Discussion\nA few major takeaways can be gleaned from these results,\nmainly the applicability of pre-trained models in computer\nvision and limitations and gaps in the field of 360-degree\nlivestreaming.\nThis discussion aims to provide explanations and theories\nfor notable areas in the results, as well as guide future work\nand ideas for exploration.\n6\nCPSC 488/588 - AI Foundation Models HW 3 Page 3\nproposed system level method to improve the efficiency of inference in Transformers. We did not cover\nthis in class. Please read the paper and answer the following questions.\n(a) (5 points) Summarize how the proposed approach works on high-level (no more than 5 sentences).\nDo you think such approach can be combined with FlashAttention?\nSolution:\n5-sentence summary: Prompt Cache reduces the computational overhead of, and thus acceler-\nates, LLM inference by reusing attention states from different input prompts and supporting\nmodularity to maximize reuse. It takes advantage of the fact that prompts often have signifi-\ncant text overlap, sharing frequently occurring phrases/segments of text (e.g. system messages,\ndomain-specific documents, templates). Prompt Cache pre-computes the attention states of\nthese frequent text segments as prompt modules and stores them in a modular key-value cache,\nso that when an LLM encounters a frequent text segment, it does not need to re-compute full\nattention for its tokens. It requires users to write prompts in Prompt Markup Language (PML),\nin which they use\nAnswer: This document discusses the results of a study on the applicability of pre-trained models in computer vision and limitations in the field of 360-degree livestreaming. It also presents a proposed method called Prompt Cache for improving the efficiency of inference in Transformers. The method involves reusing attention states from different input prompts and storing frequently occurring text segments in a modular cache. The document concludes with a discussion on the implications of the results and suggestions for future research in these areas.\nRating:', 'max_tokens': 60}}
2023-12-12 15:48:26,857 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:48:26,858 [DEBUG]: send_request_headers.complete
2023-12-12 15:48:26,858 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:48:26,858 [DEBUG]: send_request_body.complete
2023-12-12 15:48:26,858 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:48:27,368 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:48:27 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'text-davinci-003'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'216'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-limit-tokens_usage_based', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'249940'), (b'x-ratelimit-remaining-tokens_usage_based', b'249839'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'14ms'), (b'x-ratelimit-reset-tokens_usage_based', b'38ms'), (b'x-request-id', b'd8389b565a615bdf7add2fa628d4024f'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d4086dea43ef-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:48:27,368 [INFO]: HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2023-12-12 15:48:27,369 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:48:27,369 [DEBUG]: receive_response_body.complete
2023-12-12 15:48:27,369 [DEBUG]: response_closed.started
2023-12-12 15:48:27,369 [DEBUG]: response_closed.complete
2023-12-12 15:48:27,369 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2023-12-12 15:48:27,371 [DEBUG]: Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'text-davinci-003', 'prompt': 'Please rate the relevance of the answers to the questions on a scale from 0 to 1, where 0 is not relevant and 1 is highly relevant.\nExample 1:\nQuestion: What is the capital of France?\nAnswer: Paris is the capital of France.\nRating: 1\nExample 2:\nQuestion: What is the capital of France?\nAnswer: France is in Europe.\nRating: 0.5\nExample 3:\nQuestion: What is the capital of France?\nAnswer: Elephants are the largest land animals.\nRating: 0\nYour Turn:\nQuestion: Summarize the contents of this document\nAnswer: This document discusses the results of a study on the applicability of pre-trained models in computer vision and limitations in the field of 360-degree livestreaming. It also presents a proposed method called Prompt Cache for improving the efficiency of inference in Transformers. The method involves reusing attention states from different input prompts and storing frequently occurring text segments in a modular cache. The document concludes with a discussion on the implications of the results and suggestions for future research in these areas.\nRating:', 'max_tokens': 60}}
2023-12-12 15:48:27,372 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:48:27,372 [DEBUG]: send_request_headers.complete
2023-12-12 15:48:27,373 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:48:27,373 [DEBUG]: send_request_body.complete
2023-12-12 15:48:27,373 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:48:27,704 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:48:27 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'text-davinci-003'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'127'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-limit-tokens_usage_based', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'249939'), (b'x-ratelimit-remaining-tokens_usage_based', b'249770'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'14ms'), (b'x-ratelimit-reset-tokens_usage_based', b'55ms'), (b'x-request-id', b'cb02b7182522dc7f48dc5450e989b24f'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d40b8acf43ef-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:48:27,705 [INFO]: HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2023-12-12 15:48:27,705 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:48:27,706 [DEBUG]: receive_response_body.complete
2023-12-12 15:48:27,706 [DEBUG]: response_closed.started
2023-12-12 15:48:27,706 [DEBUG]: response_closed.complete
2023-12-12 15:48:27,706 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2023-12-12 15:48:27,707 [INFO]: 127.0.0.1 - - [12/Dec/2023 15:48:27] "POST /api/evaluate-answers HTTP/1.1" 200 -
2023-12-12 15:50:30,743 [DEBUG]: Using selector: KqueueSelector
2023-12-12 15:50:30,744 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-12 15:50:30,745 [DEBUG]: load_verify_locations cafile='/Users/aliceao/opt/miniconda3/envs/chat-lore/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-12 15:50:30,788 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1114c9580>, 'json_data': {'input': 'what were the three models compared in the study', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:50:30,789 [DEBUG]: close.started
2023-12-12 15:50:30,789 [DEBUG]: close.complete
2023-12-12 15:50:30,789 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-12 15:50:30,898 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x11140ed90>
2023-12-12 15:50:30,898 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x111350b00> server_hostname='api.openai.com' timeout=5.0
2023-12-12 15:50:30,912 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x11140d290>
2023-12-12 15:50:30,913 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:50:30,913 [DEBUG]: send_request_headers.complete
2023-12-12 15:50:30,913 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:50:30,913 [DEBUG]: send_request_body.complete
2023-12-12 15:50:30,914 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:50:31,045 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:50:31 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'22'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999987'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'402d31da3e718f1dab257e5c33d2d105'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d70fb9414356-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:50:31,046 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:50:31,046 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:50:31,047 [DEBUG]: receive_response_body.complete
2023-12-12 15:50:31,047 [DEBUG]: response_closed.started
2023-12-12 15:50:31,047 [DEBUG]: response_closed.complete
2023-12-12 15:50:31,047 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:50:31,680 [DEBUG]: Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'AI assistant is a brand new, powerful, human-like artificial intelligence.\n                    The traits of AI include expert knowledge, helpfulness, cleverness, and articulateness.\n                    AI is a well-behaved and well-mannered individual.\n                    AI is always friendly, kind, and inspiring, and he is eager to provide vivid and thoughtful responses to the user.\n                    AI has the sum of all knowledge in their brain, and is able to accurately answer nearly any question about any topic in conversation.\n                    AI assistant is a big fan of Pinecone and Vercel.\n                    START CONTEXT BLOCK\n                    edge cases/unexpected behavior, etc. had to be inferred from\nreading the papers and the code.\nFinally, some of the datasets used were not publicly avail-\nable â€“ access had to be requested for them, either by com-\npleting a form [2] or writing an email to the study authors.\nMany of the modelsâ€™ results could not be easily reproduced, as\ntheir training and evaluation datasets were all private and/or\ndeleted [3]. Therefore, the first part of the project was not\nto reproduce the three selected modelsâ€™ results but to gener-\nate new saliency maps on a different dataset and informally\ncompare their results.\n4 Implementation\n4.1 Saliency Maps\n4.1.1 Generation\nAfter all three model environments were set up, the pre-trained\nmodels were run on the Wild360 dataset (described in further\ndetail below). ffmpeg was used to extract frames from each\nof the 360-degree videos with a frame rate of 4fps. For each\nvideo frame, the models treated them as unique images and\ngenerated a binary saliency map for them. (For the actual\nadaptive bitrate allocation, a different dataset [14] was used,\nFigure 3: A colored heatmap from the Wild360 dataset (top)\nand black-and-white saliency maps (bottom)\nand saliency maps were generated for all of the videos in that\ndataset.)\n4.1.2 Informal Evaluation\nBefore implementing the adaptive bitrate allocation portion\nof the project, the three modelsâ€™ saliency detection abili-\nties were compared using the Wild-360 dataset. The dataset\nages. The model is trained using stochastic gradient de-\n2\ntransformer-inspired architectures for the application of live\n360-degree video. Therefore, this project focuses on three\nsaliency map models, with varying degrees of similarity to\na transformer-based architecture, and evaluates their mapsâ€™\nperformance in viewport prediction and adaptive bitrate allo-\ncation.\n2 Methodology\nThere are three main parts to this project: 1) identifying dif-\nferent ML saliency detection models, with different levels\nof similarity to transformers, and 2) constructing a bitrate\nallocation scheme and 3) evaluating their results with metrics\ndefined in [3]. The workflow of the first two parts is illustrated\nin Figure 1.\n3 Preliminary Work\n3.1 Selecting Machine Learning Techniques\nSimilar to previous works [11], this paper identifies and com-\npares state-of-the-art machine learning techniques on view-\nport prediction. All of the selected techniques focus on build-\ningsaliency maps , which identify "salient" areas of each\nframe that users are likely to focus on.\nThe final three selected machine-learning techniques were\nas follows:\n1.PanoSalNet (published in MMSys â€™18): a deep CNN-\nbased architecture specifically for generating saliency\nmaps of 360-degree videos, with optional integration of\nhead-tracking history. The architecture for the saliency\nmap model is based on Deep Convnet, a state-of-the-\nart DCNN for 2D images, and transfer learning is used\nto adapt a pre-trained DCNN model to 360-degree im-\n                    END OF CONTEXT BLOCK\n                    AI assistant will take into account any CONTEXT BLOCK that is provided in a conversation.\n                    If the context does not provide the answer to question, the AI assistant will say, "I\'m sorry, but I don\'t know the answer to that question".\n                    AI assistant will not apologize for previous responses, but instead will indicated new information was gained.\n                    AI assistant will not invent anything that is not drawn directly from the context.\n                    '}, {'role': 'user', 'content': 'what were the three models compared in the study'}], 'model': 'gpt-3.5-turbo'}}
2023-12-12 15:50:31,681 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-12 15:50:31,691 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1114d4e10>
2023-12-12 15:50:31,691 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x111350290> server_hostname='api.openai.com' timeout=5.0
2023-12-12 15:50:31,706 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1114d4ed0>
2023-12-12 15:50:31,706 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:50:31,706 [DEBUG]: send_request_headers.complete
2023-12-12 15:50:31,706 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:50:31,707 [DEBUG]: send_request_body.complete
2023-12-12 15:50:31,707 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:50:33,432 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:50:33 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'1594'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'160000'), (b'x-ratelimit-limit-tokens_usage_based', b'160000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'158914'), (b'x-ratelimit-remaining-tokens_usage_based', b'158914'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'407ms'), (b'x-ratelimit-reset-tokens_usage_based', b'407ms'), (b'x-request-id', b'7492a35a0a68876f0be3ea85620deca0'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=ik2q7kNZKvEpHsQF.tBeImemp6fuNdaVLXCCUDnUlkA-1702414233-1-AZY7BDsH3Bggac9YdTlDnw8qXiibPJrHkJAEsM/ASlUJ8koxkAutBAoC1tNAptMLdqwFKmr71ra2DOEUeGzSWZM=; path=/; expires=Tue, 12-Dec-23 21:20:33 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=DgBizeQhjbGOM3GfVUi2cqSuxi1pWodTIC0mHkzewLU-1702414233500-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d714aed9178c-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:50:33,433 [INFO]: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2023-12-12 15:50:33,433 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:50:33,434 [DEBUG]: receive_response_body.complete
2023-12-12 15:50:33,434 [DEBUG]: response_closed.started
2023-12-12 15:50:33,434 [DEBUG]: response_closed.complete
2023-12-12 15:50:33,434 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2023-12-12 15:50:33,436 [INFO]: 127.0.0.1 - - [12/Dec/2023 15:50:33] "POST /api/chat HTTP/1.1" 200 -
2023-12-12 15:50:33,444 [DEBUG]: Request received for answer evaluation
2023-12-12 15:50:33,446 [DEBUG]: Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'text-davinci-003', 'prompt': 'Please rate the faithfulness of the answers to their contexts on a scale from 0 to 1, where 0 is completely unfaithful and 1 is completely faithful.\nExample 1:\nContext: The sky is blue.\nAnswer: The sky is blue.\nRating: 1\nExample 2:\nContext: The sky is blue.\nAnswer: The sky is often cloudy.\nRating: 0.5\nExample 3:\nContext: Apples are usually red.\nAnswer: Oranges are orange.\nRating: 0\nYour Turn:\nContext: edge cases/unexpected behavior, etc. had to be inferred from\nreading the papers and the code.\nFinally, some of the datasets used were not publicly avail-\nable â€“ access had to be requested for them, either by com-\npleting a form [2] or writing an email to the study authors.\nMany of the modelsâ€™ results could not be easily reproduced, as\ntheir training and evaluation datasets were all private and/or\ndeleted [3]. Therefore, the first part of the project was not\nto reproduce the three selected modelsâ€™ results but to gener-\nate new saliency maps on a different dataset and informally\ncompare their results.\n4 Implementation\n4.1 Saliency Maps\n4.1.1 Generation\nAfter all three model environments were set up, the pre-trained\nmodels were run on the Wild360 dataset (described in further\ndetail below). ffmpeg was used to extract frames from each\nof the 360-degree videos with a frame rate of 4fps. For each\nvideo frame, the models treated them as unique images and\ngenerated a binary saliency map for them. (For the actual\nadaptive bitrate allocation, a different dataset [14] was used,\nFigure 3: A colored heatmap from the Wild360 dataset (top)\nand black-and-white saliency maps (bottom)\nand saliency maps were generated for all of the videos in that\ndataset.)\n4.1.2 Informal Evaluation\nBefore implementing the adaptive bitrate allocation portion\nof the project, the three modelsâ€™ saliency detection abili-\nties were compared using the Wild-360 dataset. The dataset\nages. The model is trained using stochastic gradient de-\n2\ntransformer-inspired architectures for the application of live\n360-degree video. Therefore, this project focuses on three\nsaliency map models, with varying degrees of similarity to\na transformer-based architecture, and evaluates their mapsâ€™\nperformance in viewport prediction and adaptive bitrate allo-\ncation.\n2 Methodology\nThere are three main parts to this project: 1) identifying dif-\nferent ML saliency detection models, with different levels\nof similarity to transformers, and 2) constructing a bitrate\nallocation scheme and 3) evaluating their results with metrics\ndefined in [3]. The workflow of the first two parts is illustrated\nin Figure 1.\n3 Preliminary Work\n3.1 Selecting Machine Learning Techniques\nSimilar to previous works [11], this paper identifies and com-\npares state-of-the-art machine learning techniques on view-\nport prediction. All of the selected techniques focus on build-\ningsaliency maps , which identify "salient" areas of each\nframe that users are likely to focus on.\nThe final three selected machine-learning techniques were\nas follows:\n1.PanoSalNet (published in MMSys â€™18): a deep CNN-\nbased architecture specifically for generating saliency\nmaps of 360-degree videos, with optional integration of\nhead-tracking history. The architecture for the saliency\nmap model is based on Deep Convnet, a state-of-the-\nart DCNN for 2D images, and transfer learning is used\nto adapt a pre-trained DCNN model to 360-degree im-\nAnswer: The three models compared in the study were PanoSalNet, DeepGaze360, and NRPano. These models were selected for their ability to generate saliency maps for 360-degree videos, which identify the "salient" areas that users are likely to focus on.\nRating:', 'max_tokens': 60}}
2023-12-12 15:50:33,447 [DEBUG]: close.started
2023-12-12 15:50:33,447 [DEBUG]: close.complete
2023-12-12 15:50:33,448 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-12 15:50:33,458 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1113e8950>
2023-12-12 15:50:33,458 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x108e65370> server_hostname='api.openai.com' timeout=5.0
2023-12-12 15:50:33,472 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1114102d0>
2023-12-12 15:50:33,472 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:50:33,472 [DEBUG]: send_request_headers.complete
2023-12-12 15:50:33,472 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:50:33,473 [DEBUG]: send_request_body.complete
2023-12-12 15:50:33,473 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:50:33,802 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:50:33 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'text-davinci-003'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'202'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-limit-tokens_usage_based', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'249940'), (b'x-ratelimit-remaining-tokens_usage_based', b'249940'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'14ms'), (b'x-ratelimit-reset-tokens_usage_based', b'14ms'), (b'x-request-id', b'49fff466d6d1d28a502e0fb815e54653'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d71fbba21906-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:50:33,803 [INFO]: HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2023-12-12 15:50:33,803 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:50:33,804 [DEBUG]: receive_response_body.complete
2023-12-12 15:50:33,804 [DEBUG]: response_closed.started
2023-12-12 15:50:33,804 [DEBUG]: response_closed.complete
2023-12-12 15:50:33,804 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2023-12-12 15:50:33,806 [DEBUG]: Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'text-davinci-003', 'prompt': 'Please rate the context relevance of the answers on a scale from 0 to 1, where 0 is not relevant and 1 is highly relevant.\nExample 1:\nContext: History of the Roman Empire.\nAnswer: Julius Caesar was a Roman ruler.\nRating: 1\nExample 2:\nContext: History of the Roman Empire.\nAnswer: The Roman Empire was in Africa.\nRating: 0.5\nExample 3:\nContext: Computer Programming Basics.\nAnswer: Apples grow on trees.\nRating: 0\nYour Turn:\nContext: edge cases/unexpected behavior, etc. had to be inferred from\nreading the papers and the code.\nFinally, some of the datasets used were not publicly avail-\nable â€“ access had to be requested for them, either by com-\npleting a form [2] or writing an email to the study authors.\nMany of the modelsâ€™ results could not be easily reproduced, as\ntheir training and evaluation datasets were all private and/or\ndeleted [3]. Therefore, the first part of the project was not\nto reproduce the three selected modelsâ€™ results but to gener-\nate new saliency maps on a different dataset and informally\ncompare their results.\n4 Implementation\n4.1 Saliency Maps\n4.1.1 Generation\nAfter all three model environments were set up, the pre-trained\nmodels were run on the Wild360 dataset (described in further\ndetail below). ffmpeg was used to extract frames from each\nof the 360-degree videos with a frame rate of 4fps. For each\nvideo frame, the models treated them as unique images and\ngenerated a binary saliency map for them. (For the actual\nadaptive bitrate allocation, a different dataset [14] was used,\nFigure 3: A colored heatmap from the Wild360 dataset (top)\nand black-and-white saliency maps (bottom)\nand saliency maps were generated for all of the videos in that\ndataset.)\n4.1.2 Informal Evaluation\nBefore implementing the adaptive bitrate allocation portion\nof the project, the three modelsâ€™ saliency detection abili-\nties were compared using the Wild-360 dataset. The dataset\nages. The model is trained using stochastic gradient de-\n2\ntransformer-inspired architectures for the application of live\n360-degree video. Therefore, this project focuses on three\nsaliency map models, with varying degrees of similarity to\na transformer-based architecture, and evaluates their mapsâ€™\nperformance in viewport prediction and adaptive bitrate allo-\ncation.\n2 Methodology\nThere are three main parts to this project: 1) identifying dif-\nferent ML saliency detection models, with different levels\nof similarity to transformers, and 2) constructing a bitrate\nallocation scheme and 3) evaluating their results with metrics\ndefined in [3]. The workflow of the first two parts is illustrated\nin Figure 1.\n3 Preliminary Work\n3.1 Selecting Machine Learning Techniques\nSimilar to previous works [11], this paper identifies and com-\npares state-of-the-art machine learning techniques on view-\nport prediction. All of the selected techniques focus on build-\ningsaliency maps , which identify "salient" areas of each\nframe that users are likely to focus on.\nThe final three selected machine-learning techniques were\nas follows:\n1.PanoSalNet (published in MMSys â€™18): a deep CNN-\nbased architecture specifically for generating saliency\nmaps of 360-degree videos, with optional integration of\nhead-tracking history. The architecture for the saliency\nmap model is based on Deep Convnet, a state-of-the-\nart DCNN for 2D images, and transfer learning is used\nto adapt a pre-trained DCNN model to 360-degree im-\nAnswer: The three models compared in the study were PanoSalNet, DeepGaze360, and NRPano. These models were selected for their ability to generate saliency maps for 360-degree videos, which identify the "salient" areas that users are likely to focus on.\nRating:', 'max_tokens': 60}}
2023-12-12 15:50:33,807 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:50:33,807 [DEBUG]: send_request_headers.complete
2023-12-12 15:50:33,807 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:50:33,835 [DEBUG]: send_request_body.complete
2023-12-12 15:50:33,835 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:50:34,473 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:50:34 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'text-davinci-003'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'179'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-limit-tokens_usage_based', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'249940'), (b'x-ratelimit-remaining-tokens_usage_based', b'249940'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'14ms'), (b'x-ratelimit-reset-tokens_usage_based', b'14ms'), (b'x-request-id', b'54532b4a7373064eb5364781ecd99fcf'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d721cdd51906-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:50:34,473 [INFO]: HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2023-12-12 15:50:34,474 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:50:34,474 [DEBUG]: receive_response_body.complete
2023-12-12 15:50:34,474 [DEBUG]: response_closed.started
2023-12-12 15:50:34,474 [DEBUG]: response_closed.complete
2023-12-12 15:50:34,474 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2023-12-12 15:50:34,476 [DEBUG]: Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'text-davinci-003', 'prompt': 'Please rate the relevance of the answers to the questions on a scale from 0 to 1, where 0 is not relevant and 1 is highly relevant.\nExample 1:\nQuestion: What is the capital of France?\nAnswer: Paris is the capital of France.\nRating: 1\nExample 2:\nQuestion: What is the capital of France?\nAnswer: France is in Europe.\nRating: 0.5\nExample 3:\nQuestion: What is the capital of France?\nAnswer: Elephants are the largest land animals.\nRating: 0\nYour Turn:\nQuestion: what were the three models compared in the study\nAnswer: The three models compared in the study were PanoSalNet, DeepGaze360, and NRPano. These models were selected for their ability to generate saliency maps for 360-degree videos, which identify the "salient" areas that users are likely to focus on.\nRating:', 'max_tokens': 60}}
2023-12-12 15:50:34,477 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:50:34,478 [DEBUG]: send_request_headers.complete
2023-12-12 15:50:34,478 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:50:34,478 [DEBUG]: send_request_body.complete
2023-12-12 15:50:34,478 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:50:35,086 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:50:34 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'text-davinci-003'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'117'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-limit-tokens_usage_based', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'249940'), (b'x-ratelimit-remaining-tokens_usage_based', b'249940'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'14ms'), (b'x-ratelimit-reset-tokens_usage_based', b'14ms'), (b'x-request-id', b'441f564188bd985637de963adcbd8948'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d725fa2d1906-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:50:35,086 [INFO]: HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2023-12-12 15:50:35,087 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:50:35,087 [DEBUG]: receive_response_body.complete
2023-12-12 15:50:35,087 [DEBUG]: response_closed.started
2023-12-12 15:50:35,087 [DEBUG]: response_closed.complete
2023-12-12 15:50:35,088 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2023-12-12 15:50:35,088 [INFO]: 127.0.0.1 - - [12/Dec/2023 15:50:35] "POST /api/evaluate-answers HTTP/1.1" 200 -
2023-12-12 15:51:06,396 [DEBUG]: Using selector: KqueueSelector
2023-12-12 15:51:06,397 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-12 15:51:06,398 [DEBUG]: load_verify_locations cafile='/Users/aliceao/opt/miniconda3/envs/chat-lore/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-12 15:51:06,441 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x111418720>, 'json_data': {'input': 'why was VST selected as a model for the study', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:51:06,441 [DEBUG]: close.started
2023-12-12 15:51:06,442 [DEBUG]: close.complete
2023-12-12 15:51:06,442 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-12 15:51:06,453 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1113ee250>
2023-12-12 15:51:06,453 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x111350b00> server_hostname='api.openai.com' timeout=5.0
2023-12-12 15:51:06,468 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1113ec310>
2023-12-12 15:51:06,468 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:51:06,469 [DEBUG]: send_request_headers.complete
2023-12-12 15:51:06,469 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:51:06,469 [DEBUG]: send_request_body.complete
2023-12-12 15:51:06,469 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:51:06,829 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:51:06 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'140'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999988'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'd93514ee0f4dec6133f2fc82b572549b'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d7edfb514267-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:51:06,830 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:51:06,830 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:51:06,862 [DEBUG]: receive_response_body.complete
2023-12-12 15:51:06,862 [DEBUG]: response_closed.started
2023-12-12 15:51:06,862 [DEBUG]: response_closed.complete
2023-12-12 15:51:06,863 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:51:07,434 [DEBUG]: Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'AI assistant is a brand new, powerful, human-like artificial intelligence.\n                    The traits of AI include expert knowledge, helpfulness, cleverness, and articulateness.\n                    AI is a well-behaved and well-mannered individual.\n                    AI is always friendly, kind, and inspiring, and he is eager to provide vivid and thoughtful responses to the user.\n                    AI has the sum of all knowledge in their brain, and is able to accurately answer nearly any question about any topic in conversation.\n                    AI assistant is a big fan of Pinecone and Vercel.\n                    START CONTEXT BLOCK\n                    Figure 10: Frame from Topic 6 (top) and generated saliency\nmaps from VST (bottom left) and TranSalNet (bottom right)\n5.2 Other Takeaways\nOne interesting takeaway from the results is that VST per-\nformed especially well on one specific video, Topic 6, which is\nshown in Figure 10. Intuitively, one would expect the saliency\nmaps from TranSalNet to be better representations of where\nusers are likely to gaze, as they identify the figures, tables,\nand middle of the scene as the more salient areas of the frame.\nVST, on the other hand, seems to mark the distorted wooden\nbeams at the very top of the frame, as salient.\nHowever, VST achieves far higher average viewport quality\nthan TranSalNet, as shown in Figure 7. Video 6 is notable\nin that it is one of the few videos in the dataset that takes\nplace indoors and has extremely noticeable ceiling distortions.\nTherefore, it is possible that VST is better at generating pre-\ndictions for distorted indoor videos, and perhaps users do tend\nto look at higher areas in these videos and find them more\nsalient. However, more sample videos would be needed to\nconclude that VST is significantly better at these videos.\nAnother important note is that given the relative speed of\nthe saliency map generation and adaptive bitrate allocation, as\nwell as how the model would be pre-trained and ready to make\ninferences, these techniques would be feasible in livestream-\ning applications. For instance, these processes typically take\nsampling method, and a special decoder for saliency and\nboundary detection. VST outperforms state-of-the-art\nCNN salient object detection models [6].\nThe criteria for selecting each technique were as follows:\n1.Recency : Because of how rapidly the field of machine\nlearning is evolving, this paper focuses on comparing\nstate-of-the-art ML models published at most five years\nago, and ideally even later.\n(a)PanoSalNet : As the oldest model, PanoSalNet [10],\nwas first published in 2018, but it has been cited\nover 80 times, and the author has used the model in\nan April 2023 paper [9]. Therefore, although it pre-\ndates state-of-the-art techniques like transformers,\nit continues to have relevancy today.\n(b)TranSalNet : Published in just 2022, TranSalNet is\nfairly recent and also leverages novel mechanisms\nlike multi-head self-attention.\n(c)Visual Saliency Transformer : Similarly, VST is\nalso recent, with a publication year of 2021.\n2.Available source code and datasets : Many of the recent\nliterature on machine learning approaches to saliency\nmap creation and viewport prediction do not have\npublicly available source code [1]. Other papers may\nhave public repositories but with missing, private-access\ndatasets or incomplete code that prevents the models\nfrom running. This project considered and tested five\nother alternative codebases but ultimately could not use\nthem due to their machine incompatibility or lack of\nmaintenance.\n3\nFigure 8: Manhattan vs. Euclidean distance [8]\nVST TranSalNet\n0 5.53 7.18\n1 6.71 7.55\n2 \n                    END OF CONTEXT BLOCK\n                    AI assistant will take into account any CONTEXT BLOCK that is provided in a conversation.\n                    If the context does not provide the answer to question, the AI assistant will say, "I\'m sorry, but I don\'t know the answer to that question".\n                    AI assistant will not apologize for previous responses, but instead will indicated new information was gained.\n                    AI assistant will not invent anything that is not drawn directly from the context.\n                    '}, {'role': 'user', 'content': 'why was VST selected as a model for the study'}], 'model': 'gpt-3.5-turbo'}}
2023-12-12 15:51:07,435 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-12 15:51:07,444 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x111405750>
2023-12-12 15:51:07,445 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x111351490> server_hostname='api.openai.com' timeout=5.0
2023-12-12 15:51:07,456 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x111407e90>
2023-12-12 15:51:07,456 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:51:07,456 [DEBUG]: send_request_headers.complete
2023-12-12 15:51:07,456 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:51:07,457 [DEBUG]: send_request_body.complete
2023-12-12 15:51:07,457 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:51:12,945 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:51:13 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'5268'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'160000'), (b'x-ratelimit-limit-tokens_usage_based', b'160000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'158910'), (b'x-ratelimit-remaining-tokens_usage_based', b'158910'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'408ms'), (b'x-ratelimit-reset-tokens_usage_based', b'408ms'), (b'x-request-id', b'faea2c219d4967e42840bf605d7b9c40'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=mbOaS4q47xxCxzONpMRqia1hi2jTJF_5s3Mye8H2wzM-1702414273-1-AUzAIOsH0q5rI9VfEf+Pk9TMgsmRGyjWWEOd2bExkBsl4HoKsPto5aAdf+kYSupDfv6TvcoTc6KOD3UWMq2IXpo=; path=/; expires=Tue, 12-Dec-23 21:21:13 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=a1aHGAN2Kh9MCJarZqEmVP961lNMRE5xJqYvBKZvky4-1702414273014-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d7f41ef443be-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:51:12,946 [INFO]: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2023-12-12 15:51:12,946 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:51:12,947 [DEBUG]: receive_response_body.complete
2023-12-12 15:51:12,947 [DEBUG]: response_closed.started
2023-12-12 15:51:12,947 [DEBUG]: response_closed.complete
2023-12-12 15:51:12,947 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2023-12-12 15:51:12,948 [INFO]: 127.0.0.1 - - [12/Dec/2023 15:51:12] "POST /api/chat HTTP/1.1" 200 -
2023-12-12 15:51:12,955 [DEBUG]: Request received for answer evaluation
2023-12-12 15:51:12,957 [DEBUG]: Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'text-davinci-003', 'prompt': 'Please rate the faithfulness of the answers to their contexts on a scale from 0 to 1, where 0 is completely unfaithful and 1 is completely faithful.\nExample 1:\nContext: The sky is blue.\nAnswer: The sky is blue.\nRating: 1\nExample 2:\nContext: The sky is blue.\nAnswer: The sky is often cloudy.\nRating: 0.5\nExample 3:\nContext: Apples are usually red.\nAnswer: Oranges are orange.\nRating: 0\nYour Turn:\nContext: Figure 10: Frame from Topic 6 (top) and generated saliency\nmaps from VST (bottom left) and TranSalNet (bottom right)\n5.2 Other Takeaways\nOne interesting takeaway from the results is that VST per-\nformed especially well on one specific video, Topic 6, which is\nshown in Figure 10. Intuitively, one would expect the saliency\nmaps from TranSalNet to be better representations of where\nusers are likely to gaze, as they identify the figures, tables,\nand middle of the scene as the more salient areas of the frame.\nVST, on the other hand, seems to mark the distorted wooden\nbeams at the very top of the frame, as salient.\nHowever, VST achieves far higher average viewport quality\nthan TranSalNet, as shown in Figure 7. Video 6 is notable\nin that it is one of the few videos in the dataset that takes\nplace indoors and has extremely noticeable ceiling distortions.\nTherefore, it is possible that VST is better at generating pre-\ndictions for distorted indoor videos, and perhaps users do tend\nto look at higher areas in these videos and find them more\nsalient. However, more sample videos would be needed to\nconclude that VST is significantly better at these videos.\nAnother important note is that given the relative speed of\nthe saliency map generation and adaptive bitrate allocation, as\nwell as how the model would be pre-trained and ready to make\ninferences, these techniques would be feasible in livestream-\ning applications. For instance, these processes typically take\nsampling method, and a special decoder for saliency and\nboundary detection. VST outperforms state-of-the-art\nCNN salient object detection models [6].\nThe criteria for selecting each technique were as follows:\n1.Recency : Because of how rapidly the field of machine\nlearning is evolving, this paper focuses on comparing\nstate-of-the-art ML models published at most five years\nago, and ideally even later.\n(a)PanoSalNet : As the oldest model, PanoSalNet [10],\nwas first published in 2018, but it has been cited\nover 80 times, and the author has used the model in\nan April 2023 paper [9]. Therefore, although it pre-\ndates state-of-the-art techniques like transformers,\nit continues to have relevancy today.\n(b)TranSalNet : Published in just 2022, TranSalNet is\nfairly recent and also leverages novel mechanisms\nlike multi-head self-attention.\n(c)Visual Saliency Transformer : Similarly, VST is\nalso recent, with a publication year of 2021.\n2.Available source code and datasets : Many of the recent\nliterature on machine learning approaches to saliency\nmap creation and viewport prediction do not have\npublicly available source code [1]. Other papers may\nhave public repositories but with missing, private-access\ndatasets or incomplete code that prevents the models\nfrom running. This project considered and tested five\nother alternative codebases but ultimately could not use\nthem due to their machine incompatibility or lack of\nmaintenance.\n3\nFigure 8: Manhattan vs. Euclidean distance [8]\nVST TranSalNet\n0 5.53 7.18\n1 6.71 7.55\n2 \nAnswer: VST (Visual Saliency Transformer) was selected as a model for the study based on several criteria. \n\nFirstly, the criteria of recency was taken into account. The study focused on comparing state-of-the-art machine learning models published within the last five years. VST was published in 2021, making it a relatively recent model.\n\nSecondly, the availability of source code and datasets was considered. Many recent papers on machine learning approaches to saliency map creation and viewport prediction do not have publicly available source code. However, VST had an available codebase and dataset, which made it feasible for testing and implementation.\n\nLastly, the performance of VST was assessed. In comparison to other state-of-the-art saliency map models, VST demonstrated a superior average viewport quality. This indicates that VST has the potential to be effective in predicting areas of visual attention in videos.\n\nOverall, the combination of recency, availability of code and datasets, and promising performance made VST a suitable choice for the study.\nRating:', 'max_tokens': 60}}
2023-12-12 15:51:12,958 [DEBUG]: close.started
2023-12-12 15:51:12,958 [DEBUG]: close.complete
2023-12-12 15:51:12,958 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-12 15:51:12,969 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1113dcb50>
2023-12-12 15:51:12,969 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x108e65370> server_hostname='api.openai.com' timeout=5.0
2023-12-12 15:51:12,985 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1113de190>
2023-12-12 15:51:12,985 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:51:12,985 [DEBUG]: send_request_headers.complete
2023-12-12 15:51:12,986 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:51:12,986 [DEBUG]: send_request_body.complete
2023-12-12 15:51:12,986 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:51:13,304 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:51:13 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'text-davinci-003'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'198'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-limit-tokens_usage_based', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'249940'), (b'x-ratelimit-remaining-tokens_usage_based', b'249940'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'14ms'), (b'x-ratelimit-reset-tokens_usage_based', b'14ms'), (b'x-request-id', b'5d733fc208566bec4140156934b27030'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d816a95a0f88-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:51:13,305 [INFO]: HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2023-12-12 15:51:13,305 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:51:13,305 [DEBUG]: receive_response_body.complete
2023-12-12 15:51:13,306 [DEBUG]: response_closed.started
2023-12-12 15:51:13,306 [DEBUG]: response_closed.complete
2023-12-12 15:51:13,306 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2023-12-12 15:51:13,308 [DEBUG]: Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'text-davinci-003', 'prompt': 'Please rate the context relevance of the answers on a scale from 0 to 1, where 0 is not relevant and 1 is highly relevant.\nExample 1:\nContext: History of the Roman Empire.\nAnswer: Julius Caesar was a Roman ruler.\nRating: 1\nExample 2:\nContext: History of the Roman Empire.\nAnswer: The Roman Empire was in Africa.\nRating: 0.5\nExample 3:\nContext: Computer Programming Basics.\nAnswer: Apples grow on trees.\nRating: 0\nYour Turn:\nContext: Figure 10: Frame from Topic 6 (top) and generated saliency\nmaps from VST (bottom left) and TranSalNet (bottom right)\n5.2 Other Takeaways\nOne interesting takeaway from the results is that VST per-\nformed especially well on one specific video, Topic 6, which is\nshown in Figure 10. Intuitively, one would expect the saliency\nmaps from TranSalNet to be better representations of where\nusers are likely to gaze, as they identify the figures, tables,\nand middle of the scene as the more salient areas of the frame.\nVST, on the other hand, seems to mark the distorted wooden\nbeams at the very top of the frame, as salient.\nHowever, VST achieves far higher average viewport quality\nthan TranSalNet, as shown in Figure 7. Video 6 is notable\nin that it is one of the few videos in the dataset that takes\nplace indoors and has extremely noticeable ceiling distortions.\nTherefore, it is possible that VST is better at generating pre-\ndictions for distorted indoor videos, and perhaps users do tend\nto look at higher areas in these videos and find them more\nsalient. However, more sample videos would be needed to\nconclude that VST is significantly better at these videos.\nAnother important note is that given the relative speed of\nthe saliency map generation and adaptive bitrate allocation, as\nwell as how the model would be pre-trained and ready to make\ninferences, these techniques would be feasible in livestream-\ning applications. For instance, these processes typically take\nsampling method, and a special decoder for saliency and\nboundary detection. VST outperforms state-of-the-art\nCNN salient object detection models [6].\nThe criteria for selecting each technique were as follows:\n1.Recency : Because of how rapidly the field of machine\nlearning is evolving, this paper focuses on comparing\nstate-of-the-art ML models published at most five years\nago, and ideally even later.\n(a)PanoSalNet : As the oldest model, PanoSalNet [10],\nwas first published in 2018, but it has been cited\nover 80 times, and the author has used the model in\nan April 2023 paper [9]. Therefore, although it pre-\ndates state-of-the-art techniques like transformers,\nit continues to have relevancy today.\n(b)TranSalNet : Published in just 2022, TranSalNet is\nfairly recent and also leverages novel mechanisms\nlike multi-head self-attention.\n(c)Visual Saliency Transformer : Similarly, VST is\nalso recent, with a publication year of 2021.\n2.Available source code and datasets : Many of the recent\nliterature on machine learning approaches to saliency\nmap creation and viewport prediction do not have\npublicly available source code [1]. Other papers may\nhave public repositories but with missing, private-access\ndatasets or incomplete code that prevents the models\nfrom running. This project considered and tested five\nother alternative codebases but ultimately could not use\nthem due to their machine incompatibility or lack of\nmaintenance.\n3\nFigure 8: Manhattan vs. Euclidean distance [8]\nVST TranSalNet\n0 5.53 7.18\n1 6.71 7.55\n2 \nAnswer: VST (Visual Saliency Transformer) was selected as a model for the study based on several criteria. \n\nFirstly, the criteria of recency was taken into account. The study focused on comparing state-of-the-art machine learning models published within the last five years. VST was published in 2021, making it a relatively recent model.\n\nSecondly, the availability of source code and datasets was considered. Many recent papers on machine learning approaches to saliency map creation and viewport prediction do not have publicly available source code. However, VST had an available codebase and dataset, which made it feasible for testing and implementation.\n\nLastly, the performance of VST was assessed. In comparison to other state-of-the-art saliency map models, VST demonstrated a superior average viewport quality. This indicates that VST has the potential to be effective in predicting areas of visual attention in videos.\n\nOverall, the combination of recency, availability of code and datasets, and promising performance made VST a suitable choice for the study.\nRating:', 'max_tokens': 60}}
2023-12-12 15:51:13,308 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:51:13,309 [DEBUG]: send_request_headers.complete
2023-12-12 15:51:13,309 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:51:13,309 [DEBUG]: send_request_body.complete
2023-12-12 15:51:13,309 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:51:13,657 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:51:13 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'text-davinci-003'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'215'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-limit-tokens_usage_based', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'249940'), (b'x-ratelimit-remaining-tokens_usage_based', b'249260'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'14ms'), (b'x-ratelimit-reset-tokens_usage_based', b'177ms'), (b'x-request-id', b'e78157fac0737f9ca93dc01e30a521d7'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d818abe50f88-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:51:13,657 [INFO]: HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2023-12-12 15:51:13,658 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:51:13,658 [DEBUG]: receive_response_body.complete
2023-12-12 15:51:13,658 [DEBUG]: response_closed.started
2023-12-12 15:51:13,658 [DEBUG]: response_closed.complete
2023-12-12 15:51:13,658 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2023-12-12 15:51:13,660 [DEBUG]: Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'text-davinci-003', 'prompt': 'Please rate the relevance of the answers to the questions on a scale from 0 to 1, where 0 is not relevant and 1 is highly relevant.\nExample 1:\nQuestion: What is the capital of France?\nAnswer: Paris is the capital of France.\nRating: 1\nExample 2:\nQuestion: What is the capital of France?\nAnswer: France is in Europe.\nRating: 0.5\nExample 3:\nQuestion: What is the capital of France?\nAnswer: Elephants are the largest land animals.\nRating: 0\nYour Turn:\nQuestion: why was VST selected as a model for the study\nAnswer: VST (Visual Saliency Transformer) was selected as a model for the study based on several criteria. \n\nFirstly, the criteria of recency was taken into account. The study focused on comparing state-of-the-art machine learning models published within the last five years. VST was published in 2021, making it a relatively recent model.\n\nSecondly, the availability of source code and datasets was considered. Many recent papers on machine learning approaches to saliency map creation and viewport prediction do not have publicly available source code. However, VST had an available codebase and dataset, which made it feasible for testing and implementation.\n\nLastly, the performance of VST was assessed. In comparison to other state-of-the-art saliency map models, VST demonstrated a superior average viewport quality. This indicates that VST has the potential to be effective in predicting areas of visual attention in videos.\n\nOverall, the combination of recency, availability of code and datasets, and promising performance made VST a suitable choice for the study.\nRating:', 'max_tokens': 60}}
2023-12-12 15:51:13,661 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:51:13,662 [DEBUG]: send_request_headers.complete
2023-12-12 15:51:13,662 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:51:13,662 [DEBUG]: send_request_body.complete
2023-12-12 15:51:13,662 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:51:13,914 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:51:13 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'text-davinci-003'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'140'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-limit-tokens_usage_based', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'249940'), (b'x-ratelimit-remaining-tokens_usage_based', b'249285'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'14ms'), (b'x-ratelimit-reset-tokens_usage_based', b'171ms'), (b'x-request-id', b'b2628388df8966413ee35da64f1da801'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d81ade460f88-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:51:13,914 [INFO]: HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2023-12-12 15:51:13,914 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:51:13,915 [DEBUG]: receive_response_body.complete
2023-12-12 15:51:13,915 [DEBUG]: response_closed.started
2023-12-12 15:51:13,915 [DEBUG]: response_closed.complete
2023-12-12 15:51:13,915 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2023-12-12 15:51:13,916 [INFO]: 127.0.0.1 - - [12/Dec/2023 15:51:13] "POST /api/evaluate-answers HTTP/1.1" 200 -
2023-12-12 15:51:46,853 [DEBUG]: Using selector: KqueueSelector
2023-12-12 15:51:46,855 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-12 15:51:46,855 [DEBUG]: load_verify_locations cafile='/Users/aliceao/opt/miniconda3/envs/chat-lore/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-12 15:51:46,899 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x1114b7380>, 'json_data': {'input': 'what are some areas of future work identified in the study', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 15:51:46,900 [DEBUG]: close.started
2023-12-12 15:51:46,900 [DEBUG]: close.complete
2023-12-12 15:51:46,900 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-12 15:51:46,919 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1113edbd0>
2023-12-12 15:51:46,919 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x111350b00> server_hostname='api.openai.com' timeout=5.0
2023-12-12 15:51:46,936 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1113ef6d0>
2023-12-12 15:51:46,937 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:51:46,937 [DEBUG]: send_request_headers.complete
2023-12-12 15:51:46,937 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:51:46,937 [DEBUG]: send_request_body.complete
2023-12-12 15:51:46,937 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:51:47,077 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:51:47 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'19'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999986'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'e175f9db6191689007d5ac5207fbbdce'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d8ead9ccc47f-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:51:47,078 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 15:51:47,078 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:51:47,092 [DEBUG]: receive_response_body.complete
2023-12-12 15:51:47,092 [DEBUG]: response_closed.started
2023-12-12 15:51:47,093 [DEBUG]: response_closed.complete
2023-12-12 15:51:47,093 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 15:51:47,852 [DEBUG]: Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'AI assistant is a brand new, powerful, human-like artificial intelligence.\n                    The traits of AI include expert knowledge, helpfulness, cleverness, and articulateness.\n                    AI is a well-behaved and well-mannered individual.\n                    AI is always friendly, kind, and inspiring, and he is eager to provide vivid and thoughtful responses to the user.\n                    AI has the sum of all knowledge in their brain, and is able to accurately answer nearly any question about any topic in conversation.\n                    AI assistant is a big fan of Pinecone and Vercel.\n                    START CONTEXT BLOCK\n                    only a few milliseconds per frame, and this project tested a\nframe rate of 1fps. Therefore, it would be possible to test this\nworkflow with a live, real-world streaming system and human\nusers.\n6 Future Work\nWhile the basic evaluation of content-aware adaptive bitrate\nallocation methods has been completed, this project will be\nfurther extended into the spring semester to allow for further\nwork in quality optimization. The primary goals of future\nwork will be to address the limited applicability of VST andother pre-trained 2D image models, as mentioned in the dis-\ncussion.\n6.1 Main Goals\n6.1.1 Saliency-Only PanoSalNet\nThe code for PanoSalNet is open-source and was used to\nproduce the saliency results in Table 1. However, due to de-\npendency issues and outdated packages, it was difficult to\ngenerate more saliency maps for the new dataset and compare\nthem to VST and TranSalNet. This paper was therefore only\nable to compare those two models to published PanoSalNet\nresults with head-movement data. It would be helpful to then\nfind a way to fix the dependency and compatibility issues to\nhave a more direct comparison of saliency-only models and\ntheir performance on adaptive bitrate allocation.\n6.1.2 Transfer Learning, Finetuning, and Training\nGiven the high compute required to train transformers and\ntransformer-like models, this project did not include any fine-\ntuning or additional training. However, in a future semester,\nthis project could explore fine-tuning the pre-trained two\nment data because its authors defined the exact bitrate alloca-\ntion scheme and conducted the evaluations themselves.\n5 Discussion\nA few major takeaways can be gleaned from these results,\nmainly the applicability of pre-trained models in computer\nvision and limitations and gaps in the field of 360-degree\nlivestreaming.\nThis discussion aims to provide explanations and theories\nfor notable areas in the results, as well as guide future work\nand ideas for exploration.\n6\nlikely look at next, are streamed in high-quality, and the rest\nare delivered in lower quality [15]. Various ML approaches\ncan be used to predict which areas the user will look at\nnext, but these approaches can oftentimes be computation-\nally expensive and require lots of overhead, especially for\nlivestreams, which require content to be delivered quickly\n1\n                    END OF CONTEXT BLOCK\n                    AI assistant will take into account any CONTEXT BLOCK that is provided in a conversation.\n                    If the context does not provide the answer to question, the AI assistant will say, "I\'m sorry, but I don\'t know the answer to that question".\n                    AI assistant will not apologize for previous responses, but instead will indicated new information was gained.\n                    AI assistant will not invent anything that is not drawn directly from the context.\n                    '}, {'role': 'user', 'content': 'what are some areas of future work identified in the study'}], 'model': 'gpt-3.5-turbo'}}
2023-12-12 15:51:47,852 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-12 15:51:47,862 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x111411750>
2023-12-12 15:51:47,862 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x111350a70> server_hostname='api.openai.com' timeout=5.0
2023-12-12 15:51:47,878 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x111412610>
2023-12-12 15:51:47,879 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:51:47,879 [DEBUG]: send_request_headers.complete
2023-12-12 15:51:47,879 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:51:47,879 [DEBUG]: send_request_body.complete
2023-12-12 15:51:47,879 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:51:52,163 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:51:52 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'3999'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'160000'), (b'x-ratelimit-limit-tokens_usage_based', b'160000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'159075'), (b'x-ratelimit-remaining-tokens_usage_based', b'159075'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'346ms'), (b'x-ratelimit-reset-tokens_usage_based', b'346ms'), (b'x-request-id', b'f123590b3480109de3e3a890e2e207d8'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=YulXlhwSKfUTcevScFJltci0Jvd0oZQhyKU.zf1qL.A-1702414312-1-AXI4O8cUcoVD0YS2HYghohNxwOl5RKKKl6W8FbvhA06336F7jDZ1Q8CQoP/wFon3oEM0jRiXSzP8PWAsPZRZGCA=; path=/; expires=Tue, 12-Dec-23 21:21:52 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=ILgbiJeWq9ItpFJ18K6pkl2MIkIIvne.HLyMe7Zcurk-1702414312230-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d8f0bad68c05-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:51:52,163 [INFO]: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2023-12-12 15:51:52,164 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:51:52,164 [DEBUG]: receive_response_body.complete
2023-12-12 15:51:52,164 [DEBUG]: response_closed.started
2023-12-12 15:51:52,164 [DEBUG]: response_closed.complete
2023-12-12 15:51:52,164 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2023-12-12 15:51:52,166 [INFO]: 127.0.0.1 - - [12/Dec/2023 15:51:52] "POST /api/chat HTTP/1.1" 200 -
2023-12-12 15:51:52,172 [DEBUG]: Request received for answer evaluation
2023-12-12 15:51:52,173 [DEBUG]: Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'text-davinci-003', 'prompt': 'Please rate the faithfulness of the answers to their contexts on a scale from 0 to 1, where 0 is completely unfaithful and 1 is completely faithful.\nExample 1:\nContext: The sky is blue.\nAnswer: The sky is blue.\nRating: 1\nExample 2:\nContext: The sky is blue.\nAnswer: The sky is often cloudy.\nRating: 0.5\nExample 3:\nContext: Apples are usually red.\nAnswer: Oranges are orange.\nRating: 0\nYour Turn:\nContext: only a few milliseconds per frame, and this project tested a\nframe rate of 1fps. Therefore, it would be possible to test this\nworkflow with a live, real-world streaming system and human\nusers.\n6 Future Work\nWhile the basic evaluation of content-aware adaptive bitrate\nallocation methods has been completed, this project will be\nfurther extended into the spring semester to allow for further\nwork in quality optimization. The primary goals of future\nwork will be to address the limited applicability of VST andother pre-trained 2D image models, as mentioned in the dis-\ncussion.\n6.1 Main Goals\n6.1.1 Saliency-Only PanoSalNet\nThe code for PanoSalNet is open-source and was used to\nproduce the saliency results in Table 1. However, due to de-\npendency issues and outdated packages, it was difficult to\ngenerate more saliency maps for the new dataset and compare\nthem to VST and TranSalNet. This paper was therefore only\nable to compare those two models to published PanoSalNet\nresults with head-movement data. It would be helpful to then\nfind a way to fix the dependency and compatibility issues to\nhave a more direct comparison of saliency-only models and\ntheir performance on adaptive bitrate allocation.\n6.1.2 Transfer Learning, Finetuning, and Training\nGiven the high compute required to train transformers and\ntransformer-like models, this project did not include any fine-\ntuning or additional training. However, in a future semester,\nthis project could explore fine-tuning the pre-trained two\nment data because its authors defined the exact bitrate alloca-\ntion scheme and conducted the evaluations themselves.\n5 Discussion\nA few major takeaways can be gleaned from these results,\nmainly the applicability of pre-trained models in computer\nvision and limitations and gaps in the field of 360-degree\nlivestreaming.\nThis discussion aims to provide explanations and theories\nfor notable areas in the results, as well as guide future work\nand ideas for exploration.\n6\nlikely look at next, are streamed in high-quality, and the rest\nare delivered in lower quality [15]. Various ML approaches\ncan be used to predict which areas the user will look at\nnext, but these approaches can oftentimes be computation-\nally expensive and require lots of overhead, especially for\nlivestreams, which require content to be delivered quickly\n1\nAnswer: Some areas of future work identified in the study include:\n\n1. Saliency-Only PanoSalNet: The study mentioned difficulty in generating more saliency maps for the new dataset and comparing them to other models. Future work could focus on fixing the dependency and compatibility issues to have a more direct comparison of saliency-only models and their performance on adaptive bitrate allocation.\n\n2. Transfer Learning, Finetuning, and Training: Due to the high compute required to train transformers and transformer-like models, the study did not include any finetuning or additional training. Future work could explore fine-tuning the pre-trained models, allowing for further optimization and improvement.\n\nOverall, these areas of future work aim to address the limited applicability of existing models, explore new approaches, and optimize the quality of adaptive bitrate allocation methods in livestreaming systems.\nRating:', 'max_tokens': 60}}
2023-12-12 15:51:52,174 [DEBUG]: close.started
2023-12-12 15:51:52,175 [DEBUG]: close.complete
2023-12-12 15:51:52,175 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-12 15:51:52,184 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1113deed0>
2023-12-12 15:51:52,184 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x108e65370> server_hostname='api.openai.com' timeout=5.0
2023-12-12 15:51:52,198 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x111413310>
2023-12-12 15:51:52,198 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:51:52,198 [DEBUG]: send_request_headers.complete
2023-12-12 15:51:52,198 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:51:52,198 [DEBUG]: send_request_body.complete
2023-12-12 15:51:52,199 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:51:52,690 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:51:52 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'text-davinci-003'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'195'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-limit-tokens_usage_based', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'249940'), (b'x-ratelimit-remaining-tokens_usage_based', b'249940'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'14ms'), (b'x-ratelimit-reset-tokens_usage_based', b'14ms'), (b'x-request-id', b'321d767b97bb231a78d943d458c3fe62'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d90bbf29c445-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:51:52,690 [INFO]: HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2023-12-12 15:51:52,691 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:51:52,691 [DEBUG]: receive_response_body.complete
2023-12-12 15:51:52,691 [DEBUG]: response_closed.started
2023-12-12 15:51:52,691 [DEBUG]: response_closed.complete
2023-12-12 15:51:52,691 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2023-12-12 15:51:52,693 [DEBUG]: Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'text-davinci-003', 'prompt': 'Please rate the context relevance of the answers on a scale from 0 to 1, where 0 is not relevant and 1 is highly relevant.\nExample 1:\nContext: History of the Roman Empire.\nAnswer: Julius Caesar was a Roman ruler.\nRating: 1\nExample 2:\nContext: History of the Roman Empire.\nAnswer: The Roman Empire was in Africa.\nRating: 0.5\nExample 3:\nContext: Computer Programming Basics.\nAnswer: Apples grow on trees.\nRating: 0\nYour Turn:\nContext: only a few milliseconds per frame, and this project tested a\nframe rate of 1fps. Therefore, it would be possible to test this\nworkflow with a live, real-world streaming system and human\nusers.\n6 Future Work\nWhile the basic evaluation of content-aware adaptive bitrate\nallocation methods has been completed, this project will be\nfurther extended into the spring semester to allow for further\nwork in quality optimization. The primary goals of future\nwork will be to address the limited applicability of VST andother pre-trained 2D image models, as mentioned in the dis-\ncussion.\n6.1 Main Goals\n6.1.1 Saliency-Only PanoSalNet\nThe code for PanoSalNet is open-source and was used to\nproduce the saliency results in Table 1. However, due to de-\npendency issues and outdated packages, it was difficult to\ngenerate more saliency maps for the new dataset and compare\nthem to VST and TranSalNet. This paper was therefore only\nable to compare those two models to published PanoSalNet\nresults with head-movement data. It would be helpful to then\nfind a way to fix the dependency and compatibility issues to\nhave a more direct comparison of saliency-only models and\ntheir performance on adaptive bitrate allocation.\n6.1.2 Transfer Learning, Finetuning, and Training\nGiven the high compute required to train transformers and\ntransformer-like models, this project did not include any fine-\ntuning or additional training. However, in a future semester,\nthis project could explore fine-tuning the pre-trained two\nment data because its authors defined the exact bitrate alloca-\ntion scheme and conducted the evaluations themselves.\n5 Discussion\nA few major takeaways can be gleaned from these results,\nmainly the applicability of pre-trained models in computer\nvision and limitations and gaps in the field of 360-degree\nlivestreaming.\nThis discussion aims to provide explanations and theories\nfor notable areas in the results, as well as guide future work\nand ideas for exploration.\n6\nlikely look at next, are streamed in high-quality, and the rest\nare delivered in lower quality [15]. Various ML approaches\ncan be used to predict which areas the user will look at\nnext, but these approaches can oftentimes be computation-\nally expensive and require lots of overhead, especially for\nlivestreams, which require content to be delivered quickly\n1\nAnswer: Some areas of future work identified in the study include:\n\n1. Saliency-Only PanoSalNet: The study mentioned difficulty in generating more saliency maps for the new dataset and comparing them to other models. Future work could focus on fixing the dependency and compatibility issues to have a more direct comparison of saliency-only models and their performance on adaptive bitrate allocation.\n\n2. Transfer Learning, Finetuning, and Training: Due to the high compute required to train transformers and transformer-like models, the study did not include any finetuning or additional training. Future work could explore fine-tuning the pre-trained models, allowing for further optimization and improvement.\n\nOverall, these areas of future work aim to address the limited applicability of existing models, explore new approaches, and optimize the quality of adaptive bitrate allocation methods in livestreaming systems.\nRating:', 'max_tokens': 60}}
2023-12-12 15:51:52,694 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:51:52,694 [DEBUG]: send_request_headers.complete
2023-12-12 15:51:52,695 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:51:52,695 [DEBUG]: send_request_body.complete
2023-12-12 15:51:52,695 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:51:53,082 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:51:53 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'text-davinci-003'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'197'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-limit-tokens_usage_based', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'249940'), (b'x-ratelimit-remaining-tokens_usage_based', b'249940'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'14ms'), (b'x-ratelimit-reset-tokens_usage_based', b'14ms'), (b'x-request-id', b'192374f24d9ac7601e1275057b7d0d45'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d90edb54c445-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:51:53,082 [INFO]: HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2023-12-12 15:51:53,082 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:51:53,083 [DEBUG]: receive_response_body.complete
2023-12-12 15:51:53,083 [DEBUG]: response_closed.started
2023-12-12 15:51:53,083 [DEBUG]: response_closed.complete
2023-12-12 15:51:53,083 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2023-12-12 15:51:53,085 [DEBUG]: Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'text-davinci-003', 'prompt': 'Please rate the relevance of the answers to the questions on a scale from 0 to 1, where 0 is not relevant and 1 is highly relevant.\nExample 1:\nQuestion: What is the capital of France?\nAnswer: Paris is the capital of France.\nRating: 1\nExample 2:\nQuestion: What is the capital of France?\nAnswer: France is in Europe.\nRating: 0.5\nExample 3:\nQuestion: What is the capital of France?\nAnswer: Elephants are the largest land animals.\nRating: 0\nYour Turn:\nQuestion: what are some areas of future work identified in the study\nAnswer: Some areas of future work identified in the study include:\n\n1. Saliency-Only PanoSalNet: The study mentioned difficulty in generating more saliency maps for the new dataset and comparing them to other models. Future work could focus on fixing the dependency and compatibility issues to have a more direct comparison of saliency-only models and their performance on adaptive bitrate allocation.\n\n2. Transfer Learning, Finetuning, and Training: Due to the high compute required to train transformers and transformer-like models, the study did not include any finetuning or additional training. Future work could explore fine-tuning the pre-trained models, allowing for further optimization and improvement.\n\nOverall, these areas of future work aim to address the limited applicability of existing models, explore new approaches, and optimize the quality of adaptive bitrate allocation methods in livestreaming systems.\nRating:', 'max_tokens': 60}}
2023-12-12 15:51:53,086 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 15:51:53,086 [DEBUG]: send_request_headers.complete
2023-12-12 15:51:53,087 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 15:51:53,087 [DEBUG]: send_request_body.complete
2023-12-12 15:51:53,087 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 15:51:53,371 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 20:51:53 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'text-davinci-003'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'165'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-limit-tokens_usage_based', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'249939'), (b'x-ratelimit-remaining-tokens_usage_based', b'249758'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'14ms'), (b'x-ratelimit-reset-tokens_usage_based', b'58ms'), (b'x-request-id', b'f3b58958769670c7b272472ddf367aad'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348d9114e5cc445-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 15:51:53,371 [INFO]: HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2023-12-12 15:51:53,372 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 15:51:53,372 [DEBUG]: receive_response_body.complete
2023-12-12 15:51:53,372 [DEBUG]: response_closed.started
2023-12-12 15:51:53,372 [DEBUG]: response_closed.complete
2023-12-12 15:51:53,372 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2023-12-12 15:51:53,373 [INFO]: 127.0.0.1 - - [12/Dec/2023 15:51:53] "POST /api/evaluate-answers HTTP/1.1" 200 -
2023-12-12 16:11:43,722 [DEBUG]: Using selector: KqueueSelector
2023-12-12 16:11:43,723 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-12 16:11:43,724 [DEBUG]: load_verify_locations cafile='/Users/aliceao/opt/miniconda3/envs/chat-lore/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-12 16:11:43,768 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x111486660>, 'json_data': {'input': 'what is adaptive bitrate allocation', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-12 16:11:43,769 [DEBUG]: close.started
2023-12-12 16:11:43,769 [DEBUG]: close.complete
2023-12-12 16:11:43,769 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-12 16:11:43,784 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1113ec9d0>
2023-12-12 16:11:43,784 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x111350b00> server_hostname='api.openai.com' timeout=5.0
2023-12-12 16:11:43,797 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x111410090>
2023-12-12 16:11:43,797 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 16:11:43,798 [DEBUG]: send_request_headers.complete
2023-12-12 16:11:43,798 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 16:11:43,798 [DEBUG]: send_request_body.complete
2023-12-12 16:11:43,798 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 16:11:43,934 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 21:11:43 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'18'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999992'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'180d2bbc650af498699ce1b775ac0c45'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=wLYoZO3.9LUVoLvUkfu6jjYwmf9rMKlEjDqKDhVm3_U-1702415503-1-AXFQhp9x/NQEvcm+7aQIWVkg2lQ8zDRnaurOvBBWAuDHJhTgNum8RIPA2DHsKLTuL2wqusUOCHokroA0LtIZlPA=; path=/; expires=Tue, 12-Dec-23 21:41:43 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348f6230ac117fd-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 16:11:43,934 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-12 16:11:43,934 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 16:11:44,026 [DEBUG]: receive_response_body.complete
2023-12-12 16:11:44,027 [DEBUG]: response_closed.started
2023-12-12 16:11:44,027 [DEBUG]: response_closed.complete
2023-12-12 16:11:44,027 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-12 16:11:45,298 [DEBUG]: Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'AI assistant is a brand new, powerful, human-like artificial intelligence.\n                    The traits of AI include expert knowledge, helpfulness, cleverness, and articulateness.\n                    AI is a well-behaved and well-mannered individual.\n                    AI is always friendly, kind, and inspiring, and he is eager to provide vivid and thoughtful responses to the user.\n                    AI has the sum of all knowledge in their brain, and is able to accurately answer nearly any question about any topic in conversation.\n                    AI assistant is a big fan of Pinecone and Vercel.\n                    START CONTEXT BLOCK\n                    menting adaptive bitrate allocation, these preliminary results\ncan simply show that the models tend to identify similar areas\nas "salient." Additionally, all of these models have different\nideas of what qualifies as salient â€“ VST, for instance, is fo-\ncused on object detection and produces binary outputs of what\nis included in a salient object and what is not, while the other\nmodels produce more nuanced, gradual models of saliency.\nDespite these different goals, the ultimate measure of perfor-\nmance is the set of results with adaptive bitrate allocation\n(4.3.2).\nBelow are the informal evaluation results:\nPanoSalNet TranSalNet VST\nmae 0.1408 0.2387 0.3103\nmax-fm 0.3659 0.3655 0.3655\nmean-fm 0.1995 0.1688 0.1284\nmax e-measure 0.4300 0.4203 0.4275\nmean e-measure 0.3436 0.3688 0.4094\ns measure 0.4305 0.4776 0.4649\nAP 0.3213 0.3318 0.3165\nAUC 0.5112 0.5161 0.5127\nTable 1: Saliency detection results\nThese evaluation metrics are the same as those used in [6],\nand better results among the three are depicted in bold.\n4.2 Adaptive Bitrate Allocation\nAdaptive bitrate schemes typically use a tile-based approach\n[11] [15], in which video frames are split into rectangular\ntiles, for computational efficiency. Computing and assigning a\nunique bitrate to each pixel in the frame would be impractical,\nso tiling offers a feasible, yet still effective alternative. With\ntile-based approaches, tiles likely to be in or near the viewport\narea will be streamed in high quality, and others will be lower\nquality.\nIn [3], various content-agnostic models, such as PanoS-\nalNet [10] with head-movement maps added, are used in\nadaptive bitrate schemes and evaluated in terms of quality-\nof-experience and viewport prediction accuracy. This project\nutilizes the source code and same basic bitrate allocation\nof [3], as well as one of the datasets they used [14]. Unlike\nWild360, the dataset in [14] contains additional head move-\nment data taken from dozens of user testers that can serve\nas ground truth annotations, or representations of actual user\nviewports.\nThe original tiling scheme that [3] used with PanoSalNet\nhad9Ã—16tiles, so all three models also used 9Ã—16tiles and\nthe same bitrate allocation algorithm as in [3].\nFigure 4: Bitrate allocation and Manhattan Tile Error algo-\nrithm\nFigure 5: Example tiling schemes [15]\n5\nand continuously [16]. However, as the field of ML rapidly\nadvances, there is growing potential for new techniques and\nimprovements to solve the problem of viewport prediction.\n1.3 Related Work\n1.3.1 Past Approaches\nApproaches to adaptive bitrate allocation can be content-\naware and/or content-agnostic . A purely content-aware ap-\nproach identifies "salient" regions, where the user is likely to\nlook [13], whereas a content-agnostic approach considers a\nuserâ€™s past head or eye-tracking movement. Previous litera-\nture supports that the latter category typically exhibits higher\nperformance and less computational complexity [15].\nNguyen et al. created a novel framework t\n                    END OF CONTEXT BLOCK\n                    AI assistant will take into account any CONTEXT BLOCK that is provided in a conversation.\n                    If the context does not provide the answer to question, the AI assistant will say, "I\'m sorry, but I don\'t know the answer to that question".\n                    AI assistant will not apologize for previous responses, but instead will indicated new information was gained.\n                    AI assistant will not invent anything that is not drawn directly from the context.\n                    '}, {'role': 'user', 'content': 'what is adaptive bitrate allocation'}], 'model': 'gpt-3.5-turbo'}}
2023-12-12 16:11:45,298 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-12 16:11:45,308 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1116709d0>
2023-12-12 16:11:45,309 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x111350950> server_hostname='api.openai.com' timeout=5.0
2023-12-12 16:11:45,323 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x111670a90>
2023-12-12 16:11:45,323 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 16:11:45,324 [DEBUG]: send_request_headers.complete
2023-12-12 16:11:45,324 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 16:11:45,324 [DEBUG]: send_request_body.complete
2023-12-12 16:11:45,324 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 16:11:50,979 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 21:11:50 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'gpt-3.5-turbo-0613'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'5420'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'160000'), (b'x-ratelimit-limit-tokens_usage_based', b'160000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'158911'), (b'x-ratelimit-remaining-tokens_usage_based', b'158911'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'408ms'), (b'x-ratelimit-reset-tokens_usage_based', b'408ms'), (b'x-request-id', b'332a4a7e72e19ef3099781e2617665d1'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=IJv5G.fPiXxQLr7FVWOOO_s4sX7VNex.J_Ill0OWe7o-1702415510-1-AYmlcgcrine3mTUKGCioZr6phw3YV2eWaZi31v9uHszD5VtVpst2HhykBBlclNW7kqVh8cc7l4YCxzKyZJHsXlU=; path=/; expires=Tue, 12-Dec-23 21:41:50 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=8cP1BGoCt95wxi_gE0t1C1pCBkq_nz6ZiPjitMjVaXw-1702415510906-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348f62c8ada8c51-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 16:11:50,979 [INFO]: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2023-12-12 16:11:50,979 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 16:11:50,980 [DEBUG]: receive_response_body.complete
2023-12-12 16:11:50,980 [DEBUG]: response_closed.started
2023-12-12 16:11:50,980 [DEBUG]: response_closed.complete
2023-12-12 16:11:50,980 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/chat/completions "200 OK"
2023-12-12 16:11:50,982 [INFO]: 127.0.0.1 - - [12/Dec/2023 16:11:50] "POST /api/chat HTTP/1.1" 200 -
2023-12-12 16:11:50,988 [DEBUG]: Request received for answer evaluation
2023-12-12 16:11:50,990 [DEBUG]: Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'text-davinci-003', 'prompt': 'Please rate the faithfulness of the answers to their contexts on a scale from 0 to 1, where 0 is completely unfaithful and 1 is completely faithful.\nExample 1:\nContext: The sky is blue.\nAnswer: The sky is blue.\nRating: 1\nExample 2:\nContext: The sky is blue.\nAnswer: The sky is often cloudy.\nRating: 0.5\nExample 3:\nContext: Apples are usually red.\nAnswer: Oranges are orange.\nRating: 0\nYour Turn:\nContext: menting adaptive bitrate allocation, these preliminary results\ncan simply show that the models tend to identify similar areas\nas "salient." Additionally, all of these models have different\nideas of what qualifies as salient â€“ VST, for instance, is fo-\ncused on object detection and produces binary outputs of what\nis included in a salient object and what is not, while the other\nmodels produce more nuanced, gradual models of saliency.\nDespite these different goals, the ultimate measure of perfor-\nmance is the set of results with adaptive bitrate allocation\n(4.3.2).\nBelow are the informal evaluation results:\nPanoSalNet TranSalNet VST\nmae 0.1408 0.2387 0.3103\nmax-fm 0.3659 0.3655 0.3655\nmean-fm 0.1995 0.1688 0.1284\nmax e-measure 0.4300 0.4203 0.4275\nmean e-measure 0.3436 0.3688 0.4094\ns measure 0.4305 0.4776 0.4649\nAP 0.3213 0.3318 0.3165\nAUC 0.5112 0.5161 0.5127\nTable 1: Saliency detection results\nThese evaluation metrics are the same as those used in [6],\nand better results among the three are depicted in bold.\n4.2 Adaptive Bitrate Allocation\nAdaptive bitrate schemes typically use a tile-based approach\n[11] [15], in which video frames are split into rectangular\ntiles, for computational efficiency. Computing and assigning a\nunique bitrate to each pixel in the frame would be impractical,\nso tiling offers a feasible, yet still effective alternative. With\ntile-based approaches, tiles likely to be in or near the viewport\narea will be streamed in high quality, and others will be lower\nquality.\nIn [3], various content-agnostic models, such as PanoS-\nalNet [10] with head-movement maps added, are used in\nadaptive bitrate schemes and evaluated in terms of quality-\nof-experience and viewport prediction accuracy. This project\nutilizes the source code and same basic bitrate allocation\nof [3], as well as one of the datasets they used [14]. Unlike\nWild360, the dataset in [14] contains additional head move-\nment data taken from dozens of user testers that can serve\nas ground truth annotations, or representations of actual user\nviewports.\nThe original tiling scheme that [3] used with PanoSalNet\nhad9Ã—16tiles, so all three models also used 9Ã—16tiles and\nthe same bitrate allocation algorithm as in [3].\nFigure 4: Bitrate allocation and Manhattan Tile Error algo-\nrithm\nFigure 5: Example tiling schemes [15]\n5\nand continuously [16]. However, as the field of ML rapidly\nadvances, there is growing potential for new techniques and\nimprovements to solve the problem of viewport prediction.\n1.3 Related Work\n1.3.1 Past Approaches\nApproaches to adaptive bitrate allocation can be content-\naware and/or content-agnostic . A purely content-aware ap-\nproach identifies "salient" regions, where the user is likely to\nlook [13], whereas a content-agnostic approach considers a\nuserâ€™s past head or eye-tracking movement. Previous litera-\nture supports that the latter category typically exhibits higher\nperformance and less computational complexity [15].\nNguyen et al. created a novel framework t\nAnswer: Adaptive bitrate allocation is a technique used in video streaming to dynamically adjust the quality of the video based on the available network conditions. It involves splitting video frames into smaller tiles and assigning different bitrates (quality levels) to each tile. The goal is to deliver the best possible video quality to the viewer while maintaining a smooth and uninterrupted viewing experience.\n\nThe allocation of bitrates is typically done based on the viewport, which refers to the area of the video frame that is currently visible to the viewer. Higher quality tiles within or near the viewport are streamed at a higher bitrate, while tiles outside the viewport are streamed at a lower bitrate.\n\nThere are two main approaches to adaptive bitrate allocation: content-agnostic and content-aware. Content-agnostic approaches consider the viewer\'s past head or eye-tracking movement to predict the viewport, while content-aware approaches use saliency detection models to identify the salient regions in the video that the viewer is likely to focus on.\n\nDifferent evaluation metrics are used to assess the performance of adaptive bitrate schemes, including quality-of-experience, viewport prediction accuracy, and various measures of saliency detection.\n\nI hope this answers your question!\nRating:', 'max_tokens': 60}}
2023-12-12 16:11:50,991 [DEBUG]: close.started
2023-12-12 16:11:50,991 [DEBUG]: close.complete
2023-12-12 16:11:50,991 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-12 16:11:51,001 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1113bc510>
2023-12-12 16:11:51,001 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x108e65370> server_hostname='api.openai.com' timeout=5.0
2023-12-12 16:11:51,014 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1113de190>
2023-12-12 16:11:51,014 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 16:11:51,015 [DEBUG]: send_request_headers.complete
2023-12-12 16:11:51,015 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 16:11:51,015 [DEBUG]: send_request_body.complete
2023-12-12 16:11:51,015 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 16:11:51,418 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 21:11:51 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'text-davinci-003'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'256'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-limit-tokens_usage_based', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'249940'), (b'x-ratelimit-remaining-tokens_usage_based', b'249940'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'14ms'), (b'x-ratelimit-reset-tokens_usage_based', b'14ms'), (b'x-request-id', b'3d5e3359a946d827ac7dc7da9cfa252c'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=G2BMVRBSuy3mofwTFjYLX.uz.Ck5Dkw6TtyX6zAHBvk-1702415511-1-AcoI2o+szQNT/pWeR2mzTP6COLne0my8gc4N23lL7Z70FLwcxBILnOPUIRhpfl1gu5Okh2Ed/PFyqqBtBhMWe8A=; path=/; expires=Tue, 12-Dec-23 21:41:51 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348f6501fea43bd-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 16:11:51,418 [INFO]: HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2023-12-12 16:11:51,418 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 16:11:51,419 [DEBUG]: receive_response_body.complete
2023-12-12 16:11:51,419 [DEBUG]: response_closed.started
2023-12-12 16:11:51,419 [DEBUG]: response_closed.complete
2023-12-12 16:11:51,419 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2023-12-12 16:11:51,421 [DEBUG]: Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'text-davinci-003', 'prompt': 'Please rate the context relevance of the answers on a scale from 0 to 1, where 0 is not relevant and 1 is highly relevant.\nExample 1:\nContext: History of the Roman Empire.\nAnswer: Julius Caesar was a Roman ruler.\nRating: 1\nExample 2:\nContext: History of the Roman Empire.\nAnswer: The Roman Empire was in Africa.\nRating: 0.5\nExample 3:\nContext: Computer Programming Basics.\nAnswer: Apples grow on trees.\nRating: 0\nYour Turn:\nContext: menting adaptive bitrate allocation, these preliminary results\ncan simply show that the models tend to identify similar areas\nas "salient." Additionally, all of these models have different\nideas of what qualifies as salient â€“ VST, for instance, is fo-\ncused on object detection and produces binary outputs of what\nis included in a salient object and what is not, while the other\nmodels produce more nuanced, gradual models of saliency.\nDespite these different goals, the ultimate measure of perfor-\nmance is the set of results with adaptive bitrate allocation\n(4.3.2).\nBelow are the informal evaluation results:\nPanoSalNet TranSalNet VST\nmae 0.1408 0.2387 0.3103\nmax-fm 0.3659 0.3655 0.3655\nmean-fm 0.1995 0.1688 0.1284\nmax e-measure 0.4300 0.4203 0.4275\nmean e-measure 0.3436 0.3688 0.4094\ns measure 0.4305 0.4776 0.4649\nAP 0.3213 0.3318 0.3165\nAUC 0.5112 0.5161 0.5127\nTable 1: Saliency detection results\nThese evaluation metrics are the same as those used in [6],\nand better results among the three are depicted in bold.\n4.2 Adaptive Bitrate Allocation\nAdaptive bitrate schemes typically use a tile-based approach\n[11] [15], in which video frames are split into rectangular\ntiles, for computational efficiency. Computing and assigning a\nunique bitrate to each pixel in the frame would be impractical,\nso tiling offers a feasible, yet still effective alternative. With\ntile-based approaches, tiles likely to be in or near the viewport\narea will be streamed in high quality, and others will be lower\nquality.\nIn [3], various content-agnostic models, such as PanoS-\nalNet [10] with head-movement maps added, are used in\nadaptive bitrate schemes and evaluated in terms of quality-\nof-experience and viewport prediction accuracy. This project\nutilizes the source code and same basic bitrate allocation\nof [3], as well as one of the datasets they used [14]. Unlike\nWild360, the dataset in [14] contains additional head move-\nment data taken from dozens of user testers that can serve\nas ground truth annotations, or representations of actual user\nviewports.\nThe original tiling scheme that [3] used with PanoSalNet\nhad9Ã—16tiles, so all three models also used 9Ã—16tiles and\nthe same bitrate allocation algorithm as in [3].\nFigure 4: Bitrate allocation and Manhattan Tile Error algo-\nrithm\nFigure 5: Example tiling schemes [15]\n5\nand continuously [16]. However, as the field of ML rapidly\nadvances, there is growing potential for new techniques and\nimprovements to solve the problem of viewport prediction.\n1.3 Related Work\n1.3.1 Past Approaches\nApproaches to adaptive bitrate allocation can be content-\naware and/or content-agnostic . A purely content-aware ap-\nproach identifies "salient" regions, where the user is likely to\nlook [13], whereas a content-agnostic approach considers a\nuserâ€™s past head or eye-tracking movement. Previous litera-\nture supports that the latter category typically exhibits higher\nperformance and less computational complexity [15].\nNguyen et al. created a novel framework t\nAnswer: Adaptive bitrate allocation is a technique used in video streaming to dynamically adjust the quality of the video based on the available network conditions. It involves splitting video frames into smaller tiles and assigning different bitrates (quality levels) to each tile. The goal is to deliver the best possible video quality to the viewer while maintaining a smooth and uninterrupted viewing experience.\n\nThe allocation of bitrates is typically done based on the viewport, which refers to the area of the video frame that is currently visible to the viewer. Higher quality tiles within or near the viewport are streamed at a higher bitrate, while tiles outside the viewport are streamed at a lower bitrate.\n\nThere are two main approaches to adaptive bitrate allocation: content-agnostic and content-aware. Content-agnostic approaches consider the viewer\'s past head or eye-tracking movement to predict the viewport, while content-aware approaches use saliency detection models to identify the salient regions in the video that the viewer is likely to focus on.\n\nDifferent evaluation metrics are used to assess the performance of adaptive bitrate schemes, including quality-of-experience, viewport prediction accuracy, and various measures of saliency detection.\n\nI hope this answers your question!\nRating:', 'max_tokens': 60}}
2023-12-12 16:11:51,424 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 16:11:51,424 [DEBUG]: send_request_headers.complete
2023-12-12 16:11:51,424 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 16:11:51,424 [DEBUG]: send_request_body.complete
2023-12-12 16:11:51,424 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 16:11:51,773 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 21:11:51 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'text-davinci-003'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'226'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-limit-tokens_usage_based', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'249940'), (b'x-ratelimit-remaining-tokens_usage_based', b'249206'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'14ms'), (b'x-ratelimit-reset-tokens_usage_based', b'190ms'), (b'x-request-id', b'a73e949cc875c61e4977c93dbcc06993'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348f652ab0043bd-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 16:11:51,773 [INFO]: HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2023-12-12 16:11:51,773 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 16:11:51,774 [DEBUG]: receive_response_body.complete
2023-12-12 16:11:51,774 [DEBUG]: response_closed.started
2023-12-12 16:11:51,774 [DEBUG]: response_closed.complete
2023-12-12 16:11:51,774 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2023-12-12 16:11:51,776 [DEBUG]: Request options: {'method': 'post', 'url': '/completions', 'files': None, 'json_data': {'model': 'text-davinci-003', 'prompt': "Please rate the relevance of the answers to the questions on a scale from 0 to 1, where 0 is not relevant and 1 is highly relevant.\nExample 1:\nQuestion: What is the capital of France?\nAnswer: Paris is the capital of France.\nRating: 1\nExample 2:\nQuestion: What is the capital of France?\nAnswer: France is in Europe.\nRating: 0.5\nExample 3:\nQuestion: What is the capital of France?\nAnswer: Elephants are the largest land animals.\nRating: 0\nYour Turn:\nQuestion: what is adaptive bitrate allocation\nAnswer: Adaptive bitrate allocation is a technique used in video streaming to dynamically adjust the quality of the video based on the available network conditions. It involves splitting video frames into smaller tiles and assigning different bitrates (quality levels) to each tile. The goal is to deliver the best possible video quality to the viewer while maintaining a smooth and uninterrupted viewing experience.\n\nThe allocation of bitrates is typically done based on the viewport, which refers to the area of the video frame that is currently visible to the viewer. Higher quality tiles within or near the viewport are streamed at a higher bitrate, while tiles outside the viewport are streamed at a lower bitrate.\n\nThere are two main approaches to adaptive bitrate allocation: content-agnostic and content-aware. Content-agnostic approaches consider the viewer's past head or eye-tracking movement to predict the viewport, while content-aware approaches use saliency detection models to identify the salient regions in the video that the viewer is likely to focus on.\n\nDifferent evaluation metrics are used to assess the performance of adaptive bitrate schemes, including quality-of-experience, viewport prediction accuracy, and various measures of saliency detection.\n\nI hope this answers your question!\nRating:", 'max_tokens': 60}}
2023-12-12 16:11:51,777 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-12 16:11:51,777 [DEBUG]: send_request_headers.complete
2023-12-12 16:11:51,777 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-12 16:11:51,777 [DEBUG]: send_request_body.complete
2023-12-12 16:11:51,777 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-12 16:11:52,063 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Dec 2023 21:11:52 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'Cache-Control', b'no-cache, must-revalidate'), (b'openai-model', b'text-davinci-003'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'150'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'3000'), (b'x-ratelimit-limit-tokens', b'250000'), (b'x-ratelimit-limit-tokens_usage_based', b'250000'), (b'x-ratelimit-remaining-requests', b'2999'), (b'x-ratelimit-remaining-tokens', b'249940'), (b'x-ratelimit-remaining-tokens_usage_based', b'249234'), (b'x-ratelimit-reset-requests', b'20ms'), (b'x-ratelimit-reset-tokens', b'14ms'), (b'x-ratelimit-reset-tokens_usage_based', b'183ms'), (b'x-request-id', b'7a3522e8f218b79a63716b975e2a82ea'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8348f654ddc243bd-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-12 16:11:52,064 [INFO]: HTTP Request: POST https://api.openai.com/v1/completions "HTTP/1.1 200 OK"
2023-12-12 16:11:52,064 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-12 16:11:52,065 [DEBUG]: receive_response_body.complete
2023-12-12 16:11:52,065 [DEBUG]: response_closed.started
2023-12-12 16:11:52,065 [DEBUG]: response_closed.complete
2023-12-12 16:11:52,065 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/completions "200 OK"
2023-12-12 16:11:52,066 [INFO]: 127.0.0.1 - - [12/Dec/2023 16:11:52] "POST /api/evaluate-answers HTTP/1.1" 200 -
2023-12-12 16:12:41,484 [INFO]: 127.0.0.1 - - [12/Dec/2023 16:12:41] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-12 16:16:41,058 [INFO]: 127.0.0.1 - - [12/Dec/2023 16:16:41] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-12 16:16:56,443 [INFO]: 127.0.0.1 - - [12/Dec/2023 16:16:56] "[33mPOST /api/crawl HTTP/1.1[0m" 404 -
2023-12-12 16:32:21,037 [INFO]: 127.0.0.1 - - [12/Dec/2023 16:32:21] "[33mPOST /api/clearIndex HTTP/1.1[0m" 404 -
2023-12-12 16:39:27,897 [INFO]: 127.0.0.1 - - [12/Dec/2023 16:39:27] "[33mPOST /api/crawl HTTP/1.1[0m" 404 -
2023-12-12 16:39:30,051 [INFO]: 127.0.0.1 - - [12/Dec/2023 16:39:30] "[33mPOST /api/crawl HTTP/1.1[0m" 404 -
2023-12-12 17:01:29,038 [INFO]: 127.0.0.1 - - [12/Dec/2023 17:01:29] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-12 17:01:31,672 [INFO]: 127.0.0.1 - - [12/Dec/2023 17:01:31] "[33mPOST /api/crawl HTTP/1.1[0m" 404 -
2023-12-12 17:01:32,921 [INFO]: 127.0.0.1 - - [12/Dec/2023 17:01:32] "[33mPOST /api/crawl HTTP/1.1[0m" 404 -
2023-12-12 17:01:33,824 [INFO]: 127.0.0.1 - - [12/Dec/2023 17:01:33] "[33mPOST /api/crawl HTTP/1.1[0m" 404 -
2023-12-12 17:01:34,987 [INFO]: 127.0.0.1 - - [12/Dec/2023 17:01:34] "[33mPOST /api/clearIndex HTTP/1.1[0m" 404 -
2023-12-12 17:01:35,657 [INFO]: 127.0.0.1 - - [12/Dec/2023 17:01:35] "[33mPOST /api/crawl HTTP/1.1[0m" 404 -
2023-12-12 17:01:36,211 [INFO]: 127.0.0.1 - - [12/Dec/2023 17:01:36] "[33mPOST /api/crawl HTTP/1.1[0m" 404 -
2023-12-12 17:01:46,279 [INFO]: 127.0.0.1 - - [12/Dec/2023 17:01:46] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-12 17:02:38,617 [INFO]: 127.0.0.1 - - [12/Dec/2023 17:02:38] "[33mPOST /api/crawl HTTP/1.1[0m" 404 -
2023-12-12 17:02:39,834 [INFO]: 127.0.0.1 - - [12/Dec/2023 17:02:39] "[33mPOST /api/crawl HTTP/1.1[0m" 404 -
2023-12-12 17:03:01,877 [INFO]: 127.0.0.1 - - [12/Dec/2023 17:03:01] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-12 17:04:04,288 [INFO]: 127.0.0.1 - - [12/Dec/2023 17:04:04] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-12 17:05:03,566 [INFO]: 127.0.0.1 - - [12/Dec/2023 17:05:03] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-12 17:08:28,372 [INFO]: 127.0.0.1 - - [12/Dec/2023 17:08:28] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-12 17:08:37,525 [INFO]: 127.0.0.1 - - [12/Dec/2023 17:08:37] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-12 17:08:47,773 [INFO]: 127.0.0.1 - - [12/Dec/2023 17:08:47] "[33mPOST /api/crawl HTTP/1.1[0m" 404 -
2023-12-12 18:38:46,435 [INFO]: 127.0.0.1 - - [12/Dec/2023 18:38:46] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-12 18:38:52,591 [INFO]: 127.0.0.1 - - [12/Dec/2023 18:38:52] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-12 18:39:43,248 [INFO]: 127.0.0.1 - - [12/Dec/2023 18:39:43] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-12 18:40:57,773 [INFO]: 127.0.0.1 - - [12/Dec/2023 18:40:57] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-12 18:43:48,154 [INFO]: 127.0.0.1 - - [12/Dec/2023 18:43:48] "[33mPOST /api/clearIndex HTTP/1.1[0m" 404 -
2023-12-12 18:43:51,075 [INFO]: 127.0.0.1 - - [12/Dec/2023 18:43:51] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-12 18:44:23,385 [INFO]: 127.0.0.1 - - [12/Dec/2023 18:44:23] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-12 18:44:54,300 [INFO]: 127.0.0.1 - - [12/Dec/2023 18:44:54] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-12 18:48:03,566 [INFO]: 127.0.0.1 - - [12/Dec/2023 18:48:03] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-12 18:50:41,502 [INFO]: 127.0.0.1 - - [12/Dec/2023 18:50:41] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-12 19:01:28,542 [INFO]: 127.0.0.1 - - [12/Dec/2023 19:01:28] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-13 10:39:12,663 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-13 10:39:12,664 [DEBUG]: load_verify_locations cafile='/Users/aliceao/opt/miniconda3/envs/chat-lore/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-13 10:39:12,816 [INFO]: [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5328
2023-12-13 10:39:12,817 [INFO]: [33mPress CTRL+C to quit[0m
2023-12-13 10:39:22,707 [DEBUG]: come to upload fiels 
2023-12-13 10:39:22,747 [DEBUG]: <FileStorage: 'CPSC_490_Final_Report (1).pdf' ('application/pdf')>
2023-12-13 10:39:22,747 [DEBUG]: files
2023-12-13 10:39:22,761 [INFO]: 127.0.0.1 - - [13/Dec/2023 10:39:22] "POST /api/uploadFiles HTTP/1.1" 200 -
2023-12-13 10:39:22,777 [DEBUG]: Using selector: KqueueSelector
2023-12-13 10:39:23,100 [DEBUG]: Initialized Pinecone
2023-12-13 10:39:23,100 [DEBUG]: Type of pdf_file: <class 'str'>
2023-12-13 10:39:23,571 [DEBUG]: Parsed PDF
2023-12-13 10:39:23,771 [DEBUG]: List of indexes: ['research-chat-index']
2023-12-13 10:39:23,772 [DEBUG]: Initialized Pinecone Index
2023-12-13 10:39:23,776 [DEBUG]: Chunked PDF and obtained vectors
2023-12-13 10:39:23,778 [DEBUG]: load_ssl_context verify=True cert=None trust_env=True http2=False
2023-12-13 10:39:23,779 [DEBUG]: load_verify_locations cafile='/Users/aliceao/opt/miniconda3/envs/chat-lore/lib/python3.11/site-packages/certifi/cacert.pem'
2023-12-13 10:39:23,848 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x110ff4540>, 'json_data': {'input': 'Transformer-Based Machine Learning Techniques to Optimize Live 360 Â°Viewport Predictions Alice Ao alice.ao@yale.eduAdvisor: Prof. Sohee Kim Park sohee.park@yale.edu Abstract Recent investment in "the metaverse," an immersive world powered by virtual reality (VR), is indicative of a significant demand for high-quality 360 Â°livestreaming. However, there are many technical challenges involved with delivering such streams. First, the vastness of a full 360 Â°view typically de- mands substantially high bandwidth, and much of that view will be outside of the actual viewport and go unnoticed by the user. Second, the live nature of the videos also requires especially low latency and fast computation times. This project explores how novel machine-learning (ML) techniques can address these challenges related to 360 Â° livestreaming. It specifically focuses on whether transformer- based architectures, which have been key to the recent success of large language models, can also be effective for computer vision applications. This project ultimately 1) compares mul- tiple techniques that generate saliency maps 2) utilizes these saliency maps for viewport prediction and 3) evaluates the quality of the predicted viewports in adaptive bitrate stream- ing. 1 Background 1.1 Introduction In recent years, there has been a strong rise in demand for VR and immersive technologies. Perhaps most notably, in October 2021, Facebook CEO and founder Mark Zuckerberg', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-13 10:39:23,907 [DEBUG]: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2023-12-13 10:39:23,936 [DEBUG]: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x110f6c6d0>
2023-12-13 10:39:23,937 [DEBUG]: start_tls.started ssl_context=<ssl.SSLContext object at 0x1087c7650> server_hostname='api.openai.com' timeout=5.0
2023-12-13 10:39:23,967 [DEBUG]: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x110f6cdd0>
2023-12-13 10:39:23,968 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-13 10:39:23,968 [DEBUG]: send_request_headers.complete
2023-12-13 10:39:23,969 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-13 10:39:23,969 [DEBUG]: send_request_body.complete
2023-12-13 10:39:23,970 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-13 10:39:24,125 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 13 Dec 2023 15:39:24 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'30'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999635'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'0cfb3b5b5cd7ac24bd8d78e8b8994137'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=JTibhLBJz8e3CJndsKfF2zatvQl0eIhRaZsy5ekMKms-1702481964-1-Ad6RplhMX94HoA2q1dxLJLlMSI2JWP5ETNqFMMTVMgyND0EnEANr+ltA9OX7LEgwDOcP3EqGwTQqmF9VEsziYLE=; path=/; expires=Wed, 13-Dec-23 16:09:24 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=94HxTe0eq9GsbEymfO4ZPPAAPa2zXvLf0jngNd0M0Hc-1702481964197-0-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'834f4cb36b5642f5-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-13 10:39:24,126 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-13 10:39:24,127 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-13 10:39:24,130 [DEBUG]: receive_response_body.complete
2023-12-13 10:39:24,130 [DEBUG]: response_closed.started
2023-12-13 10:39:24,130 [DEBUG]: response_closed.complete
2023-12-13 10:39:24,131 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-13 10:39:24,134 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x110ff4540>, 'json_data': {'input': 'announced that the social media giant would be rebranded to Meta, signaling his desire to invest in "the metaverse," and ushering in a new wave of interest in new immersive technologies. While it remains unclear whether "the metaverse" will achieve the widespread adoption and popularity that Zucker- berg hopes for, there is undeniably strong interest in 360 Â° VR for applications like entertainment, education, and gam- ing. Major news broadcasters like CNN, NBC, BBC, and Al Jazeera now have 360 Â°streams, theme parks have incorpo- rated 360 Â°VR into their attractions, and medical training forphysicians can occur in 360 Â°formats [15]. Some estimates claim that by 2024, the market value of VR could even rise to $44.7 billion [9]. Todayâ€™s 360 Â°VR experiences follow a similar format, in which a user typically wears a head-mounted display (HMD) device to view and interact with the 360 Â°scene. The user can only see a small section of the entire scene at once, known as theviewport . By shifting and turning their head, and therefore changing the position of the viewport, the user can see differ- ent parts of the scene. While the format of the experiences are relatively similar, different end-to-end systems have different strategies on how to optimally deliver 360 Â°content, which this project aims to explore. 1.2 Problem Description There have been many advances in adaptive bitrate allocation and improving video resolution for regular 2D videos â€“ but', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-13 10:39:24,136 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-13 10:39:24,136 [DEBUG]: send_request_headers.complete
2023-12-13 10:39:24,136 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-13 10:39:24,137 [DEBUG]: send_request_body.complete
2023-12-13 10:39:24,137 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-13 10:39:24,281 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 13 Dec 2023 15:39:24 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'27'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999631'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'e783cc6700a6a746124f4bd8b046d51d'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'834f4cb46cb442f5-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-13 10:39:24,282 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-13 10:39:24,282 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-13 10:39:24,283 [DEBUG]: receive_response_body.complete
2023-12-13 10:39:24,283 [DEBUG]: response_closed.started
2023-12-13 10:39:24,283 [DEBUG]: response_closed.complete
2023-12-13 10:39:24,284 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-13 10:39:24,285 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x110f399e0>, 'json_data': {'input': 'the added complexities and distortions of the 360 Â°format prevent the typical reinforcement learning strategies for 2D videos from being effective for 360 Â°content. 360 Â°videos pose a complex problem that involves constant tradeoffs be- tween "streaming quality, spatial quality variance, viewport prediction errors, and bandwidth efficiency" [15]. First, streaming 360 Â°content often requires high bandwidth and low latency, and broadcasting the entire 360 Â°scene in high quality often consumes much of the networkâ€™s resources [15]. The viewport takes up a mere 20% of the full scene [11], so about 80% of the high-quality stream may be wasted on content that the user cannot see. Therefore, one technique to efficiently stream 360 Â°video and save bandwidth is to use adaptive bitrate schemes, in which the viewport area, as well as areas that the user will likely look at next, are streamed in high-quality, and the rest are delivered in lower quality [15]. Various ML approaches can be used to predict which areas the user will look at', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-13 10:39:24,286 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-13 10:39:24,287 [DEBUG]: send_request_headers.complete
2023-12-13 10:39:24,287 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-13 10:39:24,287 [DEBUG]: send_request_body.complete
2023-12-13 10:39:24,288 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-13 10:39:24,446 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 13 Dec 2023 15:39:24 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'38'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999738'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'3ms'), (b'x-request-id', b'd86d9f1eb3e5d12733af40779c2906ae'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'834f4cb55e1142f5-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-13 10:39:24,447 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-13 10:39:24,447 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-13 10:39:24,479 [DEBUG]: receive_response_body.complete
2023-12-13 10:39:24,479 [DEBUG]: response_closed.started
2023-12-13 10:39:24,480 [DEBUG]: response_closed.complete
2023-12-13 10:39:24,480 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-13 10:39:24,481 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x110f399e0>, 'json_data': {'input': 'likely look at next, are streamed in high-quality, and the rest are delivered in lower quality [15]. Various ML approaches can be used to predict which areas the user will look at next, but these approaches can oftentimes be computation- ally expensive and require lots of overhead, especially for livestreams, which require content to be delivered quickly 1', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-13 10:39:24,482 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-13 10:39:24,482 [DEBUG]: send_request_headers.complete
2023-12-13 10:39:24,483 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-13 10:39:24,483 [DEBUG]: send_request_body.complete
2023-12-13 10:39:24,483 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-13 10:39:37,713 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 13 Dec 2023 15:39:37 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'1833'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999910'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'1ms'), (b'x-request-id', b'bb177f11f34833e270d432f4399667d4'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'834f4cb68f9542f5-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-13 10:39:37,713 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-13 10:39:37,713 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-13 10:39:37,714 [DEBUG]: receive_response_body.complete
2023-12-13 10:39:37,714 [DEBUG]: response_closed.started
2023-12-13 10:39:37,714 [DEBUG]: response_closed.complete
2023-12-13 10:39:37,715 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-13 10:39:37,716 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x110f399e0>, 'json_data': {'input': 'and continuously [16]. However, as the field of ML rapidly advances, there is growing potential for new techniques and improvements to solve the problem of viewport prediction. 1.3 Related Work 1.3.1 Past Approaches Approaches to adaptive bitrate allocation can be content- aware and/or content-agnostic . A purely content-aware ap- proach identifies "salient" regions, where the user is likely to look [13], whereas a content-agnostic approach considers a userâ€™s past head or eye-tracking movement. Previous litera- ture supports that the latter category typically exhibits higher performance and less computational complexity [15]. Nguyen et al. created a novel framework that effectively combines both approaches, using a deep convolutional neural network to detect salient regions of 360 Â°videos and a Long Short-Term Memory Network (LSTM) that predicts head movements with both saliency maps and head orientations [9]. Similarly, Park et al. developed Mosaic, an end-to-end implementation for adaptive bitrate allocation, and found a 3D Convolutional Neural Network to have relatively high prediction accuracy and low prediction latency [11]. However, approaches that are purely content-agnostic can also deliver promising results. For instance, the discrete vari- ational multiple sequence (DVMS) learning framework uses deep latent variable models and deep neural networks to pre- dict multiple head trajectories. By considering multiple po-', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-13 10:39:37,717 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-13 10:39:37,717 [DEBUG]: send_request_headers.complete
2023-12-13 10:39:37,717 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-13 10:39:37,718 [DEBUG]: send_request_body.complete
2023-12-13 10:39:37,718 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-13 10:39:37,915 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 13 Dec 2023 15:39:37 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'54'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999638'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'b9caf1ef0526f6a57c4ec2d220048b49'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'834f4d094ae242f5-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-13 10:39:37,916 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-13 10:39:37,916 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-13 10:39:37,917 [DEBUG]: receive_response_body.complete
2023-12-13 10:39:37,917 [DEBUG]: response_closed.started
2023-12-13 10:39:37,917 [DEBUG]: response_closed.complete
2023-12-13 10:39:37,917 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-13 10:39:37,919 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x110f399e0>, 'json_data': {'input': 'tential trajectories, DVMS effectively accounts for how di- verse head trajectories can be and therefore greatly improves streaming quality [5]. These studies, however, are focused on on-demand videos, which typically have historical head/eye-tracking data avail- able of their previous viewers. They do not focus on live videostreams, which are constrained by the lack of historical user data and their requirements for especially low latency and high processing speed [4]. Therefore, in live viewport pre- diction often must rely on content-aware techniques as well. This project builds on existing work on regular 360 Â°videos by focusing on the unique needs of live content. It examines models that only rely on content-aware approaches and/or live head-tracking data for the individual user, and therefore can be used in live contexts. 1.3.2 Transformers This project also examines the possibility of using a novel ma- chine learning architecture, the transformer. During the past few years, computer vision models often used convolutional neural networks (CNNs) as their underlying architecture [11]. CNNs usually break an input up into small patches, allow- ing for more efficient and practical deep learning, and then gradually build up a global understanding of an input.However, computer vision models could improve by im- mediately leveraging the global context of images or videos. This global context is key to transformers and other self-', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-13 10:39:37,920 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-13 10:39:37,921 [DEBUG]: send_request_headers.complete
2023-12-13 10:39:37,921 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-13 10:39:37,922 [DEBUG]: send_request_body.complete
2023-12-13 10:39:37,922 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-13 10:39:39,201 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 13 Dec 2023 15:39:39 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'160'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999637'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'a59ce0e589a7779f15935cdd8524fc00'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'834f4d0a9ce542f5-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-13 10:39:39,202 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-13 10:39:39,202 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-13 10:39:39,203 [DEBUG]: receive_response_body.complete
2023-12-13 10:39:39,203 [DEBUG]: response_closed.started
2023-12-13 10:39:39,203 [DEBUG]: response_closed.complete
2023-12-13 10:39:39,204 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-13 10:39:39,205 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x110f399e0>, 'json_data': {'input': 'attention models, which make connections between different parts of their input using a mechanism called multi-head self- attention . Transformers have recently gained traction in natu- ral language processing (NLP) and other ML applications, as they were first designed and published in 2017 [12] and now serve as the architecture of nearly all large language models. One of their disadvantages, however, is that they require large amounts of compute and training data, the latter of which is more readily available for language research than vision research. Past research papers have recognized transformers as promising machine learning architectures for visual saliency detection [9] [5], and other viewport prediction models using transformer architectures have achieved promising results [1], so this project explores the effectiveness of transformers and transformer-inspired architectures for the application of live 360-degree video. Therefore, this project focuses on three saliency map models, with varying degrees of similarity to', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-13 10:39:39,206 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-13 10:39:39,206 [DEBUG]: send_request_headers.complete
2023-12-13 10:39:39,207 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-13 10:39:39,207 [DEBUG]: send_request_body.complete
2023-12-13 10:39:39,207 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-13 10:39:39,353 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 13 Dec 2023 15:39:39 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'27'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999740'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'3ms'), (b'x-request-id', b'602b247d5d577984eb15eec0cc071c8b'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'834f4d129ee842f5-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-13 10:39:39,353 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-13 10:39:39,354 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-13 10:39:39,354 [DEBUG]: receive_response_body.complete
2023-12-13 10:39:39,355 [DEBUG]: response_closed.started
2023-12-13 10:39:39,355 [DEBUG]: response_closed.complete
2023-12-13 10:39:39,355 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-13 10:39:39,356 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x110ff4540>, 'json_data': {'input': 'transformer-inspired architectures for the application of live 360-degree video. Therefore, this project focuses on three saliency map models, with varying degrees of similarity to a transformer-based architecture, and evaluates their mapsâ€™ performance in viewport prediction and adaptive bitrate allo- cation. 2 Methodology There are three main parts to this project: 1) identifying dif- ferent ML saliency detection models, with different levels of similarity to transformers, and 2) constructing a bitrate allocation scheme and 3) evaluating their results with metrics defined in [3]. The workflow of the first two parts is illustrated in Figure 1. 3 Preliminary Work 3.1 Selecting Machine Learning Techniques Similar to previous works [11], this paper identifies and com- pares state-of-the-art machine learning techniques on view- port prediction. All of the selected techniques focus on build- ingsaliency maps , which identify "salient" areas of each frame that users are likely to focus on. The final three selected machine-learning techniques were as follows: 1.PanoSalNet (published in MMSys â€™18): a deep CNN- based architecture specifically for generating saliency maps of 360-degree videos, with optional integration of head-tracking history. The architecture for the saliency map model is based on Deep Convnet, a state-of-the- art DCNN for 2D images, and transfer learning is used to adapt a pre-trained DCNN model to 360-degree im-', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-13 10:39:39,357 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-13 10:39:39,358 [DEBUG]: send_request_headers.complete
2023-12-13 10:39:39,358 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-13 10:39:39,358 [DEBUG]: send_request_body.complete
2023-12-13 10:39:39,358 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-13 10:39:39,530 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 13 Dec 2023 15:39:39 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'31'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999638'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'705ce09cf3c0e7968610d2ca5e8a1fe5'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'834f4d13880842f5-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-13 10:39:39,531 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-13 10:39:39,531 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-13 10:39:39,532 [DEBUG]: receive_response_body.complete
2023-12-13 10:39:39,532 [DEBUG]: response_closed.started
2023-12-13 10:39:39,532 [DEBUG]: response_closed.complete
2023-12-13 10:39:39,532 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-13 10:39:39,533 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x110ff4540>, 'json_data': {'input': 'ages. The model is trained using stochastic gradient de- 2', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-13 10:39:39,534 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-13 10:39:39,535 [DEBUG]: send_request_headers.complete
2023-12-13 10:39:39,535 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-13 10:39:39,536 [DEBUG]: send_request_body.complete
2023-12-13 10:39:39,536 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-13 10:39:39,688 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 13 Dec 2023 15:39:39 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'30'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999986'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'2d72d9b8ec234c60c27d9cffbc9cb5f4'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'834f4d1499c642f5-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-13 10:39:39,689 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-13 10:39:39,689 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-13 10:39:39,689 [DEBUG]: receive_response_body.complete
2023-12-13 10:39:39,689 [DEBUG]: response_closed.started
2023-12-13 10:39:39,690 [DEBUG]: response_closed.complete
2023-12-13 10:39:39,690 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-13 10:39:39,691 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x110ff4540>, 'json_data': {'input': 'Figure 1: Adaptive bitrate allocation process: 1) Videos are split into frames (fps=1). 2) ML model generates saliency maps, with white patches representing "salient" areas. 3) Saliency is averaged together for each of the 9x16 tiles and can be represented as a numerical value. 4) Tiles are assigned a bitrate based on their saliency value, with higher opacity in this figure representing higher bitrate. Figure 2: PanoSalNet final architecture [10]scent with a fixed learning rate. This model produces saliency maps, which can be combined with head ori- entation maps and then passed into a Long Short-Term Memory (LSTM) model (Figure 2) that predicts the next head orientation map, or viewport. Experimentally, the model achieved significant improvements in viewport quality [10]. 2.TranSalNet (published in Neurocomputing â€™22): a novel saliency model that adds transformer components to a traditional CNN backbone in order to capture long- range context and more closely emulate the human vi- sual system. The architecture includes some transformer encoders, complete with position embeddings and multi- head self-attention, as well as regular CNN encoders and decoders. This model has been trained and evaluated on 2D images, achieving state-of-the-art performance [7]. 3.Visual Saliency Transformer (published in CVPR â€™21): a pure transformer (i.e. convolution-free) saliency model for both RGB and RGB-D inputs. Novel components include multi-token fusion, a newly designed token up-', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-13 10:39:39,692 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-13 10:39:39,693 [DEBUG]: send_request_headers.complete
2023-12-13 10:39:39,693 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-13 10:39:39,693 [DEBUG]: send_request_body.complete
2023-12-13 10:39:39,694 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-13 10:39:39,854 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 13 Dec 2023 15:39:39 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'28'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999626'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'3df8b8ba663bc6ab8b041db973819371'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'834f4d159b3d42f5-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-13 10:39:39,855 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-13 10:39:39,855 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-13 10:39:39,856 [DEBUG]: receive_response_body.complete
2023-12-13 10:39:39,856 [DEBUG]: response_closed.started
2023-12-13 10:39:39,856 [DEBUG]: response_closed.complete
2023-12-13 10:39:39,857 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-13 10:39:39,858 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x110ff4540>, 'json_data': {'input': 'sampling method, and a special decoder for saliency and boundary detection. VST outperforms state-of-the-art CNN salient object detection models [6]. The criteria for selecting each technique were as follows: 1.Recency : Because of how rapidly the field of machine learning is evolving, this paper focuses on comparing state-of-the-art ML models published at most five years ago, and ideally even later. (a)PanoSalNet : As the oldest model, PanoSalNet [10], was first published in 2018, but it has been cited over 80 times, and the author has used the model in an April 2023 paper [9]. Therefore, although it pre- dates state-of-the-art techniques like transformers, it continues to have relevancy today. (b)TranSalNet : Published in just 2022, TranSalNet is fairly recent and also leverages novel mechanisms like multi-head self-attention. (c)Visual Saliency Transformer : Similarly, VST is also recent, with a publication year of 2021. 2.Available source code and datasets : Many of the recent literature on machine learning approaches to saliency map creation and viewport prediction do not have publicly available source code [1]. Other papers may have public repositories but with missing, private-access datasets or incomplete code that prevents the models from running. This project considered and tested five other alternative codebases but ultimately could not use them due to their machine incompatibility or lack of maintenance. 3', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-13 10:39:39,859 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-13 10:39:39,860 [DEBUG]: send_request_headers.complete
2023-12-13 10:39:39,860 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-13 10:39:39,860 [DEBUG]: send_request_body.complete
2023-12-13 10:39:39,861 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-13 10:39:40,452 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 13 Dec 2023 15:39:40 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'473'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999639'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'aa2ebe1e56b37c88a5bfd3e5a364a553'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'834f4d16ac7442f5-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-13 10:39:40,453 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-13 10:39:40,453 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-13 10:39:40,454 [DEBUG]: receive_response_body.complete
2023-12-13 10:39:40,454 [DEBUG]: response_closed.started
2023-12-13 10:39:40,454 [DEBUG]: response_closed.complete
2023-12-13 10:39:40,454 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-13 10:39:40,456 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x110ff4540>, 'json_data': {'input': '3.Relevance in computer vision and/or VR : Notably, some of the selected models [7] [6] have used images , not videos, as training and evaluation data. These models have largely been influential in the field of computer vi- sion and image processing, but they can be extrapolated to videos, which are simply a collection of image frames. However, because they have not been trained on 360- degree images, which must be projected and flattened onto a 2D plane, they may not be able to handle image distortions as well, so additional training and fine-tuning of the model may need to occur in the future. 3.2 Set-Up A non-trivial portion of the project was accessing published, open-source models and setting up their environments. Some of the models did not have environment files, so all of their dependencies had to be downloaded manually. Older mod- els [10] also had deprecated packages and dependencies, such as Caffe, that also had to be manually built and created, which was a non-trivial amount of work. Determining which pub- licly available models could produce valid results was also no trivial task. Some had broken pre-trained model files or missing files, so it was a trial-and-error process of identifying which repositories would be usable for the project. Additionally, many of the usable codebases had limited documentation, so important information, such as the in- put/output formats, function arguments, original datasets,', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-13 10:39:40,457 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-13 10:39:40,458 [DEBUG]: send_request_headers.complete
2023-12-13 10:39:40,458 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-13 10:39:40,458 [DEBUG]: send_request_body.complete
2023-12-13 10:39:40,458 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-13 10:39:40,600 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 13 Dec 2023 15:39:40 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'30'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999639'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'fc961e9b268d72303c1ad61745d9afbf'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'834f4d1a697e42f5-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-13 10:39:40,600 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-13 10:39:40,601 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-13 10:39:40,601 [DEBUG]: receive_response_body.complete
2023-12-13 10:39:40,602 [DEBUG]: response_closed.started
2023-12-13 10:39:40,602 [DEBUG]: response_closed.complete
2023-12-13 10:39:40,602 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-13 10:39:40,603 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x110f399e0>, 'json_data': {'input': 'edge cases/unexpected behavior, etc. had to be inferred from reading the papers and the code. Finally, some of the datasets used were not publicly avail- able â€“ access had to be requested for them, either by com- pleting a form [2] or writing an email to the study authors. Many of the modelsâ€™ results could not be easily reproduced, as their training and evaluation datasets were all private and/or deleted [3]. Therefore, the first part of the project was not to reproduce the three selected modelsâ€™ results but to gener- ate new saliency maps on a different dataset and informally compare their results. 4 Implementation 4.1 Saliency Maps 4.1.1 Generation After all three model environments were set up, the pre-trained models were run on the Wild360 dataset (described in further detail below). ffmpeg was used to extract frames from each of the 360-degree videos with a frame rate of 4fps. For each video frame, the models treated them as unique images and generated a binary saliency map for them. (For the actual adaptive bitrate allocation, a different dataset [14] was used, Figure 3: A colored heatmap from the Wild360 dataset (top) and black-and-white saliency maps (bottom) and saliency maps were generated for all of the videos in that dataset.) 4.1.2 Informal Evaluation Before implementing the adaptive bitrate allocation portion of the project, the three modelsâ€™ saliency detection abili- ties were compared using the Wild-360 dataset. The dataset', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-13 10:39:40,604 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-13 10:39:40,605 [DEBUG]: send_request_headers.complete
2023-12-13 10:39:40,605 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-13 10:39:40,605 [DEBUG]: send_request_body.complete
2023-12-13 10:39:40,606 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-13 10:39:40,800 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 13 Dec 2023 15:39:40 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'73'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999632'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'f83348adc7e1540f3375a1a629f7a787'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'834f4d1b4aa842f5-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-13 10:39:40,801 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-13 10:39:40,801 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-13 10:39:40,802 [DEBUG]: receive_response_body.complete
2023-12-13 10:39:40,802 [DEBUG]: response_closed.started
2023-12-13 10:39:40,802 [DEBUG]: response_closed.complete
2023-12-13 10:39:40,803 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-13 10:39:40,803 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x110f399e0>, 'json_data': {'input': 'contains 360-degree videos and their corresponding ground- truth saliency annotations, allowing for the evaluation of 360- degree saliency detection models. These annotations, which are in heatmap form, were generated from the scanpaths of several different human users [2]. Given time and compute constraints, the pre-trained ma- chine models were used without any additional fine-tuning and evaluated on 25 testing videos in the Wild-360 dataset. However, there are important caveats to the below results. First, because the ground-truth annotations are in colored heatmap format, and the generated saliency maps for all three models are in binary format, the evaluation metrics are imper- fect and should only be used for a very informal comparison of performance. Using a dataset with black-and-white ground truth saliency annotations would be ideal, but 360-degree video training datasets themselves have been difficult to come by. Minor ad- justments could theoretically be made to the Wild360 dataset so that itâ€™d be more compatible with the generated saliency', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-13 10:39:40,805 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-13 10:39:40,805 [DEBUG]: send_request_headers.complete
2023-12-13 10:39:40,805 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-13 10:39:40,806 [DEBUG]: send_request_body.complete
2023-12-13 10:39:40,806 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-13 10:39:40,963 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 13 Dec 2023 15:39:41 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'27'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999733'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'3ms'), (b'x-request-id', b'97b2f22cff81ffe5593f623aa9229153'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'834f4d1c9c7542f5-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-13 10:39:40,964 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-13 10:39:40,964 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-13 10:39:40,965 [DEBUG]: receive_response_body.complete
2023-12-13 10:39:40,965 [DEBUG]: response_closed.started
2023-12-13 10:39:40,965 [DEBUG]: response_closed.complete
2023-12-13 10:39:40,966 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-13 10:39:40,966 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x110f399e0>, 'json_data': {'input': 'datasets themselves have been difficult to come by. Minor ad- justments could theoretically be made to the Wild360 dataset so that itâ€™d be more compatible with the generated saliency maps. One naÃ¯ve approach, simply converting the colored heatmaps to grayscale, did not yield different results. How- ever, another option could be defining a threshold for the heatmaps, and making all values above the threshold salient (i.e. white) or non-salient (i.e. black). This would more closely resemble the black-and-white format of the saliency map out- put. However, itâ€™s unclear what threshold to use, and whether the choice of threshold could greatly influence the results. If this were a formal evaluation, more rigorous and thor- ough comparisons would be conducted. However, since this was only intended to be a simple baseline testing before imple- 4', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-13 10:39:40,967 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-13 10:39:40,968 [DEBUG]: send_request_headers.complete
2023-12-13 10:39:40,968 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-13 10:39:40,968 [DEBUG]: send_request_body.complete
2023-12-13 10:39:40,968 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-13 10:39:41,141 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 13 Dec 2023 15:39:41 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'53'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999787'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'2ms'), (b'x-request-id', b'0de8acdc5fe43ae6ec1cb8da86177de2'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'834f4d1d9ddd42f5-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-13 10:39:41,142 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-13 10:39:41,142 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-13 10:39:41,142 [DEBUG]: receive_response_body.complete
2023-12-13 10:39:41,143 [DEBUG]: response_closed.started
2023-12-13 10:39:41,143 [DEBUG]: response_closed.complete
2023-12-13 10:39:41,143 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-13 10:39:41,144 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x110f399e0>, 'json_data': {'input': 'menting adaptive bitrate allocation, these preliminary results can simply show that the models tend to identify similar areas as "salient." Additionally, all of these models have different ideas of what qualifies as salient â€“ VST, for instance, is fo- cused on object detection and produces binary outputs of what is included in a salient object and what is not, while the other models produce more nuanced, gradual models of saliency. Despite these different goals, the ultimate measure of perfor- mance is the set of results with adaptive bitrate allocation (4.3.2). Below are the informal evaluation results: PanoSalNet TranSalNet VST mae 0.1408 0.2387 0.3103 max-fm 0.3659 0.3655 0.3655 mean-fm 0.1995 0.1688 0.1284 max e-measure 0.4300 0.4203 0.4275 mean e-measure 0.3436 0.3688 0.4094 s measure 0.4305 0.4776 0.4649 AP 0.3213 0.3318 0.3165 AUC 0.5112 0.5161 0.5127 Table 1: Saliency detection results These evaluation metrics are the same as those used in [6], and better results among the three are depicted in bold. 4.2 Adaptive Bitrate Allocation Adaptive bitrate schemes typically use a tile-based approach [11] [15], in which video frames are split into rectangular tiles, for computational efficiency. Computing and assigning a unique bitrate to each pixel in the frame would be impractical, so tiling offers a feasible, yet still effective alternative. With tile-based approaches, tiles likely to be in or near the viewport area will be streamed in high quality, and others will be lower', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-13 10:39:41,146 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-13 10:39:41,146 [DEBUG]: send_request_headers.complete
2023-12-13 10:39:41,146 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-13 10:39:41,147 [DEBUG]: send_request_body.complete
2023-12-13 10:39:41,147 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-13 10:39:41,315 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 13 Dec 2023 15:39:41 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'44'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999625'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'15061aa619ec5996f3a09461482a13aa'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'834f4d1ebf3d42f5-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-13 10:39:41,316 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-13 10:39:41,316 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-13 10:39:41,317 [DEBUG]: receive_response_body.complete
2023-12-13 10:39:41,317 [DEBUG]: response_closed.started
2023-12-13 10:39:41,317 [DEBUG]: response_closed.complete
2023-12-13 10:39:41,317 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-13 10:39:41,319 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x110f399e0>, 'json_data': {'input': 'quality. In [3], various content-agnostic models, such as PanoS- alNet [10] with head-movement maps added, are used in adaptive bitrate schemes and evaluated in terms of quality- of-experience and viewport prediction accuracy. This project utilizes the source code and same basic bitrate allocation of [3], as well as one of the datasets they used [14]. Unlike Wild360, the dataset in [14] contains additional head move- ment data taken from dozens of user testers that can serve as ground truth annotations, or representations of actual user viewports. The original tiling scheme that [3] used with PanoSalNet had9Ã—16tiles, so all three models also used 9Ã—16tiles and the same bitrate allocation algorithm as in [3]. Figure 4: Bitrate allocation and Manhattan Tile Error algo- rithm Figure 5: Example tiling schemes [15] 5', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-13 10:39:41,320 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-13 10:39:41,320 [DEBUG]: send_request_headers.complete
2023-12-13 10:39:41,321 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-13 10:39:41,321 [DEBUG]: send_request_body.complete
2023-12-13 10:39:41,321 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-13 10:39:41,576 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 13 Dec 2023 15:39:41 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'29'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999794'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'2ms'), (b'x-request-id', b'7b09477ccdc1d36fe8e67bfb319cb611'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'834f4d1fc8bd42f5-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-13 10:39:41,577 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-13 10:39:41,577 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-13 10:39:41,578 [DEBUG]: receive_response_body.complete
2023-12-13 10:39:41,578 [DEBUG]: response_closed.started
2023-12-13 10:39:41,578 [DEBUG]: response_closed.complete
2023-12-13 10:39:41,579 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-13 10:39:41,579 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x110ff4540>, 'json_data': {'input': 'Figure 6: QoE aggregate score from [3] 4.3 Evaluation 4.3.1 Defining Quality-of-Experience 360-degree streaming projects typically define their own quality-of-experience (QoE) metric, which ideally allows them to quantify and compare the effectiveness of various viewport prediction methods. There is no universally agreed- upon standard for QoE, leaving room for subjectivity and author discretion. Therefore, a significant part of this project was also selecting and defining an appropriate QoE metric. The initial approach was to use the same QoE metric as defined in [3], as it considered four different aspects of a 360-degree viewing experience: 1) the average bitrate in a userâ€™s viewport for each frame 2) the variation of the bitrate in the viewport for each frame 3) the variation of bitrate across frames and 4) the variation of bitrate across successive chunks, or tiles. These four metrics are combined into an aggregate QoE score, as shown in Figure 6. However, examining the published code repository and running some small experiments demonstrated that it was difficult to compare models using this aggregated QoE score. Sometimes, QoE could be less than 0, and the results became less meaningful and interpretable. Therefore, this project ex- amines the four QOE metrics separately to extract insights of video quality. Since the frame rate was 1, QoE 3 is 0for every method, QoE 3 is omitted from the table of results. Furthermore, since the QoE score was the sum of all cal-', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-13 10:39:41,580 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-13 10:39:41,581 [DEBUG]: send_request_headers.complete
2023-12-13 10:39:41,581 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-13 10:39:41,581 [DEBUG]: send_request_body.complete
2023-12-13 10:39:41,582 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-13 10:39:41,734 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 13 Dec 2023 15:39:41 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'35'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999626'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'0443a31b8741dd22b906910d305a08bb'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'834f4d216ad342f5-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-13 10:39:41,734 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-13 10:39:41,734 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-13 10:39:41,735 [DEBUG]: receive_response_body.complete
2023-12-13 10:39:41,735 [DEBUG]: response_closed.started
2023-12-13 10:39:41,736 [DEBUG]: response_closed.complete
2023-12-13 10:39:41,736 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-13 10:39:41,737 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x110ff4540>, 'json_data': {'input': 'culated QoEs for all frames, it seemed that videos with more frames, as long as their QoEs were positive, would achieve better results. This makes it difficult to compare videos of different lengths, so in this project, QoE is divided by the number of frames in the end to achieve a fairer comparison. 4.3.2 Results As anticipated, the evaluation results of these three content- aware models are lower than the content-agnostic ones evalu- ated in [3]. Since content-agnostic approaches have access to the userâ€™s current viewport, they can assume that the userâ€™s next viewport will not deviate much from the current one, an assumption that tends to be correct. Content-agnostic meth- ods can also generate a unique predicted viewport for each user based on their individual head movement data, whereas for the content-aware ones, each userâ€™s individual movement was compared against the same model-generated prediction. The QoE results, as shown in Table 2 are significantly lower than those in [3]. However, as previously mentioned, this is to be expected, as the QoE results in this project are averaged per second, and the viewport accuracy is lower due Figure 7: QoE 1 Scores by Topic/Video to no head-movement data. We can also see that there is a very clear outlier for Topic 6, which VST achieves very high average quality for, which is explained more in the discussion section. VST TranSalNet QoE 1 0.00757 0.00873 QoE 2 0.01755 0.0122 QoE 4 0.00649 0.00200', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-13 10:39:41,738 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-13 10:39:41,739 [DEBUG]: send_request_headers.complete
2023-12-13 10:39:41,739 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-13 10:39:41,739 [DEBUG]: send_request_body.complete
2023-12-13 10:39:41,739 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-13 10:39:41,887 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 13 Dec 2023 15:39:41 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'29'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999632'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'93b5b232476287a6b0b21d80cbdd41ce'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'834f4d226c1f42f5-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-13 10:39:41,887 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-13 10:39:41,888 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-13 10:39:41,888 [DEBUG]: receive_response_body.complete
2023-12-13 10:39:41,889 [DEBUG]: response_closed.started
2023-12-13 10:39:41,889 [DEBUG]: response_closed.complete
2023-12-13 10:39:41,889 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-13 10:39:41,890 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x110ff4540>, 'json_data': {'input': 'QoE 2 0.01755 0.0122 QoE 4 0.00649 0.00200 Table 2: Median QoE scores across all videos. QoE 1: average bitrate in viewport QoE 2: variation of bitrate in viewport QoE 4: variation of bitrate across chunks However, the Manhattan Tile Error, a measure of viewport prediction accuracy, is promising. Figure 8 illustrates how the Manhattan distance, is typically calculated. The Manhattan Tile Error is then the minimum tile distance between the actual viewport tile and the predicted viewport tile. Table 3 displays the Manhattan Tile Error for the selected models, TranSalNet and VST, and Table 4 displays the Manhattan Tile Error for two models compared in [3]. Again, given that they have access to past head movement data, the models from [3] have predictably better Manhattan Tile Errors than saliency-only models. Furthermore, PARIMA understandably [3] outperforms PanoSalNet with head move- ment data because its authors defined the exact bitrate alloca- tion scheme and conducted the evaluations themselves. 5 Discussion A few major takeaways can be gleaned from these results,', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-13 10:39:41,891 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-13 10:39:41,892 [DEBUG]: send_request_headers.complete
2023-12-13 10:39:41,892 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-13 10:39:41,892 [DEBUG]: send_request_body.complete
2023-12-13 10:39:41,892 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-13 10:39:42,040 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 13 Dec 2023 15:39:42 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'27'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999730'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'3ms'), (b'x-request-id', b'092023d574becca8e721507c8817de30'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'834f4d235d6b42f5-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-13 10:39:42,040 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-13 10:39:42,041 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-13 10:39:42,041 [DEBUG]: receive_response_body.complete
2023-12-13 10:39:42,041 [DEBUG]: response_closed.started
2023-12-13 10:39:42,041 [DEBUG]: response_closed.complete
2023-12-13 10:39:42,042 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-13 10:39:42,043 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x110ff4540>, 'json_data': {'input': 'ment data because its authors defined the exact bitrate alloca- tion scheme and conducted the evaluations themselves. 5 Discussion A few major takeaways can be gleaned from these results, mainly the applicability of pre-trained models in computer vision and limitations and gaps in the field of 360-degree livestreaming. This discussion aims to provide explanations and theories for notable areas in the results, as well as guide future work and ideas for exploration. 6', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-13 10:39:42,044 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-13 10:39:42,044 [DEBUG]: send_request_headers.complete
2023-12-13 10:39:42,044 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-13 10:39:42,044 [DEBUG]: send_request_body.complete
2023-12-13 10:39:42,045 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-13 10:39:42,200 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 13 Dec 2023 15:39:42 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'36'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999882'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'1ms'), (b'x-request-id', b'2656dbf8ccb367dde2a6993834aefc71'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'834f4d244e7d42f5-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-13 10:39:42,201 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-13 10:39:42,201 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-13 10:39:42,202 [DEBUG]: receive_response_body.complete
2023-12-13 10:39:42,202 [DEBUG]: response_closed.started
2023-12-13 10:39:42,202 [DEBUG]: response_closed.complete
2023-12-13 10:39:42,202 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-13 10:39:42,204 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x110ff4540>, 'json_data': {'input': 'Figure 8: Manhattan vs. Euclidean distance [8] VST TranSalNet 0 5.53 7.18 1 6.71 7.55 2 7.69 8.43 3 6.61 9.38 4 8.01 8.13 5 7.18 7.71 6 5.93 8.66 7 8.20 8.26 8 8.18 8.35 Table 3: Manhattan Tile Error for tested models (fps=1) PARIMA PanoSalNet 0 0.14 1.65 1 0.16 2.69 2 0.15 2.35 3 0.13 2.25 4 0.18 1.63 5 0.18 1.66 6 0.16 1.36 7 0.18 2.51 8 0.10 2.42 Table 4: Original Manhattan Tile Error for models with HMD information [3] (fps=1) 5.1 Applicability of VST/TranSalNet One goal of this project was to determine whether pre-trained, open-source transformers [6] or transformer-like models [7] could accurately identify salient areas of a 360-degree video frame. The evaluation results demonstrate that "out-of-the- box" VST or TranSalNet, without any further training or mod- ifications, cannot be effectively applied to 360-degree video use cases. This can be partly attributed to the fact that VSTâ€™s and Figure 9: A 360-degree frame, and its corresponding VST- generated saliency map TranSalNetâ€™s training data of 2D images differs greatly from the format of 360-degree video frames. Visually examining some of VSTâ€™s generated saliency maps shows that at times, the model struggles with the distortion of these 360-degree frames. As demonstrated by Figure 9, the model sometimes selects extremely distorted objects, such as the aircraftâ€™s wing, even though humans may not deem it a visually salient feature. Another barrier to using VST, and to some extent, TranSal-', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-13 10:39:42,205 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-13 10:39:42,205 [DEBUG]: send_request_headers.complete
2023-12-13 10:39:42,206 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-13 10:39:42,206 [DEBUG]: send_request_body.complete
2023-12-13 10:39:42,206 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-13 10:39:42,400 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 13 Dec 2023 15:39:42 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'76'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999631'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'5f65847eff2aa8831eddbd5a63034a1e'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'834f4d254f8e42f5-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-13 10:39:42,400 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-13 10:39:42,401 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-13 10:39:42,401 [DEBUG]: receive_response_body.complete
2023-12-13 10:39:42,401 [DEBUG]: response_closed.started
2023-12-13 10:39:42,401 [DEBUG]: response_closed.complete
2023-12-13 10:39:42,402 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-13 10:39:42,403 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x110f399e0>, 'json_data': {'input': 'Net as well, for adaptive bitrate allocation is that the saliency maps they generate are mostly binary. Pixels are either white (salient) or black (not salient), which works for saliency evalu- ation but not nuanced head movement prediction and adaptive bitrate allocation, in which differences in bitrates should ide- ally be gradual. VST especially generates saliency maps with regions that are either sharply white or sharply black. TranSalNet theoretically achieves better QoE results than VST in terms of median average viewport bitrate, variation of bitrate in the viewport, and variation of bitrate between nearby chunks â€“ despite having lower Manhattan Tile Errors on nearly every video. Visually examining the differences between the two modelsâ€™ saliency maps may also explain why. TranSalNet tends to generate more subtle saliency maps that gradually shift from white to black, and therefore ends up allocating more medium bitrates to moderately salient/salient- adjacent regions, which leads to better quality streaming. Purely content-aware approaches are also more suited to 2D static images, as they typically assume that users will always be drawn to the most salient object in an image. However, with a 360-degree video, users are more likely to explore and gradually glance around, sometimes examining less salient details. Additionally, because they do not have the full context of the scene, they also may be more drawn to these less salient', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-13 10:39:42,404 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-13 10:39:42,404 [DEBUG]: send_request_headers.complete
2023-12-13 10:39:42,405 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-13 10:39:42,405 [DEBUG]: send_request_body.complete
2023-12-13 10:39:42,405 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-13 10:39:42,573 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 13 Dec 2023 15:39:42 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'34'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999633'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'1ee81f32c31e0459dcdb30618308d6ba'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'834f4d26893642f5-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-13 10:39:42,573 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-13 10:39:42,573 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-13 10:39:42,588 [DEBUG]: receive_response_body.complete
2023-12-13 10:39:42,588 [DEBUG]: response_closed.started
2023-12-13 10:39:42,588 [DEBUG]: response_closed.complete
2023-12-13 10:39:42,589 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-13 10:39:42,590 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x110f399e0>, 'json_data': {'input': 'details. Therefore, integrating head and eye-tracking data is crucial to improving viewport prediction. Finetuning or training both models from scratch could also improve performance, especially if 360-degree video data is used. It is unclear how much of the better performance of PanoSalNet and PARIMA can be attributed to their use of head-movement data, and how much is due to their training on 360-degree video. 7', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-13 10:39:42,592 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-13 10:39:42,592 [DEBUG]: send_request_headers.complete
2023-12-13 10:39:42,592 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-13 10:39:42,592 [DEBUG]: send_request_body.complete
2023-12-13 10:39:42,592 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-13 10:39:42,764 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 13 Dec 2023 15:39:42 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'21'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999895'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'1ms'), (b'x-request-id', b'b2f40584dfb065ad5ff07e716ca1ff17'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'834f4d27ba9242f5-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-13 10:39:42,764 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-13 10:39:42,764 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-13 10:39:42,765 [DEBUG]: receive_response_body.complete
2023-12-13 10:39:42,765 [DEBUG]: response_closed.started
2023-12-13 10:39:42,765 [DEBUG]: response_closed.complete
2023-12-13 10:39:42,765 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-13 10:39:42,766 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x110f399e0>, 'json_data': {'input': 'Figure 10: Frame from Topic 6 (top) and generated saliency maps from VST (bottom left) and TranSalNet (bottom right) 5.2 Other Takeaways One interesting takeaway from the results is that VST per- formed especially well on one specific video, Topic 6, which is shown in Figure 10. Intuitively, one would expect the saliency maps from TranSalNet to be better representations of where users are likely to gaze, as they identify the figures, tables, and middle of the scene as the more salient areas of the frame. VST, on the other hand, seems to mark the distorted wooden beams at the very top of the frame, as salient. However, VST achieves far higher average viewport quality than TranSalNet, as shown in Figure 7. Video 6 is notable in that it is one of the few videos in the dataset that takes place indoors and has extremely noticeable ceiling distortions. Therefore, it is possible that VST is better at generating pre- dictions for distorted indoor videos, and perhaps users do tend to look at higher areas in these videos and find them more salient. However, more sample videos would be needed to conclude that VST is significantly better at these videos. Another important note is that given the relative speed of the saliency map generation and adaptive bitrate allocation, as well as how the model would be pre-trained and ready to make inferences, these techniques would be feasible in livestream- ing applications. For instance, these processes typically take', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-13 10:39:42,767 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-13 10:39:42,768 [DEBUG]: send_request_headers.complete
2023-12-13 10:39:42,768 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-13 10:39:42,769 [DEBUG]: send_request_body.complete
2023-12-13 10:39:42,769 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-13 10:39:42,927 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 13 Dec 2023 15:39:43 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'49'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999632'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'23146aa21e552cb70372142e3652e500'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'834f4d28dbf042f5-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-13 10:39:42,928 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-13 10:39:42,928 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-13 10:39:42,929 [DEBUG]: receive_response_body.complete
2023-12-13 10:39:42,929 [DEBUG]: response_closed.started
2023-12-13 10:39:42,929 [DEBUG]: response_closed.complete
2023-12-13 10:39:42,929 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-13 10:39:42,930 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x110f399e0>, 'json_data': {'input': 'only a few milliseconds per frame, and this project tested a frame rate of 1fps. Therefore, it would be possible to test this workflow with a live, real-world streaming system and human users. 6 Future Work While the basic evaluation of content-aware adaptive bitrate allocation methods has been completed, this project will be further extended into the spring semester to allow for further work in quality optimization. The primary goals of future work will be to address the limited applicability of VST andother pre-trained 2D image models, as mentioned in the dis- cussion. 6.1 Main Goals 6.1.1 Saliency-Only PanoSalNet The code for PanoSalNet is open-source and was used to produce the saliency results in Table 1. However, due to de- pendency issues and outdated packages, it was difficult to generate more saliency maps for the new dataset and compare them to VST and TranSalNet. This paper was therefore only able to compare those two models to published PanoSalNet results with head-movement data. It would be helpful to then find a way to fix the dependency and compatibility issues to have a more direct comparison of saliency-only models and their performance on adaptive bitrate allocation. 6.1.2 Transfer Learning, Finetuning, and Training Given the high compute required to train transformers and transformer-like models, this project did not include any fine- tuning or additional training. However, in a future semester, this project could explore fine-tuning the pre-trained two', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-13 10:39:42,931 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-13 10:39:42,932 [DEBUG]: send_request_headers.complete
2023-12-13 10:39:42,932 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-13 10:39:42,932 [DEBUG]: send_request_body.complete
2023-12-13 10:39:42,933 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-13 10:39:43,109 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 13 Dec 2023 15:39:43 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'47'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999625'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'5fbb859e2f1664d940c8f9aa6c780d53'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'834f4d29dd0042f5-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-13 10:39:43,110 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-13 10:39:43,110 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-13 10:39:43,111 [DEBUG]: receive_response_body.complete
2023-12-13 10:39:43,111 [DEBUG]: response_closed.started
2023-12-13 10:39:43,111 [DEBUG]: response_closed.complete
2023-12-13 10:39:43,111 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-13 10:39:43,112 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x110f399e0>, 'json_data': {'input': 'transformer-like models, VST and TranSalNet, on 360-degree video frames, since their original inputs are 2D images. This would be similar to the work completed in PanoSalNet [10] and could likely reduce some of the saliency detection errors involved with these models. Another option could be fully training the initial models on 360-degree videos only, as the codebases provide the option of training from scratch. 6.1.3 Adding Content-Agnostic Methods Given the effectiveness of past content-agnostic methods on viewport prediction and adaptive bitrate allocation [15] [3], as well as the enhanced performance provided by combined content-agnostic and content-aware methods [9], the QoE could be greatly improved by adding head-movement infor- mation. This is one of the first priorities of the next semester, and it should be easy to integrate the LSTM architecture published in [10]. 6.1.4 Testing Different Tiling Schemes and Frame Rates Currently, the tiling scheme used was 9Ã—16, but these dimen-', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-13 10:39:43,113 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-13 10:39:43,113 [DEBUG]: send_request_headers.complete
2023-12-13 10:39:43,114 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-13 10:39:43,114 [DEBUG]: send_request_body.complete
2023-12-13 10:39:43,114 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-13 10:39:43,447 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 13 Dec 2023 15:39:43 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'26'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999749'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'3ms'), (b'x-request-id', b'911a077de90113f03847ea1ca272f792'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'834f4d2afe8342f5-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-13 10:39:43,447 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-13 10:39:43,447 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-13 10:39:43,448 [DEBUG]: receive_response_body.complete
2023-12-13 10:39:43,448 [DEBUG]: response_closed.started
2023-12-13 10:39:43,448 [DEBUG]: response_closed.complete
2023-12-13 10:39:43,449 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-13 10:39:43,450 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x110ff4540>, 'json_data': {'input': 'should be easy to integrate the LSTM architecture published in [10]. 6.1.4 Testing Different Tiling Schemes and Frame Rates Currently, the tiling scheme used was 9Ã—16, but these dimen- sions are slightly larger than those of other works [11] [3]. Therefore, it may be better to try out schemes with fewer tiles, such as an 8Ã—8one like [3] did, and compare the results in future work. Testing smaller and larger frame rates could also yield interesting takeaways for QoE analysis. 8', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-13 10:39:43,451 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-13 10:39:43,451 [DEBUG]: send_request_headers.complete
2023-12-13 10:39:43,451 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-13 10:39:43,452 [DEBUG]: send_request_body.complete
2023-12-13 10:39:43,452 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-13 10:39:43,607 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 13 Dec 2023 15:39:43 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'46'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999880'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'1ms'), (b'x-request-id', b'2e7e40298839945add0e95a8fd72fc44'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'834f4d2d18e142f5-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-13 10:39:43,607 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-13 10:39:43,608 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-13 10:39:43,608 [DEBUG]: receive_response_body.complete
2023-12-13 10:39:43,608 [DEBUG]: response_closed.started
2023-12-13 10:39:43,608 [DEBUG]: response_closed.complete
2023-12-13 10:39:43,609 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-13 10:39:43,610 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x110ff4540>, 'json_data': {'input': '6.2 Stretch Goals One major stretch goal for the following semester would be to deploy an end-to-end system using the finalized adaptive bitrate allocation scheme. Other objectives include defining a subjective QoE metric and conducting real-world testing with existing open-source tools for VR surveys and tests [15]. Participants could use a VR headset to interact with a sample of videos and rate and qualitatively describe their experience. Measuring QoE through participantsâ€™ subjective assessments could capture the real, human experience of viewing a 360 Â°livestream and gauge the effectiveness of the new adaptive bitrate scheme in a real-world setting. 7 Acknowledgements I would like to thank my advisor, Dr. Sohee Park, and her mentorship and guidance throughout this semester. I began this project without any knowledge of 360-degree streaming, and I am grateful for her continuous support and patience. I would also like to acknowledge Rachel Liang, the 490 teach- ing assistant, as well as the many computer science professors and friends who have helped encourage me throughout my time here at Yale. I cannot imagine being able to make it to the senior thesis without their help and kindness! References [1]F. Y . Chao, C. Ozcinar, and A. Smolic. Transformer- based long-term viewport prediction in 360 Â°video: Scanpath is all you need. In IEEE 23nd International Workshop on Multimedia Signal Processing (MMSP) , 2021. [2]H. Cheng, C. Chao, J. Dong, H. Wen, T. Liu, and M. Sun.', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-13 10:39:43,611 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-13 10:39:43,611 [DEBUG]: send_request_headers.complete
2023-12-13 10:39:43,611 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-13 10:39:43,612 [DEBUG]: send_request_body.complete
2023-12-13 10:39:43,612 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-13 10:39:43,793 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 13 Dec 2023 15:39:43 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'46'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999625'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'ed9ee3b2320adab883de7ecf997550dd'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'834f4d2e1a4d42f5-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-13 10:39:43,794 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-13 10:39:43,794 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-13 10:39:43,794 [DEBUG]: receive_response_body.complete
2023-12-13 10:39:43,794 [DEBUG]: response_closed.started
2023-12-13 10:39:43,795 [DEBUG]: response_closed.complete
2023-12-13 10:39:43,795 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-13 10:39:43,796 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x110ff4540>, 'json_data': {'input': 'Cube padding for weakly-supervised saliency prediction in 360 Â°videos. In 2018 IEEE/CVF Conference on Com- puter Vision and Pattern Recognition (CVPR) , pages 1420â€“1429, Los Alamitos, CA, USA, jun 2018. IEEE Computer Society. [3]Lovish Chopra, Sarthak Chakraborty, Abhijit Mondal, and Sandip Chakraborty. Parima: Viewport adaptive 360-degree video streaming. In Proceedings of the Web Conference 2021 , WWW â€™21. ACM, April 2021. [4]Xianglong Feng, Yao Liu, and Sheng Wei. Livedeep: On- line viewport prediction for live virtual reality streaming using lifelong deep learning. In 2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR) , pages 800â€“808, 2020. [5]Quentin Guimard, Lucile Sassatelli, Francesco Marchetti, Federico Becattini, Lorenzo Seidenari, andAlberto Del Bimbo. Deep variational learning for multiple trajectory prediction of 360 Â°head movements. InProceedings of the 13th ACM Multimedia Systems Conference , MMSys â€™22, page 12â€“26, New York, NY , USA, 2022. Association for Computing Machinery. [6]Nian Liu, Ni Zhang, Kaiyuan Wan, Junwei Han, and Ling Shao. Visual saliency transformer. CoRR , abs/2104.12099, 2021. [7]Jianxun Lou, Hanhe Lin, David Marshall, Dietmar Saupe, and Hantao Liu. Transalnet: Towards perceptu- ally relevant visual saliency prediction. Neurocomput- ing, 494:455â€“467, 2022. [8]Vinh-Trung Luu, Germain Forestier, Jonathan Weber, Paul Bourgeois, Fahima Djelil, and Pierre-Alain Muller. A review of alignment based similarity measures for', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-13 10:39:43,797 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-13 10:39:43,798 [DEBUG]: send_request_headers.complete
2023-12-13 10:39:43,798 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-13 10:39:43,799 [DEBUG]: send_request_body.complete
2023-12-13 10:39:43,799 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-13 10:39:43,961 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 13 Dec 2023 15:39:44 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'28'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999625'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'4ms'), (b'x-request-id', b'451f5539bd62619ea559acd501d3e858'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'834f4d2f6c1542f5-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-13 10:39:43,976 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-13 10:39:43,976 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-13 10:39:43,995 [DEBUG]: receive_response_body.complete
2023-12-13 10:39:43,996 [DEBUG]: response_closed.started
2023-12-13 10:39:43,996 [DEBUG]: response_closed.complete
2023-12-13 10:39:43,996 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-13 10:39:44,016 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x110ff4540>, 'json_data': {'input': 'web usage mining. Artificial Intelligence Review , 53, 03 2020. [9]Anh Nguyen and Zhisheng Yan. Enhancing 360 video streaming through salient content in head-mounted dis- plays. Sensors , 23(8), 2023. [10] Anh Nguyen, Zhisheng Yan, and Klara Nahrstedt. Your attention is unique: Detecting 360-degree video saliency in head-mounted display for head movement prediction. pages 1190â€“1198, 10 2018. [11] Sohee Park, Arani Bhattacharya, Zhibo Yang, Samir R. Das, and Dimitris Samaras. Mosaic: Advancing user quality of experience in 360-degree video streaming with machine learning. IEEE Transactions on Network and Service Management , 18(1):1000â€“1015, 2021. [12] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023. [13] Shibo Wang, Shusen Yang, Hailiang Li, Xiaodan Zhang, Chen Zhou, Chenren Xu, Feng Qian, Nanbin Wang, and Zongben Xu. Salientvr: Saliency-driven mobile 360-', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-13 10:39:44,017 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-13 10:39:44,017 [DEBUG]: send_request_headers.complete
2023-12-13 10:39:44,018 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-13 10:39:44,018 [DEBUG]: send_request_body.complete
2023-12-13 10:39:44,018 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-13 10:39:44,164 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 13 Dec 2023 15:39:44 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'27'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999755'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'2ms'), (b'x-request-id', b'5a0abe856cb0234eb24bbbfdd7ab898d'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'834f4d30ae0442f5-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-13 10:39:44,165 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-13 10:39:44,165 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-13 10:39:44,165 [DEBUG]: receive_response_body.complete
2023-12-13 10:39:44,166 [DEBUG]: response_closed.started
2023-12-13 10:39:44,166 [DEBUG]: response_closed.complete
2023-12-13 10:39:44,166 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-13 10:39:44,167 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x110ff4540>, 'json_data': {'input': '[13] Shibo Wang, Shusen Yang, Hailiang Li, Xiaodan Zhang, Chen Zhou, Chenren Xu, Feng Qian, Nanbin Wang, and Zongben Xu. Salientvr: Saliency-driven mobile 360- degree video streaming with gaze information. In Pro- ceedings of the 28th Annual International Conference on Mobile Computing And Networking , MobiCom â€™22, page 542â€“555, New York, NY , USA, 2022. Association for Computing Machinery. [14] Chenglei Wu, Zhihao Tan, Zhi Wang, and Shiqiang Yang. A dataset for exploring user behaviors in vr spherical video streaming. Proceedings of the 8th ACM on Multi- media Systems Conference , 2017. [15] Abid Yaqoob, Ting Bi, and Gabriel-Miro Muntean. A survey on adaptive 360 Â°video streaming: Solutions, challenges and opportunities. IEEE Communications Surveys Tutorials , 22(4):2801â€“2838, 2020. 9', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-13 10:39:44,168 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-13 10:39:44,169 [DEBUG]: send_request_headers.complete
2023-12-13 10:39:44,169 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-13 10:39:44,169 [DEBUG]: send_request_body.complete
2023-12-13 10:39:44,169 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-13 10:39:44,335 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 13 Dec 2023 15:39:44 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'34'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999800'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'2ms'), (b'x-request-id', b'f0d77c1ca4601af3ba564bca6fad1c0d'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'834f4d319f3042f5-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-13 10:39:44,335 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-13 10:39:44,336 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-13 10:39:44,336 [DEBUG]: receive_response_body.complete
2023-12-13 10:39:44,336 [DEBUG]: response_closed.started
2023-12-13 10:39:44,337 [DEBUG]: response_closed.complete
2023-12-13 10:39:44,337 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-13 10:39:44,339 [DEBUG]: Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x110f399e0>, 'json_data': {'input': '[16] Hyunho Yeo, Hwijoon Lim, Jaehong Kim, Youngmok Jung, Juncheol Ye, and Dongsu Han. Neuroscaler: Neu- ral video enhancement at scale. In Proceedings of the ACM SIGCOMM 2022 Conference , SIGCOMM â€™22, page 795â€“811, New York, NY , USA, 2022. Association for Computing Machinery. 10', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}
2023-12-13 10:39:44,343 [DEBUG]: send_request_headers.started request=<Request [b'POST']>
2023-12-13 10:39:44,343 [DEBUG]: send_request_headers.complete
2023-12-13 10:39:44,343 [DEBUG]: send_request_body.started request=<Request [b'POST']>
2023-12-13 10:39:44,344 [DEBUG]: send_request_body.complete
2023-12-13 10:39:44,344 [DEBUG]: receive_response_headers.started request=<Request [b'POST']>
2023-12-13 10:39:44,509 [DEBUG]: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 13 Dec 2023 15:39:44 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'openai-organization', b'credicle'), (b'openai-processing-ms', b'31'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'5000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'4999'), (b'x-ratelimit-remaining-tokens', b'4999928'), (b'x-ratelimit-reset-requests', b'12ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'2cc015a14897de38d96f05c4fac2cd5b'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'834f4d32a8c142f5-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2023-12-13 10:39:44,510 [INFO]: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
2023-12-13 10:39:44,510 [DEBUG]: receive_response_body.started request=<Request [b'POST']>
2023-12-13 10:39:44,511 [DEBUG]: receive_response_body.complete
2023-12-13 10:39:44,511 [DEBUG]: response_closed.started
2023-12-13 10:39:44,511 [DEBUG]: response_closed.complete
2023-12-13 10:39:44,511 [DEBUG]: HTTP Request: POST https://api.openai.com/v1/embeddings "200 OK"
2023-12-13 10:39:44,512 [DEBUG]: Embedded chunks
2023-12-13 10:39:45,454 [DEBUG]: Upserted vectors into index
2023-12-13 10:39:45,458 [INFO]: 127.0.0.1 - - [13/Dec/2023 10:39:45] "POST /api/generate_embeddings HTTP/1.1" 200 -
2023-12-13 10:39:46,115 [INFO]: 127.0.0.1 - - [13/Dec/2023 10:39:46] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-13 10:44:04,922 [INFO]: 127.0.0.1 - - [13/Dec/2023 10:44:04] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-13 10:56:51,983 [INFO]: 127.0.0.1 - - [13/Dec/2023 10:56:51] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-13 11:04:41,404 [INFO]: 127.0.0.1 - - [13/Dec/2023 11:04:41] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-13 11:08:41,794 [INFO]: 127.0.0.1 - - [13/Dec/2023 11:08:41] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-13 11:19:34,451 [INFO]: 127.0.0.1 - - [13/Dec/2023 11:19:34] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-13 11:34:48,054 [INFO]: 127.0.0.1 - - [13/Dec/2023 11:34:48] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-13 11:35:16,980 [INFO]: 127.0.0.1 - - [13/Dec/2023 11:35:16] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-13 11:52:49,032 [INFO]: 127.0.0.1 - - [13/Dec/2023 11:52:49] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-13 11:56:58,159 [INFO]: 127.0.0.1 - - [13/Dec/2023 11:56:58] "GET /api/fetchFileNames HTTP/1.1" 200 -
2023-12-13 11:57:02,900 [INFO]: 127.0.0.1 - - [13/Dec/2023 11:57:02] "GET /api/fetchFileNames HTTP/1.1" 200 -
